{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset import create_dataloader\n",
    "from basemodel_loader import load_model\n",
    "from activation_analysis import ActivationAnalyzer, ActivationVisualizer, OptimizedActivationAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Max Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading model...\")\n",
    "model_path = 'base_model_best.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, config = load_model(model_path, device)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nCreating dataloader...\")\n",
    "dataloader = create_dataloader(\n",
    "    data_path='validation.bin',\n",
    "    block_size=256,\n",
    "    batch_size=32,\n",
    "    num_workers=4\n",
    ")\n",
    "print(\"Dataloader created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nCreating analyzer...\")\n",
    "analyzer = OptimizedActivationAnalyzer(model, device=device)\n",
    "results = analyzer.analyze_activations(dataloader, num_batches=100)\n",
    "print(\"Initial analysis complete!\")\n",
    "\n",
    "# Optional: print some basic stats about the results\n",
    "print(\"\\nResults summary:\")\n",
    "for layer_name in results:\n",
    "    n_neurons = len(results[layer_name]['neurons'])\n",
    "    total_examples = sum(len(data['examples']) for data in results[layer_name]['neurons'].values())\n",
    "    print(f\"{layer_name}: analyzed {n_neurons} neurons, collected {total_examples} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a directory for results if it doesn't exist\n",
    "os.makedirs('activation_results', exist_ok=True)\n",
    "\n",
    "# Create filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f'activation_results/neuron_activations_{timestamp}.json'\n",
    "\n",
    "# Convert any tensor/numpy values to Python native types and save\n",
    "def convert_for_json(obj):\n",
    "    if hasattr(obj, 'tolist'):  # Handle torch tensors and numpy arrays\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_for_json(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_for_json(item) for item in obj]  # Changed 'list' to 'obj'\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert and save results\n",
    "json_results = convert_for_json(results)\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(json_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load lbl model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from model_loader import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading ablated model...\")\n",
    "model_path = 'best_model_20241016.pt'  # Using the path from config-2.yaml\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, config = load_model(model_path, device)\n",
    "print(\"Ablated model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nCreating dataloader...\")\n",
    "dataloader = create_dataloader(\n",
    "    data_path='validation.bin',  # From config-2.yaml\n",
    "    block_size=256,\n",
    "    batch_size=24,  # Updated to match config-2.yaml\n",
    "    num_workers=4\n",
    ")\n",
    "print(\"Dataloader created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nCreating analyzer...\")\n",
    "analyzer = OptimizedActivationAnalyzer(model, device=device)\n",
    "results = analyzer.analyze_activations(dataloader, num_batches=100)\n",
    "print(\"Initial analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nResults summary:\")\n",
    "for layer_name in results:\n",
    "    n_neurons = len(results[layer_name]['neurons'])\n",
    "    total_examples = sum(len(data['examples']) for data in results[layer_name]['neurons'].values())\n",
    "    print(f\"{layer_name}: analyzed {n_neurons} neurons, collected {total_examples} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a directory for ablation results if it doesn't exist\n",
    "os.makedirs('ablation_results', exist_ok=True)\n",
    "\n",
    "# Create filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f'ablation_results/neuron_activations_ablated_{timestamp}.json'\n",
    "\n",
    "# Convert any tensor/numpy values to Python native types and save\n",
    "def convert_for_json(obj):\n",
    "    if hasattr(obj, 'tolist'):  # Handle torch tensors and numpy arrays\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_for_json(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_for_json(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert and save results\n",
    "json_results = convert_for_json(results)\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(json_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the globally ablated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from model_loader import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading globally ablated model...\")\n",
    "model_path = 'best_model_20241017.pt'  # Using the path from Globalconfig.yaml\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, config = load_model(model_path, device)\n",
    "print(\"Globally ablated model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nCreating dataloader...\")\n",
    "dataloader = create_dataloader(\n",
    "    data_path='validation.bin',  \n",
    "    block_size=256,\n",
    "    batch_size=24,  \n",
    "    num_workers=4\n",
    ")\n",
    "print(\"Dataloader created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize and run analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nCreating analyzer...\")\n",
    "analyzer = OptimizedActivationAnalyzer(model, device=device)\n",
    "results = analyzer.analyze_activations(dataloader, num_batches=100)\n",
    "print(\"Initial analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nResults summary:\")\n",
    "for layer_name in results:\n",
    "    n_neurons = len(results[layer_name]['neurons'])\n",
    "    total_examples = sum(len(data['examples']) for data in results[layer_name]['neurons'].values())\n",
    "    print(f\"{layer_name}: analyzed {n_neurons} neurons, collected {total_examples} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create a directory for global ablation results if it doesn't exist\n",
    "os.makedirs('global_ablation_results', exist_ok=True)\n",
    "\n",
    "# Create filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f'global_ablation_results/neuron_activations_global_{timestamp}.json'\n",
    "\n",
    "# Convert any tensor/numpy values to Python native types and save\n",
    "def convert_for_json(obj):\n",
    "    if hasattr(obj, 'tolist'):  # Handle torch tensors and numpy arrays\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_for_json(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_for_json(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert and save results\n",
    "json_results = convert_for_json(results)\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(json_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {filename}\")\n",
    "\n",
    "# Optional: Print CUDA status\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Optional: Additional global ablation specific analysis\n",
    "print(\"\\nGlobal Ablation Specific Stats:\")\n",
    "print(f\"Number of neurons kept per layer: {config.k_neurons}\")\n",
    "print(f\"Number of attention components kept per layer: {config.k_attention}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Import needed modules\n",
    "from pruning import SequencePruner, setup_logger\n",
    "from model_loader import load_model\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage statistics\"\"\"\n",
    "    gpu_memory = f\"{torch.cuda.max_memory_allocated()/1e9:.2f}GB\" if torch.cuda.is_available() else \"N/A\"\n",
    "    ram_memory = f\"{psutil.Process().memory_info().rss/1e9:.2f}GB\"\n",
    "    return f\"GPU: {gpu_memory}, RAM: {ram_memory}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from model_loader import load_model\n",
    "\n",
    "def test_model_load(model_path):\n",
    "    print(f\"\\nTesting model load for: {model_path}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Check CUDA\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Load model\n",
    "        print(\"\\nLoading model...\")\n",
    "        model, config = load_model(model_path, device)\n",
    "        \n",
    "        # Create test input using dictionary config access\n",
    "        print(\"\\nTesting forward pass...\")\n",
    "        test_input = torch.randint(0, config['vocab_size'], (1, 32), device=device)\n",
    "        \n",
    "        # Test forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(test_input)\n",
    "            \n",
    "        # Print output shapes based on model type\n",
    "        print(\"\\nModel outputs:\")\n",
    "        if hasattr(outputs, 'keys'):\n",
    "            for key, value in outputs.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    print(f\"- {key}: {value.shape}\")\n",
    "                elif value is None:\n",
    "                    print(f\"- {key}: None\")\n",
    "        else:\n",
    "            print(f\"- output: {outputs.shape}\")\n",
    "            \n",
    "        print(\"\\n✓ Test successful!\")\n",
    "        return model, config\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Test failed with error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Test with base model\n",
    "model_path = \"configs/base_model_best.pt\"\n",
    "model, config = test_model_load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "from analysis_utils import ModelAnalyzer, AnalysisLogger\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create output directory with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f'pruned_results_{timestamp}'\n",
    "\n",
    "# Initialize analyzer (it will create the directory structure)\n",
    "analyzer = ModelAnalyzer(output_dir=output_dir)\n",
    "\n",
    "print(f\"Analysis setup complete. Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "config_dir = Path('configs')\n",
    "print(\"Config directory exists:\", config_dir.exists())\n",
    "print(\"YAML file exists:\", (config_dir / 'base_model_best.yaml').exists())\n",
    "print(\"Model file exists:\", (config_dir / 'base_model_best.pt').exists())\n",
    "print(\"Absolute path to YAML:\", (config_dir / 'base_model_best.yaml').absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Base Model Analysis\n",
    "base_results, base_stats = analyzer.load_and_analyze(\n",
    "    model_path='/root/.local/configs/base_model_best.pt',\n",
    "    activation_file='activation_results/neuron_activations_base.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Imports for graph building\n",
    "import torch\n",
    "import networkx as nx\n",
    "from neuron_graph_builder import NeuronAnalyzer\n",
    "from model_loader import load_model\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.cuda\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our custom modules\n",
    "from model_loader import load_model\n",
    "from neuron_graph_builder import NeuronAnalyzer\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"neuron_graphs/base_model\", exist_ok=True)\n",
    "os.makedirs(\"neuron_graphs/lbl_model\", exist_ok=True)\n",
    "os.makedirs(\"neuron_graphs/global_model\", exist_ok=True)\n",
    "\n",
    "# Check CUDA availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU Memory Available: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing  Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Base Model Analysis - Single Layer Debug Version\n",
    "print(\"Processing Base Model (Single Layer Debug)...\")\n",
    "base_model_path = 'configs/base_model_best.pt'\n",
    "base_activation_file = 'activation_results/neuron_activations_base.json'\n",
    "\n",
    "# Load model\n",
    "base_model, base_config = load_model(\n",
    "    model_path=base_model_path,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# Initialize analyzer\n",
    "base_analyzer = NeuronAnalyzer(base_model)\n",
    "\n",
    "# Load activation data\n",
    "base_data = base_analyzer.load_activation_data(base_activation_file)\n",
    "\n",
    "# Process single layer (layer 0 for debug)\n",
    "base_graphs = {}\n",
    "debug_layer = 0  # Change this to analyze different layers\n",
    "print(f\"\\nAnalyzing layer {debug_layer}\")\n",
    "layer_graphs = base_analyzer.analyze_layer(\n",
    "    activation_data=base_data,\n",
    "    layer=debug_layer,\n",
    "    save_graphs=True,\n",
    "    output_dir=f\"neuron_graphs/base_model/layer_{debug_layer}\"\n",
    ")\n",
    "base_graphs[debug_layer] = layer_graphs\n",
    "\n",
    "# Cleanup\n",
    "del base_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Base Model Analysis\n",
    "print(\"Processing Base Model...\")\n",
    "base_model_path = 'configs/base_model_best.pt'\n",
    "base_activation_file = 'activation_results/neuron_activations_base.json'\n",
    "\n",
    "# Load model\n",
    "base_model, base_config = load_model(\n",
    "    model_path=base_model_path,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# Initialize analyzer\n",
    "base_analyzer = NeuronAnalyzer(base_model)\n",
    "\n",
    "# Load activation data\n",
    "base_data = base_analyzer.load_activation_data(base_activation_file)\n",
    "\n",
    "# Process each layer\n",
    "base_graphs = {}\n",
    "for layer in range(base_config.num_layers):\n",
    "    print(f\"\\nAnalyzing layer {layer}\")\n",
    "    layer_graphs = base_analyzer.analyze_layer(\n",
    "        activation_data=base_data,\n",
    "        layer=layer,\n",
    "        save_graphs=True,\n",
    "        output_dir=f\"neuron_graphs/base_model/layer_{layer}\"\n",
    "    )\n",
    "    base_graphs[layer] = layer_graphs\n",
    "\n",
    "# Cleanup\n",
    "del base_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from model_loader import load_model\n",
    "from neuron_graph_builder import NeuronAnalyzer\n",
    "\n",
    "def analyze_sequence():\n",
    "    # 1. Load model\n",
    "    print(\"Loading model...\")\n",
    "    model_path = 'configs/base_model_best.pt'  # Adjust path as needed\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, config = load_model(model_path, device)\n",
    "    print(\"Model loaded successfully\")\n",
    "    \n",
    "    # 2. Initialize analyzer\n",
    "    analyzer = NeuronAnalyzer(model)\n",
    "    print(\"Analyzer initialized\")\n",
    "    \n",
    "    # 3. Test sequence (the shortest one from your data)\n",
    "    test_sequence = [5045, 14028]\n",
    "    \n",
    "    # 4. Decode and print tokens\n",
    "    print(\"\\n=== Token Analysis ===\")\n",
    "    tokens_to_check = [14028, 5045, 1701]\n",
    "    for token_id in tokens_to_check:\n",
    "        decoded = analyzer.decode_token(token_id)\n",
    "        print(f\"Token {token_id}: '{decoded}'\")\n",
    "    \n",
    "    # 5. Get raw activations for the sequence\n",
    "    print(\"\\n=== Activation Analysis ===\")\n",
    "    # Convert sequence to tokens with BOS\n",
    "    input_tensor = analyzer.to_tokens(analyzer.decode_tokens(test_sequence), prepend_bos=True)\n",
    "    print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "    \n",
    "    try:\n",
    "        # Get raw activations from model\n",
    "        outputs, cache = analyzer._model_forward(input_tensor, return_cache=True)\n",
    "        \n",
    "        # Get activations for layer 2, neuron 5\n",
    "        activations = cache['transformer.h.2.mlp'][0, :, 5]  # [0] for batch, : for all positions, 5 for neuron 5\n",
    "        \n",
    "        print(\"\\nRaw activations for layer 2, neuron 5:\")\n",
    "        decoded_tokens = analyzer.to_str_tokens(analyzer.decode_tokens(test_sequence), prepend_bos=True)\n",
    "        for i, (token, activation) in enumerate(zip(decoded_tokens, activations)):\n",
    "            print(f\"Position {i}: Token '{token}' -> Activation: {activation:.6f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting activations: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    # Cleanup\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Run analysis\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from model_loader import load_model\n",
    "from neuron_graph_builder import NeuronAnalyzer\n",
    "from graph_utils import fast_prune, fast_measure_importance\n",
    "\n",
    "def trace_activation_flow():\n",
    "    # Setup with same test sequence\n",
    "    print(\"Loading model...\")\n",
    "    model_path = 'configs/base_model_best.pt'\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, config = load_model(model_path, device)\n",
    "    analyzer = NeuronAnalyzer(model)\n",
    "    \n",
    "    # Test sequence\n",
    "    test_sequence = [5045, 14028]  # \"Tim wondered\"\n",
    "    \n",
    "    print(\"\\n=== Step 1: Initial Sequence Processing ===\")\n",
    "    # Convert to text and back to ensure consistent processing\n",
    "    text = analyzer.decode_tokens(test_sequence)\n",
    "    print(f\"Text: {text}\")\n",
    "    \n",
    "    input_tensor = analyzer.to_tokens(text, prepend_bos=True)\n",
    "    print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "    print(f\"Input tokens: {input_tensor.tolist()}\")\n",
    "    \n",
    "    print(\"\\n=== Step 2: Getting Base Activations ===\")\n",
    "    outputs, cache = analyzer._model_forward(input_tensor, return_cache=True)\n",
    "    activations = cache['transformer.h.2.mlp'][0, :, 5]\n",
    "    print(\"Raw activations:\")\n",
    "    for i, act in enumerate(activations):\n",
    "        print(f\"Position {i}: {act.item():.6f}\")\n",
    "    \n",
    "    print(\"\\n=== Step 3: Fast Prune Entry ===\")\n",
    "    try:\n",
    "        result, max_idx, initial_max, truncated_max = fast_prune(\n",
    "            analyzer=analyzer,  # Pass analyzer instance\n",
    "            layer=2,\n",
    "            neuron=5,\n",
    "            text_input=text,\n",
    "            pivot_index=1,  # Known position of 'wondered'\n",
    "            original_activation=3.134399,  # From original data\n",
    "            proportion_threshold=-0.5,\n",
    "            batch_size=4,\n",
    "            return_maxes=True\n",
    "        )\n",
    "        print(f\"Result text: {result}\")\n",
    "        print(f\"Max index: {max_idx}\")\n",
    "        print(f\"Initial max activation: {initial_max:.6f}\")\n",
    "        print(f\"Truncated max activation: {truncated_max:.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in fast_prune: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    print(\"\\n=== Step 4: Processing Single Example ===\")\n",
    "    try:\n",
    "        processed = analyzer.process_single_example(\n",
    "            text=text,\n",
    "            pivot_index=1,\n",
    "            original_activation=3.134399,\n",
    "            layer=2,\n",
    "            neuron=5\n",
    "        )\n",
    "        print(f\"Original sequence: {processed.original_sequence}\")\n",
    "        print(f\"Pruned sequence: {processed.pruned_sequence}\")\n",
    "        print(f\"Activating token: {processed.activating_token}\")\n",
    "        print(f\"Context tokens: {processed.context_tokens}\")\n",
    "        print(f\"Activation value: {processed.activation_value}\")\n",
    "        print(f\"Activation ratio: {processed.activation_ratio}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_single_example: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trace_activation_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Base Model Analysis - Single Neuron Debug Version\n",
    "print(\"Processing Base Model (Single Neuron Debug)...\")\n",
    "base_model_path = 'configs/base_model_best.pt'\n",
    "base_activation_file = 'activation_results/neuron_activations_base.json'\n",
    "\n",
    "# Load model\n",
    "base_model, base_config = load_model(\n",
    "    model_path=base_model_path,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# Initialize analyzer\n",
    "base_analyzer = NeuronAnalyzer(base_model)\n",
    "\n",
    "# Load activation data\n",
    "base_data = base_analyzer.load_activation_data(base_activation_file)\n",
    "\n",
    "# Get specific layer and neuron data\n",
    "layer_name = 'transformer.h.2.mlp.c_fc'\n",
    "neuron_id = '5'\n",
    "\n",
    "# Get just 5 examples\n",
    "examples = base_data[layer_name]['neurons'][neuron_id]['examples'][:5]\n",
    "\n",
    "# Build graph for just this neuron\n",
    "print(f\"\\nAnalyzing layer 2, neuron 0 with 5 examples\")\n",
    "graph = base_analyzer.build_graph(\n",
    "    layer=2,\n",
    "    neuron=5,\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "# Save single graph\n",
    "base_analyzer.save_graph(\n",
    "    graph=graph,\n",
    "    layer=2,\n",
    "    neuron=5,\n",
    "    output_dir=\"neuron_graphs/debug\"\n",
    ")\n",
    "\n",
    "# Cleanup\n",
    "del base_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# test_neuron_analyzer.ipynb\n",
    "\n",
    "import torch\n",
    "from model_loader import load_model\n",
    "from neuron_graph_builder import NeuronAnalyzer\n",
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Test with just one neuron from one layer\n",
    "def test_single_neuron():\n",
    "    print(\"Setting up test...\")\n",
    "    \n",
    "    # 1. Load model\n",
    "    model_path = 'configs/base_model_best.pt'\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, config = load_model(model_path, device)\n",
    "    print(\"Model loaded successfully\")\n",
    "    \n",
    "    # 2. Initialize analyzer\n",
    "    analyzer = NeuronAnalyzer(model)\n",
    "    print(\"Analyzer initialized\")\n",
    "    \n",
    "    # 3. Load a small subset of activation data\n",
    "    activation_file = 'activation_results/neuron_activations_base.json'\n",
    "    with open(activation_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 4. Get first layer and first neuron\n",
    "    layer = 0\n",
    "    layer_name = f'transformer.h.{layer}.mlp.c_fc'\n",
    "    first_neuron_id = list(data[layer_name]['neurons'].keys())[0]\n",
    "    neuron_data = data[layer_name]['neurons'][first_neuron_id]\n",
    "    \n",
    "    print(f\"\\nTesting with:\")\n",
    "    print(f\"Layer: {layer}\")\n",
    "    print(f\"Neuron: {first_neuron_id}\")\n",
    "    print(f\"Number of examples: {len(neuron_data['examples'])}\")\n",
    "    \n",
    "    # 5. Test processing a single example\n",
    "    try:\n",
    "        print(\"\\nProcessing single example...\")\n",
    "        example = neuron_data['examples'][0]\n",
    "        processed = analyzer.process_single_example(\n",
    "            sequence=example['sequence'],\n",
    "            pivot_index=example['pivot_index'],\n",
    "            original_activation=max(example['activations']),\n",
    "            layer=layer,\n",
    "            neuron=int(first_neuron_id)\n",
    "        )\n",
    "        print(\"Single example processed successfully!\")\n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"Activating token: {processed.activating_token}\")\n",
    "        print(f\"Number of context tokens: {len(processed.context_tokens)}\")\n",
    "        print(f\"Activation ratio: {processed.activation_ratio:.3f}\")\n",
    "        \n",
    "        # 6. Test building graph\n",
    "        print(\"\\nBuilding graph...\")\n",
    "        graph = analyzer.build_graph(\n",
    "            layer=layer,\n",
    "            neuron=int(first_neuron_id),\n",
    "            examples=neuron_data['examples'][:5]  # Just use first 5 examples for test\n",
    "        )\n",
    "        print(\"Graph built successfully!\")\n",
    "        print(f\"Number of nodes: {len(graph.nodes)}\")\n",
    "        print(f\"Number of edges: {len(graph.edges)}\")\n",
    "        \n",
    "        # 7. Test saving graph\n",
    "        test_output_dir = \"test_neuron_graphs\"\n",
    "        print(f\"\\nSaving graph to {test_output_dir}...\")\n",
    "        analyzer.save_graph(graph, layer, int(first_neuron_id), test_output_dir)\n",
    "        print(\"Graph saved successfully!\")\n",
    "        \n",
    "        print(\"\\nAll tests passed!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during testing: {str(e)}\")\n",
    "        return False\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Run the test\n",
    "test_single_neuron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def test_neuron_flow():\n",
    "    # Load model as before\n",
    "    model_path = '/root/.local/configs/base_model_best.pt'\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, config = load_model(model_path, device)\n",
    "    \n",
    "    # Initialize analyzer \n",
    "    analyzer = NeuronAnalyzer(model)\n",
    "    \n",
    "    # Load data\n",
    "    with open('activation_results/neuron_activations_base.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Get example data\n",
    "    layer_name = 'transformer.h.0.mlp.c_fc'\n",
    "    first_neuron_id = list(data[layer_name]['neurons'].keys())[0]\n",
    "    example = data[layer_name]['neurons'][first_neuron_id]['examples'][0]\n",
    "    \n",
    "    # Print detailed info about the example\n",
    "    print(\"\\nExample data:\")\n",
    "    print(f\"Sequence length: {len(example['sequence'])}\")\n",
    "    print(f\"Pivot index: {example['pivot_index']}\")\n",
    "    print(f\"Raw sequence: {example['sequence'][:10]}...\")  # First 10 tokens\n",
    "    \n",
    "    # Test token decoding\n",
    "    text = analyzer.decode_tokens(example['sequence'])\n",
    "    print(f\"\\nDecoded text (first 100 chars): {text[:100]}\")\n",
    "    \n",
    "    # Test token encoding\n",
    "    tokens = analyzer.to_tokens(text)\n",
    "    print(f\"\\nRe-encoded tokens shape: {tokens.shape}\")\n",
    "    \n",
    "    str_tokens = analyzer.to_str_tokens(text)\n",
    "    print(f\"\\nString tokens (first 5): {str_tokens[:5]}\")\n",
    "    \n",
    "    return example, text, tokens, str_tokens\n",
    "\n",
    "# Run test\n",
    "test_neuron_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def test_full_flow():\n",
    "    # Setup as before\n",
    "    model_path = '/root/.local/configs/base_model_best.pt'\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, config = load_model(model_path, device)\n",
    "    analyzer = NeuronAnalyzer(model)\n",
    "    \n",
    "    # Load data\n",
    "    with open('activation_results/neuron_activations_base.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Get example data\n",
    "    layer_name = 'transformer.h.0.mlp.c_fc'\n",
    "    first_neuron_id = list(data[layer_name]['neurons'].keys())[0]\n",
    "    example = data[layer_name]['neurons'][first_neuron_id]['examples'][0]\n",
    "    \n",
    "    print(\"\\nProcessing example...\")\n",
    "    try:\n",
    "        processed = analyzer.process_single_example(\n",
    "            sequence=example['sequence'],\n",
    "            pivot_index=example['pivot_index'],\n",
    "            original_activation=max(example['activations']),\n",
    "            layer=0,\n",
    "            neuron=int(first_neuron_id)\n",
    "        )\n",
    "        \n",
    "        print(\"\\nProcessed example results:\")\n",
    "        print(f\"Original sequence: {processed.original_sequence}\")\n",
    "        print(f\"Pruned sequence: {processed.pruned_sequence}\")\n",
    "        print(f\"Activating token: {processed.activating_token}\")\n",
    "        print(f\"Context tokens: {processed.context_tokens}\")\n",
    "        print(f\"Activation value: {processed.activation_value}\")\n",
    "        print(f\"Activation ratio: {processed.activation_ratio}\")\n",
    "        \n",
    "        return processed\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during processing: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run test\n",
    "test_full_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Setting up test...\")\n",
    "    \n",
    "# 1. Load model\n",
    "model_path = '/root/.local/configs/base_model_best.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, config = load_model(model_path, device)\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# 2. Initialize analyzer\n",
    "analyzer = NeuronAnalyzer(model)\n",
    "print(\"Analyzer initialized\")\n",
    "\n",
    "# 3. Load activation data\n",
    "activation_file = 'activation_results/neuron_activations_base.json'\n",
    "with open(activation_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 4. Get first layer and first neuron\n",
    "layer = 0\n",
    "layer_name = f'transformer.h.{layer}.mlp.c_fc'\n",
    "first_neuron_id = list(data[layer_name]['neurons'].keys())[0]\n",
    "neuron_data = data[layer_name]['neurons'][first_neuron_id]\n",
    "\n",
    "print(f\"\\nTesting with:\")\n",
    "print(f\"Layer: {layer}\")\n",
    "print(f\"Neuron: {first_neuron_id}\")\n",
    "print(f\"Number of examples: {len(neuron_data['examples'])}\")\n",
    "\n",
    "# 5. Build graph using all examples\n",
    "try:\n",
    "    print(\"\\nBuilding graph...\")\n",
    "    graph = analyzer.build_graph(\n",
    "        layer=layer,\n",
    "        neuron=int(first_neuron_id),\n",
    "        examples=neuron_data['examples'][:6],  # Using all examples\n",
    "        min_pattern_frequency=2  # Keeping original frequency threshold\n",
    "    )\n",
    "    print(\"Graph built successfully!\")\n",
    "    print(f\"Number of nodes: {len(graph.nodes)}\")\n",
    "    print(f\"Number of edges: {len(graph.edges)}\")\n",
    "    \n",
    "    # 6. Print graph details\n",
    "    if len(graph.nodes) > 0:\n",
    "        print(\"\\nGraph details:\")\n",
    "        print(\"\\nNodes:\")\n",
    "        for node in graph.nodes(data=True):\n",
    "            print(f\"Token: {node[0]}\")\n",
    "            print(f\"Data: {node[1]}\")\n",
    "            print()\n",
    "            \n",
    "        print(\"\\nEdges:\")\n",
    "        for edge in graph.edges(data=True):\n",
    "            print(f\"{edge[0]} -> {edge[1]}\")\n",
    "            print(f\"Weight: {edge[2]['weight']}\")\n",
    "            print()\n",
    "    \n",
    "    # 7. Save the graph\n",
    "    test_output_dir = \"test_neuron_graphs\"\n",
    "    print(f\"\\nSaving graph to {test_output_dir}...\")\n",
    "    analyzer.save_graph(graph, layer, int(first_neuron_id), test_output_dir)\n",
    "    print(\"Graph saved successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError during graph building: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Case Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from graph_utils import fast_prune\n",
    "print(\"Setting up test...\")\n",
    "\n",
    "# 1. Load model\n",
    "model_path = '/root/.local/configs/base_model_best.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, config = load_model(model_path, device)\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# 2. Initialize analyzer\n",
    "analyzer = NeuronAnalyzer(model)\n",
    "print(\"Analyzer initialized\")\n",
    "\n",
    "# 3. Load activation data\n",
    "activation_file = 'activation_results/neuron_activations_base.json'\n",
    "with open(activation_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 4. Get first layer and first neuron\n",
    "layer = 0\n",
    "layer_name = f'transformer.h.{layer}.mlp.c_fc'\n",
    "first_neuron_id = list(data[layer_name]['neurons'].keys())[0]\n",
    "neuron_data = data[layer_name]['neurons'][first_neuron_id]\n",
    "\n",
    "print(f\"\\nTesting with:\")\n",
    "print(f\"Layer: {layer}\")\n",
    "print(f\"Neuron: {first_neuron_id}\")\n",
    "print(f\"Number of examples: {len(neuron_data['examples'])}\")\n",
    "\n",
    "# 5. Build graph using all examples\n",
    "try:\n",
    "    print(\"\\nBuilding graph...\")\n",
    "    graph = analyzer.build_graph(\n",
    "        layer=layer,\n",
    "        neuron=int(first_neuron_id),\n",
    "        examples=neuron_data['examples'][:6], # Using all examples\n",
    "        min_pattern_frequency=2 # Keeping original frequency threshold\n",
    "    )\n",
    "    print(\"Graph built successfully!\")\n",
    "    print(f\"Number of nodes: {len(graph.nodes)}\")\n",
    "    print(f\"Number of edges: {len(graph.edges)}\")\n",
    "\n",
    "    # 6. Print graph details\n",
    "    if len(graph.nodes) > 0:\n",
    "        print(\"\\nGraph details:\")\n",
    "        print(\"\\nNodes:\")\n",
    "        for node in graph.nodes(data=True):\n",
    "            print(f\"Token: {node[0]}\")\n",
    "            print(f\"Data: {node[1]}\")\n",
    "            print()\n",
    "\n",
    "        print(\"\\nEdges:\")\n",
    "        for edge in graph.edges(data=True):\n",
    "            print(f\"{edge[0]} -> {edge[1]}\")\n",
    "            print(f\"Weight: {edge[2]['weight']}\")\n",
    "            print()\n",
    "\n",
    "    # 7. Save the graph\n",
    "    test_output_dir = \"test_neuron_graphs\"\n",
    "    print(f\"\\nSaving graph to {test_output_dir}...\")\n",
    "    analyzer.save_graph(graph, layer, int(first_neuron_id), test_output_dir)\n",
    "    print(\"Graph saved successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during graph building: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# 8. Run edge case tests\n",
    "print(\"\\n=== Running Edge Case Tests ===\")\n",
    "\n",
    "def test_edge_cases(analyzer):\n",
    "    \"\"\"Test position tracking with challenging inputs\"\"\"\n",
    "    test_cases = [\n",
    "        # Case 1: Very short input\n",
    "        (\"A.\", \"Short single-token input\"),\n",
    "        \n",
    "        # Case 2: Input with all sentence boundaries\n",
    "        (\"Stop. Look. Listen.\", \"Multiple short sentences\"),\n",
    "        \n",
    "        # Case 3: No clear sentence boundaries\n",
    "        (\"this is just a bunch of words without any proper punctuation or structure\", \n",
    "         \"No sentence boundaries\"),\n",
    "        \n",
    "        # Case 4: Mixed boundaries\n",
    "        (\"First sentence. Second sentence... Third sentence! Fourth sentence?\",\n",
    "         \"Mixed punctuation\"),\n",
    "        \n",
    "        # Case 5: Nested quotes\n",
    "        ('He said \"Stop.\" Then \"Go!\" Finally, \"Wait...\"',\n",
    "         \"Nested quotes\"),\n",
    "        \n",
    "        # Case 6: Maximum length edge\n",
    "        (\"The \" * 512, \"Length limit test\"),\n",
    "        \n",
    "        # Case 7: Special characters\n",
    "        (\"Line 1\\nLine 2\\nLine 3.\", \"Multiple newlines\"),\n",
    "        \n",
    "        # Case 8: Empty spaces\n",
    "        (\"   Lots   of   spaces   \", \"Multiple spaces\"),\n",
    "    ]\n",
    "    \n",
    "    print(\"Starting edge case tests...\")\n",
    "    \n",
    "    for text, description in test_cases:\n",
    "        print(f\"\\n=== Testing: {description} ===\")\n",
    "        print(f\"Input: {text[:100]}{'...' if len(text) > 100 else ''}\")\n",
    "        \n",
    "        try:\n",
    "            result, position = fast_prune(\n",
    "                analyzer=analyzer,\n",
    "                layer=0,\n",
    "                neuron=0,\n",
    "                text_input=text,\n",
    "                max_length=1024,\n",
    "                proportion_threshold=-0.5\n",
    "            )\n",
    "            \n",
    "            print(f\"Success!\")\n",
    "            print(f\"Output length: {len(result)}\")\n",
    "            print(f\"Final position: {position}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    print(\"\\nEdge case testing complete!\")\n",
    "\n",
    "# Run edge case tests with our analyzer\n",
    "test_edge_cases(analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing LBL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "from model_loader import load_model\n",
    "from neuron_graph_builder import NeuronAnalyzer\n",
    "\n",
    "# Cell 1: Test LBL Model Loading\n",
    "print(\"Testing LBL Model Loading...\")\n",
    "try:\n",
    "    # Model paths\n",
    "    lbl_model_path = 'configs/lbl_model_20241016.pt'\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"\\nAttempting to load model from: {lbl_model_path}\")\n",
    "    lbl_model, lbl_config = load_model(\n",
    "        model_path=lbl_model_path,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        config_dir='configs'\n",
    "    )\n",
    "    \n",
    "    # Print model info\n",
    "    print(\"\\nModel Configuration:\")\n",
    "    print(f\"Ablation mask level: {'layer-by-layer' if lbl_config.has_layer_by_layer_ablation_mask else 'overall'}\")\n",
    "    print(f\"Number of layers: {lbl_config.num_layers}\")\n",
    "    print(f\"Hidden size: {lbl_config.hidden_size}\")\n",
    "    print(f\"MLP hidden size: {lbl_config.mlp_hidden_size}\")\n",
    "    print(f\"Number of heads: {lbl_config.num_heads}\")\n",
    "    print(f\"Device: {next(lbl_model.parameters()).device}\")\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in lbl_model.parameters())}\")\n",
    "\n",
    "    # Print ablation-specific config\n",
    "    print(\"\\nAblation Configuration:\")\n",
    "    print(f\"k_attention: {lbl_config.k_attention}\")\n",
    "    print(f\"k_neurons: {lbl_config.k_neurons}\")\n",
    "    print(f\"Temperature attention: {lbl_config.temperature_attention}\")\n",
    "    print(f\"Temperature neurons: {lbl_config.temperature_neurons}\")\n",
    "    print(f\"Loss coefficient base: {lbl_config.loss_coeff_base}\")\n",
    "    print(f\"Loss coefficient ablated: {lbl_config.loss_coeff_ablated}\")\n",
    "    print(f\"Reconstruction coefficient: {lbl_config.reconstruction_coeff}\")\n",
    "\n",
    "    # Cleanup\n",
    "    del lbl_model\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nCleanup complete\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nFile not found error: {str(e)}\")\n",
    "    print(\"Please check that both the model file and its config exist in the configs directory\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError loading model: {str(e)}\")\n",
    "    print(\"Stack trace:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "start_neuron = 62  # Start from the next neuron after the last successful one\n",
    "batch_size = 10\n",
    "\n",
    "print(\"Processing LBL Model (Single Layer Debug)...\")\n",
    "try:\n",
    "    lbl_model_path = 'configs/lbl_model_20241016.pt'\n",
    "    lbl_activation_file = 'activation_results/neuron_activations_lbl.json'\n",
    "    debug_layer = 1\n",
    "    \n",
    "    # Calculate batch end, now using 511 as max\n",
    "    end_neuron = min(start_neuron + batch_size, 511 + 1)  # +1 because range is exclusive\n",
    "    print(f\"\\nProcessing neurons {start_neuron} to {end_neuron-1} in layer {debug_layer}\")\n",
    "    \n",
    "    # Load model\n",
    "    lbl_model, lbl_config = load_model(\n",
    "        model_path=lbl_model_path,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    lbl_analyzer = NeuronAnalyzer(lbl_model)\n",
    "    \n",
    "    # Load activation data\n",
    "    lbl_data = lbl_analyzer.load_activation_data(lbl_activation_file)\n",
    "    \n",
    "    # Process batch of neurons\n",
    "    lbl_graphs = {}\n",
    "    layer_graphs = {}\n",
    "    layer_name = f'transformer.h.{debug_layer}.mlp.c_fc'\n",
    "    \n",
    "    # Process only the specified range of neurons\n",
    "    for neuron_id in range(start_neuron, end_neuron):\n",
    "        print(f\"\\nProcessing neuron {neuron_id}\")\n",
    "        try:\n",
    "            graph = lbl_analyzer.build_graph(\n",
    "                layer=debug_layer,\n",
    "                neuron=neuron_id,\n",
    "                examples=lbl_data[layer_name]['neurons'][str(neuron_id)]['examples']\n",
    "            )\n",
    "            layer_graphs[neuron_id] = graph\n",
    "            \n",
    "            # Save individual graph\n",
    "            lbl_analyzer.save_graph(\n",
    "                graph,\n",
    "                debug_layer,\n",
    "                neuron_id,\n",
    "                f\"neuron_graphs/lbl_model/layer_{debug_layer}\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing neuron {neuron_id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    lbl_graphs[debug_layer] = layer_graphs\n",
    "    \n",
    "    # Cleanup\n",
    "    del lbl_model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nBatch complete! Processed neurons {start_neuron} to {end_neuron-1}\")\n",
    "    if end_neuron > 511:\n",
    "        print(\"Layer processing complete!\")\n",
    "    else:\n",
    "        print(f\"Next batch will start at neuron {end_neuron}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in analysis: {str(e)}\")\n",
    "    print(\"Stack trace:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "from model_loader import load_model\n",
    "from utils.compatibility import convert_model_to_hooked_transformer, get_ablation_hooks_for_tl\n",
    "\n",
    "print(\"Testing Model Compatibility...\")\n",
    "\n",
    "try:\n",
    "    # Model paths\n",
    "    model_path = 'configs/lbl_model_20241016.pt'\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"\\nAttempting to load model from: {model_path}\")\n",
    "    our_model, config = load_model(\n",
    "        model_path=model_path,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        config_dir='configs'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel loaded successfully!\")\n",
    "    print(f\"Ablation mask level: {'layer-by-layer' if config.has_layer_by_layer_ablation_mask else 'overall'}\")\n",
    "    \n",
    "    # Create and process test input\n",
    "    test_input = torch.tensor([42, 123, 1001], device=next(our_model.parameters()).device, dtype=torch.long).unsqueeze(0)\n",
    "    our_result = our_model(test_input)\n",
    "    print(\"\\nModel inference successful!\")\n",
    "    print(f\"Attention ablations shape: {our_result['attention_ablations'].shape}\")\n",
    "    print(f\"Neuron ablations shape: {our_result['neuron_ablations'].shape}\")\n",
    "    \n",
    "    # Convert to HookedTransformer\n",
    "    ht = convert_model_to_hooked_transformer(our_model)\n",
    "    print(\"\\nConverted to HookedTransformer successfully!\")\n",
    "    \n",
    "    # Run with original model \n",
    "    ht_result = ht(test_input)\n",
    "    print(\"\\nHookedTransformer inference successful!\")\n",
    "    \n",
    "    # Verify outputs match (accounting for unembed centering)\n",
    "    softmax_diff = (our_result[\"logits_clean\"].softmax(-1) - ht_result.softmax(-1)).abs().max()\n",
    "    print(f\"\\nMax softmax difference: {softmax_diff}\")\n",
    "    \n",
    "    # Test ablation hooks\n",
    "    last_token_ablation_hooks = get_ablation_hooks_for_tl(our_result, -1, our_model.config)\n",
    "    ht_result_ablated = ht.run_with_hooks(test_input, \"logits\", fwd_hooks=last_token_ablation_hooks)\n",
    "    print(\"\\nAblation hooks test successful!\")\n",
    "    \n",
    "    # Verify ablated outputs match\n",
    "    clean_diff = (our_result[\"logits_clean\"][0,:-1].softmax(-1) - ht_result_ablated[0,:-1].softmax(-1)).abs().max()\n",
    "    ablated_diff = (our_result[\"logits_ablated\"][0,-1:].softmax(-1) - ht_result_ablated[0,-1:].softmax(-1)).abs().max()\n",
    "    \n",
    "    print(f\"\\nMax clean difference: {clean_diff}\")\n",
    "    print(f\"Max ablated difference: {ablated_diff}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del our_model\n",
    "    del ht\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nCleanup complete\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nFile not found error: {str(e)}\")\n",
    "    print(\"Please check that both the model file and its config exist in the configs directory\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError loading model: {str(e)}\")\n",
    "    print(\"Stack trace:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Test ablated model inference\n",
    "print(\"Testing Ablated Model Inference...\")\n",
    "\n",
    "try:\n",
    "    # Model paths\n",
    "    model_path = 'configs/lbl_model_20241016.pt'\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"\\nLoading model from: {model_path}\")\n",
    "    model, config = load_model(\n",
    "        model_path=model_path,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        config_dir='configs'\n",
    "    )\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Create a simple test input\n",
    "    print(\"\\nPreparing test input...\")\n",
    "    test_input = torch.tensor([[1, 2, 3, 4, 5]], device=next(model.parameters()).device)  # Simple sequence\n",
    "    \n",
    "    # Run inference\n",
    "    print(\"\\nRunning inference...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_input)\n",
    "    \n",
    "    # Check outputs\n",
    "    print(\"\\nChecking outputs...\")\n",
    "    print(\"Output keys:\", list(outputs.keys()))\n",
    "    \n",
    "    if \"logits_clean\" in outputs:\n",
    "        print(\"\\nClean logits shape:\", outputs[\"logits_clean\"].shape)\n",
    "        print(\"Clean logits sample (first position, first 5 values):\")\n",
    "        print(outputs[\"logits_clean\"][0, 0, :5])\n",
    "        \n",
    "    if \"logits_ablated\" in outputs:\n",
    "        print(\"\\nAblated logits shape:\", outputs[\"logits_ablated\"].shape)\n",
    "        print(\"Ablated logits sample (first position, first 5 values):\")\n",
    "        print(outputs[\"logits_ablated\"][0, 0, :5])\n",
    "        \n",
    "    if \"attention_ablations\" in outputs:\n",
    "        print(\"\\nAttention ablations shape:\", outputs[\"attention_ablations\"].shape)\n",
    "        print(\"Attention ablations mean:\", outputs[\"attention_ablations\"].mean().item())\n",
    "        \n",
    "    if \"neuron_ablations\" in outputs:\n",
    "        print(\"\\nNeuron ablations shape:\", outputs[\"neuron_ablations\"].shape)\n",
    "        print(\"Neuron ablations mean:\", outputs[\"neuron_ablations\"].mean().item())\n",
    "    \n",
    "    # Test text generation\n",
    "    print(\"\\nTesting text generation...\")\n",
    "    generated = model.generate(\n",
    "        input_ids=test_input,\n",
    "        max_new_tokens=5,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(\"Generated token IDs:\", generated)\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nTest complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError during inference test: {str(e)}\")\n",
    "    print(\"Stack trace:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "from model_loader import load_model\n",
    "from utils.compatibility import convert_model_to_hooked_transformer, get_ablation_hooks_for_tl\n",
    "\n",
    "print(\"Testing Full HookedTransformer Compatibility...\")\n",
    "\n",
    "try:\n",
    "    # Load model\n",
    "    model_path = 'configs/lbl_model_20241016.pt'\n",
    "    print(f\"\\nLoading model from: {model_path}\")\n",
    "    our_model, config = load_model(\n",
    "        model_path=model_path,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        config_dir='configs'\n",
    "    )\n",
    "    \n",
    "    # Convert to HookedTransformer\n",
    "    print(\"\\nConverting to HookedTransformer...\")\n",
    "    ht = convert_model_to_hooked_transformer(our_model)\n",
    "    \n",
    "    # Create test input\n",
    "    test_input = torch.tensor([42, 123, 1001], device=next(our_model.parameters()).device, dtype=torch.long).unsqueeze(0)\n",
    "    \n",
    "    # Get outputs from both models\n",
    "    print(\"\\nRunning inference on both models...\")\n",
    "    our_output = our_model(test_input)\n",
    "    \n",
    "    # Get ablation hooks\n",
    "    ablation_hooks = get_ablation_hooks_for_tl(our_output, -1, our_model.config)\n",
    "    \n",
    "    # Run HookedTransformer with caching and hooks\n",
    "    print(\"\\nRunning HookedTransformer with cache and hooks...\")\n",
    "    with ht.hooks(fwd_hooks=ablation_hooks):\n",
    "        logits, cache = ht.run_with_cache(test_input)\n",
    "    \n",
    "    # Print activation shapes from cache\n",
    "    print(\"\\nCache activation shapes:\")\n",
    "    for key in sorted(cache.keys()):\n",
    "        if 'hook_' in key:\n",
    "            print(f\"{key}: {cache[key].shape}\")\n",
    "    \n",
    "    # Verify key activations match\n",
    "    print(\"\\nVerifying specific activations...\")\n",
    "    # Check MLP activations for a specific layer\n",
    "    layer_idx = 0\n",
    "    mlp_act_key = f'blocks.{layer_idx}.mlp.hook_post'\n",
    "    print(f\"\\nLayer {layer_idx} MLP activations shape: {cache[mlp_act_key].shape}\")\n",
    "    \n",
    "    # Check attention pattern for a specific layer\n",
    "    attn_key = f'blocks.{layer_idx}.attn.hook_pattern'\n",
    "    if attn_key in cache:\n",
    "        print(f\"Layer {layer_idx} attention pattern shape: {cache[attn_key].shape}\")\n",
    "    \n",
    "    # Verify outputs match (comparing logits)\n",
    "    print(\"\\nVerifying outputs...\")\n",
    "    clean_diff = (our_output[\"logits_clean\"][0,:-1].softmax(-1) - logits[0,:-1].softmax(-1)).abs().max()\n",
    "    ablated_diff = (our_output[\"logits_ablated\"][0,-1:].softmax(-1) - logits[0,-1:].softmax(-1)).abs().max()\n",
    "    \n",
    "    print(f\"Max clean difference: {clean_diff}\")\n",
    "    print(f\"Max ablated difference: {ablated_diff}\")\n",
    "    \n",
    "    # Memory cleanup\n",
    "    del our_model\n",
    "    del ht\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nTest complete!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during testing: {str(e)}\")\n",
    "    print(\"Stack trace:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Single Neuron Analysis Debug Cell\n",
    "print(\"Setting up single neuron debug analysis...\")\n",
    "\n",
    "# Model setup\n",
    "lbl_model_path = 'configs/lbl_model_20241016.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nLoading model from {lbl_model_path}\")\n",
    "lbl_model, lbl_config = load_model(model_path=lbl_model_path, device=device)\n",
    "\n",
    "# Initialize analyzer\n",
    "lbl_analyzer = NeuronAnalyzer(lbl_model)\n",
    "\n",
    "# Load activation data\n",
    "lbl_activation_file = 'activation_results/neuron_activations_lbl.json'\n",
    "print(f\"\\nLoading activation data from {lbl_activation_file}\")\n",
    "with open(lbl_activation_file, 'r') as f:\n",
    "    lbl_data = json.load(f)\n",
    "\n",
    "# Debug settings\n",
    "debug_layer = 0\n",
    "debug_neuron = 150  # Will analyze first neuron in layer\n",
    "layer_name = f'transformer.h.{debug_layer}.mlp.c_fc'\n",
    "\n",
    "print(f\"\\nAnalyzing:\")\n",
    "print(f\"Layer: {debug_layer}\")\n",
    "print(f\"Neuron: {debug_neuron}\")\n",
    "\n",
    "try:\n",
    "    # Get neuron data\n",
    "    neuron_data = lbl_data[layer_name]['neurons'][str(debug_neuron)]\n",
    "    print(f\"Found {len(neuron_data['examples'])} examples for this neuron\")\n",
    "    \n",
    "    # Process just a few examples first\n",
    "    num_debug_examples = 3\n",
    "    debug_examples = neuron_data['examples'][:num_debug_examples]\n",
    "    \n",
    "    print(f\"\\nProcessing {num_debug_examples} examples:\")\n",
    "    for i, example in enumerate(debug_examples):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Sequence length: {len(example['sequence'])}\")\n",
    "        print(f\"Pivot index: {example['pivot_index']}\")\n",
    "        print(f\"Max activation: {max(example['activations'])}\")\n",
    "        \n",
    "        try:\n",
    "            processed = lbl_analyzer.process_single_example(\n",
    "                text=example['sequence'],\n",
    "                pivot_index=example['pivot_index'],\n",
    "                original_activation=max(example['activations']),\n",
    "                layer=debug_layer,\n",
    "                neuron=debug_neuron\n",
    "            )\n",
    "            \n",
    "            print(\"\\nProcessed successfully!\")\n",
    "            print(f\"Activating token: {processed.activating_token}\")\n",
    "            print(f\"Context tokens: {processed.context_tokens}\")\n",
    "            print(f\"Activation ratio: {processed.activation_ratio:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # If examples processed successfully, try building graph\n",
    "    print(\"\\nAttempting to build graph...\")\n",
    "    graph = lbl_analyzer.build_graph(\n",
    "        layer=debug_layer,\n",
    "        neuron=debug_neuron,\n",
    "        examples=debug_examples\n",
    "    )\n",
    "    \n",
    "    print(\"\\nGraph statistics:\")\n",
    "    print(f\"Nodes: {len(graph.nodes)}\")\n",
    "    print(f\"Edges: {len(graph.edges)}\")\n",
    "    \n",
    "    # Save debug graph\n",
    "    debug_output_dir = \"debug_neuron_graphs\"\n",
    "    os.makedirs(debug_output_dir, exist_ok=True)\n",
    "    lbl_analyzer.save_graph(\n",
    "        graph=graph,\n",
    "        layer=debug_layer,\n",
    "        neuron=debug_neuron,\n",
    "        output_dir=debug_output_dir\n",
    "    )\n",
    "    print(f\"\\nDebug graph saved to {debug_output_dir}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError in analysis: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # Cleanup\n",
    "    print(\"\\nCleaning up...\")\n",
    "    del lbl_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nDebug analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#HERE\n",
    "print(\"Processing LBL Model (Single Layer Debug)...\")\n",
    "\n",
    "try:\n",
    "    lbl_model_path = 'configs/lbl_model_20241016.pt'\n",
    "    lbl_activation_file = 'activation_results/neuron_activations_lbl.json'\n",
    "\n",
    "    # Load model\n",
    "    lbl_model, lbl_config = load_model(\n",
    "        model_path=lbl_model_path,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "\n",
    "    # Initialize analyzer\n",
    "    lbl_analyzer = NeuronAnalyzer(lbl_model)\n",
    "\n",
    "    # Load activation data\n",
    "    lbl_data = lbl_analyzer.load_activation_data(lbl_activation_file)\n",
    "\n",
    "    # Process single layer\n",
    "    lbl_graphs = {}\n",
    "    debug_layer = 1\n",
    "    print(f\"\\nAnalyzing layer {debug_layer}\")\n",
    "    layer_graphs = lbl_analyzer.analyze_layer(\n",
    "        activation_data=lbl_data,\n",
    "        layer=debug_layer,\n",
    "        save_graphs=True,\n",
    "        output_dir=f\"neuron_graphs/lbl_model/layer_{debug_layer}\"\n",
    "    )\n",
    "    lbl_graphs[debug_layer] = layer_graphs\n",
    "\n",
    "    # Cleanup\n",
    "    del lbl_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in analysis: {str(e)}\")\n",
    "    print(\"Stack trace:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Import required dependencies \n",
    "from model_loader import load_model\n",
    "from neuron_graph_builder import NeuronAnalyzer\n",
    "import torch\n",
    "import traceback\n",
    "\n",
    "print(\"Testing Ablated Model Processing...\")\n",
    "try:\n",
    "    # Load model and config\n",
    "    model_path = 'configs/lbl_model_20241016.pt'  # Adjust path as needed\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, config = load_model(\n",
    "        model_path=model_path,\n",
    "        device=device\n",
    "    )\n",
    "    print(\"Model loaded successfully\")\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = NeuronAnalyzer(model)\n",
    "    print(\"Analyzer initialized\")\n",
    "    \n",
    "    # Test single example processing\n",
    "    test_layer = 0\n",
    "    test_neuron = 0\n",
    "    test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "    print(f\"\\nTesting with:\")\n",
    "    print(f\"Layer: {test_layer}\")\n",
    "    print(f\"Neuron: {test_neuron}\")\n",
    "    print(f\"Text: {test_text}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nTesting pruning and importance measurement...\")\n",
    "        processed = analyzer.process_single_example(\n",
    "            text=test_text,\n",
    "            pivot_index=5,  # Test with a known position\n",
    "            original_activation=1.0,  # Test activation value\n",
    "            layer=test_layer,\n",
    "            neuron=test_neuron\n",
    "        )\n",
    "        \n",
    "        print(\"\\nResults:\")\n",
    "        print(f\"Pruned sequence: {processed.pruned_sequence}\")\n",
    "        print(f\"Activating token: {processed.activating_token}\")\n",
    "        print(f\"Number of context tokens: {len(processed.context_tokens)}\")\n",
    "        print(f\"Context tokens: {processed.context_tokens}\")\n",
    "        print(f\"Activation ratio: {processed.activation_ratio:.3f}\")\n",
    "        \n",
    "        print(\"\\nTest completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in example processing: {str(e)}\")\n",
    "        print(\"Stack trace:\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in setup: {str(e)}\")\n",
    "    print(\"Stack trace:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Testing Ablated Model Processing with Real Data...\")\n",
    "try:\n",
    "    # Load model and config\n",
    "    model_path = 'configs/lbl_model_20241016.pt'\n",
    "    activation_file = 'activation_results/neuron_activations_lbl.json'\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Load model\n",
    "    model, config = load_model(\n",
    "        model_path=model_path,\n",
    "        device=device\n",
    "    )\n",
    "    print(\"Model loaded successfully\")\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = NeuronAnalyzer(model)\n",
    "    print(\"Analyzer initialized\")\n",
    "    \n",
    "    # Load activation data\n",
    "    data = analyzer.load_activation_data(activation_file)\n",
    "    print(\"Activation data loaded\")\n",
    "    \n",
    "    # Get example from actual data\n",
    "    debug_layer = 0\n",
    "    layer_name = f'transformer.h.{debug_layer}.mlp.c_fc'\n",
    "    \n",
    "    # Get first neuron data\n",
    "    first_neuron_id = list(data[layer_name]['neurons'].keys())[0]\n",
    "    neuron_data = data[layer_name]['neurons'][first_neuron_id]\n",
    "    \n",
    "    # Get eleventh example (long)\n",
    "    example = neuron_data['examples'][10]\n",
    "    # After getting the example\n",
    "    #print(\"Raw example data:\")\n",
    "    #print(example)\n",
    "    #print(\"\\nAll available examples:\")\n",
    "    #for i, ex in enumerate(neuron_data['examples']):\n",
    "    #    print(f\"Example {i}: Length = {len(ex['sequence'])}, First few tokens = {ex['sequence'][:5]}\")\n",
    "    \n",
    "    print(f\"\\nTesting with real data:\")\n",
    "    print(f\"Layer: {debug_layer}\")\n",
    "    print(f\"Neuron: {first_neuron_id}\")\n",
    "    print(f\"Original sequence length: {len(example['sequence'])}\")\n",
    "    print(f\"Original pivot index: {example['pivot_index']}\")\n",
    "    print(f\"Original activation: {max(example['activations'])}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nProcessing example...\")\n",
    "        processed = analyzer.process_single_example(\n",
    "            text=example['sequence'],\n",
    "            pivot_index=example['pivot_index'],\n",
    "            original_activation=max(example['activations']),\n",
    "            layer=debug_layer,\n",
    "            neuron=int(first_neuron_id)\n",
    "        )\n",
    "        \n",
    "        print(\"\\nResults:\")\n",
    "        print(f\"Pruned sequence: {processed.pruned_sequence}\")\n",
    "        print(f\"Activating token: {processed.activating_token}\")\n",
    "        print(f\"Number of context tokens: {len(processed.context_tokens)}\")\n",
    "        print(f\"Context tokens: {processed.context_tokens}\")\n",
    "        print(f\"Activation value: {processed.activation_value}\")\n",
    "        print(f\"Activation ratio: {processed.activation_ratio:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in example processing: {str(e)}\")\n",
    "        print(\"Stack trace:\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in setup: {str(e)}\")\n",
    "    print(\"Stack trace:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Last Test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset import create_dataloader\n",
    "from model_loader import load_model\n",
    "from utils.compatibility import convert_model_to_hooked_transformer, get_ablation_hooks_for_tl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set tokenizer parallelism off\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# 1. Load model\n",
    "print(\"Loading model...\")\n",
    "model_path = 'configs/lbl_model_20241016.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, config = load_model(model_path, device)\n",
    "model.eval()  # Ensure we're in eval mode\n",
    "print(\"Model loaded\")\n",
    "\n",
    "# 2. Simple dataloader\n",
    "print(\"\\nCreating dataloader...\")\n",
    "dataloader = create_dataloader(\n",
    "    data_path='validation.bin',\n",
    "    block_size=256,\n",
    "    batch_size=1,\n",
    "    num_workers=0  # No multiprocessing for now\n",
    ")\n",
    "print(\"Dataloader created\")\n",
    "\n",
    "# 3. Collect activation statistics\n",
    "print(\"\\nCollecting activation statistics...\")\n",
    "attention_ablations = []\n",
    "neuron_ablations = []\n",
    "num_samples = 20  # Reduced sample size\n",
    "\n",
    "with torch.no_grad():  # Add no_grad context\n",
    "    for i, (input_ids, _) in enumerate(dataloader):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "            \n",
    "        print(f\"Processing sample {i+1}/{num_samples}\")\n",
    "        input_ids = input_ids.to(device)\n",
    "        \n",
    "        # Get model outputs\n",
    "        outputs = model(input_ids)\n",
    "        \n",
    "        # Collect ablation values (with detach)\n",
    "        if 'attention_ablations' in outputs:\n",
    "            attn_vals = outputs['attention_ablations'].detach().abs().cpu().numpy()\n",
    "            attention_ablations.extend(attn_vals.flatten())\n",
    "            print(f\"Attention ablation shape: {outputs['attention_ablations'].shape}\")\n",
    "        \n",
    "        if 'neuron_ablations' in outputs:\n",
    "            neuron_vals = outputs['neuron_ablations'].detach().abs().cpu().numpy()\n",
    "            neuron_ablations.extend(neuron_vals.flatten())\n",
    "            print(f\"Neuron ablation shape: {outputs['neuron_ablations'].shape}\")\n",
    "\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "\n",
    "# 4. Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Ablation Value Analysis', fontsize=14)\n",
    "\n",
    "# Attention ablations\n",
    "if attention_ablations:\n",
    "    sns.histplot(data=attention_ablations, bins=50, ax=axes[0,0])\n",
    "    axes[0,0].set_title('Attention Ablation Distribution')\n",
    "    axes[0,0].set_yscale('log')\n",
    "    axes[0,0].set_xlabel('Ablation Value')\n",
    "\n",
    "# Neuron ablations\n",
    "if neuron_ablations:\n",
    "    sns.histplot(data=neuron_ablations, bins=50, ax=axes[0,1])\n",
    "    axes[0,1].set_title('Neuron Ablation Distribution')\n",
    "    axes[0,1].set_yscale('log')\n",
    "    axes[0,1].set_xlabel('Ablation Value')\n",
    "\n",
    "# Print summary statistics\n",
    "summary_text = \"Ablation Statistics:\\n\\n\"\n",
    "\n",
    "stats = {\n",
    "    'Attention Ablations': np.array(attention_ablations) if attention_ablations else None,\n",
    "    'Neuron Ablations': np.array(neuron_ablations) if neuron_ablations else None\n",
    "}\n",
    "\n",
    "for name, values in stats.items():\n",
    "    if values is not None:\n",
    "        summary_text += f\"{name}:\\n\"\n",
    "        summary_text += f\"  Mean: {np.mean(values):.6f}\\n\"\n",
    "        summary_text += f\"  Median: {np.median(values):.6f}\\n\"\n",
    "        summary_text += f\"  Std: {np.std(values):.6f}\\n\"\n",
    "        summary_text += f\"  Max: {np.max(values):.6f}\\n\"\n",
    "        summary_text += f\"  95th pct: {np.percentile(values, 95):.6f}\\n\\n\"\n",
    "        \n",
    "        # Print to console also\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Mean: {np.mean(values):.6f}\")\n",
    "        print(f\"  Median: {np.median(values):.6f}\")\n",
    "        print(f\"  95th percentile: {np.percentile(values, 95):.6f}\")\n",
    "\n",
    "axes[1,0].text(0.05, 0.95, summary_text,\n",
    "               transform=axes[1,0].transAxes,\n",
    "               verticalalignment='top',\n",
    "               fontsize=10,\n",
    "               family='monospace')\n",
    "axes[1,0].axis('off')\n",
    "\n",
    "# Add recommendations\n",
    "recommendations = \"Recommended Thresholds:\\n\\n\"\n",
    "for name, values in stats.items():\n",
    "    if values is not None:\n",
    "        median = np.median(values)\n",
    "        p95 = np.percentile(values, 95)\n",
    "        recommendations += f\"{name}:\\n\"\n",
    "        recommendations += f\"  Standard: {median:.4f} (median)\\n\"\n",
    "        recommendations += f\"  Conservative: {p95:.4f} (95th)\\n\\n\"\n",
    "\n",
    "axes[1,1].text(0.05, 0.95, recommendations,\n",
    "               transform=axes[1,1].transAxes,\n",
    "               verticalalignment='top',\n",
    "               fontsize=10,\n",
    "               family='monospace')\n",
    "axes[1,1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset import create_dataloader\n",
    "from model_loader import load_model\n",
    "from utils.compatibility import convert_model_to_hooked_transformer, get_ablation_hooks_for_tl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set tokenizer parallelism off\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# 1. Load both models\n",
    "print(\"Loading models...\")\n",
    "model_path = 'configs/lbl_model_20241016.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, config = load_model(model_path, device)\n",
    "model.eval()\n",
    "\n",
    "# Convert to HookedTransformer\n",
    "ht_model = convert_model_to_hooked_transformer(model)\n",
    "ht_model.eval()\n",
    "print(\"Models loaded\")\n",
    "\n",
    "# 2. Create dataloader\n",
    "print(\"\\nCreating dataloader...\")\n",
    "dataloader = create_dataloader(\n",
    "    data_path='validation.bin',\n",
    "    block_size=256,\n",
    "    batch_size=1,\n",
    "    num_workers=0\n",
    ")\n",
    "print(\"Dataloader created\")\n",
    "\n",
    "# 3. Collect statistics\n",
    "print(\"\\nCollecting activation statistics...\")\n",
    "stats = {\n",
    "    'attention_ablations': [],\n",
    "    'neuron_ablations': [], \n",
    "    'hooked_attention': [],\n",
    "    'hooked_neurons': []\n",
    "}\n",
    "num_samples = 20\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (input_ids, _) in enumerate(dataloader):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "            \n",
    "        print(f\"Processing sample {i+1}/{num_samples}\")\n",
    "        input_ids = input_ids.to(device)\n",
    "        \n",
    "        # Get original model outputs\n",
    "        outputs = model(input_ids)\n",
    "        \n",
    "        # Collect ablation values\n",
    "        if 'attention_ablations' in outputs:\n",
    "            attn_vals = outputs['attention_ablations'].detach().abs().cpu().numpy()\n",
    "            stats['attention_ablations'].extend(attn_vals.flatten())\n",
    "            print(f\"Attention ablation shape: {outputs['attention_ablations'].shape}\")\n",
    "        \n",
    "        if 'neuron_ablations' in outputs:\n",
    "            neuron_vals = outputs['neuron_ablations'].detach().abs().cpu().numpy()\n",
    "            stats['neuron_ablations'].extend(neuron_vals.flatten())\n",
    "            print(f\"Neuron ablation shape: {outputs['neuron_ablations'].shape}\")\n",
    "        \n",
    "        # Process with HookedTransformer\n",
    "        ablation_hooks = get_ablation_hooks_for_tl(outputs, -1, model.config)\n",
    "        with ht_model.hooks(fwd_hooks=ablation_hooks):\n",
    "            _, cache = ht_model.run_with_cache(input_ids)\n",
    "            \n",
    "            # Collect attention and neuron values from each layer\n",
    "            for layer in range(config.num_layers):\n",
    "                # Get attention outputs (z)\n",
    "                attn_out = cache[f'blocks.{layer}.attn.hook_z'][0].detach().abs().cpu().numpy()\n",
    "                stats['hooked_attention'].extend(attn_out.flatten())\n",
    "                \n",
    "                # Get MLP outputs (after ablation)\n",
    "                mlp_out = cache[f'blocks.{layer}.mlp.hook_post'][0].detach().abs().cpu().numpy()\n",
    "                stats['hooked_neurons'].extend(mlp_out.flatten())\n",
    "                \n",
    "        print(f\"HookedTransformer shapes - Attention: {attn_out.shape}, Neurons: {mlp_out.shape}\")\n",
    "\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "\n",
    "# 4. Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Ablation Analysis: Original vs HookedTransformer', fontsize=14)\n",
    "\n",
    "# Plot distributions\n",
    "plots = [\n",
    "    ('attention_ablations', 'Attention Ablation Distribution (Original)', axes[0,0]),\n",
    "    ('neuron_ablations', 'Neuron Ablation Distribution (Original)', axes[0,1]),\n",
    "    ('hooked_attention', 'Attention Values (HookedTransformer)', axes[1,0]),\n",
    "    ('hooked_neurons', 'Neuron Values (HookedTransformer)', axes[1,1])\n",
    "]\n",
    "\n",
    "for key, title, ax in plots:\n",
    "    if stats[key]:\n",
    "        sns.histplot(data=stats[key], bins=50, ax=ax)\n",
    "        ax.set_title(title)\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_xlabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nStatistics Summary:\")\n",
    "for name, values in stats.items():\n",
    "    if values:\n",
    "        values = np.array(values)\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Mean: {np.mean(values):.6f}\")\n",
    "        print(f\"  Median: {np.median(values):.6f}\")\n",
    "        print(f\"  Std: {np.std(values):.6f}\")\n",
    "        print(f\"  Max: {np.max(values):.6f}\")\n",
    "        print(f\"  95th pct: {np.percentile(values, 95):.6f}\")\n",
    "\n",
    "# Plot additional comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "labels = []\n",
    "values = []\n",
    "\n",
    "for name, data in stats.items():\n",
    "    if data:\n",
    "        labels.append(name)\n",
    "        values.append(np.array(data))\n",
    "\n",
    "plt.boxplot(values, labels=labels)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Value Distribution Comparison')\n",
    "plt.ylabel('Value')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset import create_dataloader\n",
    "from model_loader import load_model\n",
    "from utils.compatibility import convert_model_to_hooked_transformer, get_ablation_hooks_for_tl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set tokenizer parallelism off\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# 1. Load both models\n",
    "print(\"Loading models...\")\n",
    "model_path = 'configs/lbl_model_20241016.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, config = load_model(model_path, device)\n",
    "model.eval()\n",
    "\n",
    "# Convert to HookedTransformer\n",
    "ht_model = convert_model_to_hooked_transformer(model)\n",
    "ht_model.eval()\n",
    "print(\"Models loaded\")\n",
    "\n",
    "# 2. Create dataloader\n",
    "print(\"\\nCreating dataloader...\")\n",
    "dataloader = create_dataloader(\n",
    "    data_path='validation.bin',\n",
    "    block_size=256,\n",
    "    batch_size=1,\n",
    "    num_workers=0\n",
    ")\n",
    "print(\"Dataloader created\")\n",
    "\n",
    "# 3. Collect statistics\n",
    "print(\"\\nCollecting activation statistics...\")\n",
    "stats = {\n",
    "    'original_ablations': [],\n",
    "    'hooked_ablated_values': []\n",
    "}\n",
    "num_samples = 20\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (input_ids, _) in enumerate(dataloader):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "            \n",
    "        print(f\"Processing sample {i+1}/{num_samples}\")\n",
    "        input_ids = input_ids.to(device)\n",
    "        \n",
    "        # Get original model outputs with ablation masks\n",
    "        outputs = model(input_ids)\n",
    "        \n",
    "        # Get original ablation masks\n",
    "        if 'neuron_ablations' in outputs:\n",
    "            neuron_vals = outputs['neuron_ablations'].detach().abs().cpu().numpy()\n",
    "            stats['original_ablations'].extend(neuron_vals.flatten())\n",
    "            print(f\"Original ablation shape: {outputs['neuron_ablations'].shape}\")\n",
    "        \n",
    "        # Process with HookedTransformer using same ablation masks\n",
    "        ablation_hooks = get_ablation_hooks_for_tl(outputs, -1, model.config)\n",
    "        \n",
    "        with ht_model.hooks(fwd_hooks=ablation_hooks):\n",
    "            _, cache = ht_model.run_with_cache(input_ids)\n",
    "            \n",
    "            # Collect MLP outputs after ablation hooks\n",
    "            for layer in range(config.num_layers):\n",
    "                # Get post-ablation MLP values\n",
    "                mlp_out = cache[f'blocks.{layer}.mlp.hook_post'][0].detach().abs().cpu().numpy()\n",
    "                stats['hooked_ablated_values'].extend(mlp_out.flatten())\n",
    "                print(f\"Layer {layer} HookedTransformer post-ablation shape: {mlp_out.shape}\")\n",
    "\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "\n",
    "# 4. Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Ablation Analysis: Original vs HookedTransformer (Post-Ablation)', fontsize=14)\n",
    "\n",
    "# Plot distributions\n",
    "plots = [\n",
    "    ('original_ablations', 'Original Model Ablation Masks', axes[0]),\n",
    "    ('hooked_ablated_values', 'HookedTransformer Post-Ablation Values', axes[1])\n",
    "]\n",
    "\n",
    "for key, title, ax in plots:\n",
    "    if stats[key]:\n",
    "        sns.histplot(data=stats[key], bins=50, ax=ax)\n",
    "        ax.set_title(title)\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_xlabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nStatistics Summary:\")\n",
    "for name, values in stats.items():\n",
    "    if values:\n",
    "        values = np.array(values)\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Mean: {np.mean(values):.6f}\")\n",
    "        print(f\"  Median: {np.median(values):.6f}\")\n",
    "        print(f\"  Std: {np.std(values):.6f}\")\n",
    "        print(f\"  Max: {np.max(values):.6f}\")\n",
    "        print(f\"  95th pct: {np.percentile(values, 95):.6f}\")\n",
    "\n",
    "# Plot value comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "boxplot_data = [np.array(stats[key]) for key in stats if stats[key]]\n",
    "plt.boxplot(boxplot_data, labels=list(stats.keys()))\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Value Distribution Comparison')\n",
    "plt.ylabel('Value')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset import create_dataloader\n",
    "from model_loader import load_model\n",
    "from utils.compatibility import convert_model_to_hooked_transformer, get_ablation_hooks_for_tl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def analyze_hooked_transformer(model_path: str, num_samples: int = 20):\n",
    "    \"\"\"Analyze activations and ablations using HookedTransformer\"\"\"\n",
    "    \n",
    "    print(\"Loading models...\")\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, config = load_model(model_path, device)\n",
    "    model.eval()\n",
    "\n",
    "    ht_model = convert_model_to_hooked_transformer(model)\n",
    "    ht_model.eval()\n",
    "    print(\"Models loaded\")\n",
    "\n",
    "    dataloader = create_dataloader(\n",
    "        data_path='validation.bin',\n",
    "        block_size=256,\n",
    "        batch_size=1,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    stats = {\n",
    "        'pre_ablation_activations': [],   # Values before ablation\n",
    "        'post_ablation_activations': [],  # Values after ablation\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (input_ids, _) in enumerate(dataloader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "                \n",
    "            print(f\"Processing sample {i+1}/{num_samples}\")\n",
    "            input_ids = input_ids.to(device)\n",
    "            \n",
    "            # Get ablation hooks from original model\n",
    "            output = model(input_ids)\n",
    "            ablation_hooks = get_ablation_hooks_for_tl(output, -1, model.config)\n",
    "            \n",
    "            # Run with ablation hooks\n",
    "            with ht_model.hooks(fwd_hooks=ablation_hooks):\n",
    "                _, cache = ht_model.run_with_cache(\n",
    "                    input_ids,\n",
    "                    names_filter=lambda name: 'mlp.hook_' in name\n",
    "                )\n",
    "                \n",
    "                # Collect pre and post ablation values\n",
    "                for layer in range(config.num_layers):\n",
    "                    # Pre-ablation\n",
    "                    pre_acts = cache[f'blocks.{layer}.mlp.hook_pre'][0]\n",
    "                    stats['pre_ablation_activations'].extend(\n",
    "                        pre_acts.abs().cpu().numpy().flatten()\n",
    "                    )\n",
    "                    \n",
    "                    # Post-ablation \n",
    "                    post_acts = cache[f'blocks.{layer}.mlp.hook_post'][0]\n",
    "                    stats['post_ablation_activations'].extend(\n",
    "                        post_acts.abs().cpu().numpy().flatten()\n",
    "                    )\n",
    "\n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    fig.suptitle('HookedTransformer Ablation Analysis', fontsize=14)\n",
    "\n",
    "    plots = [\n",
    "        ('pre_ablation_activations', 'Pre-Ablation Activations (hook_pre)'),\n",
    "        ('post_ablation_activations', 'Post-Ablation Activations (hook_post)')\n",
    "    ]\n",
    "\n",
    "    for (key, title), ax in zip(plots, axes):\n",
    "        if stats[key]:\n",
    "            sns.histplot(data=stats[key], bins=50, ax=ax)\n",
    "            ax.set_title(title)\n",
    "            ax.set_yscale('log')\n",
    "            ax.set_xlabel('Value')\n",
    "        else:\n",
    "            print(f\"Warning: No data collected for {key}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print statistics\n",
    "    print(\"\\nStatistics Summary:\")\n",
    "    for name, values in stats.items():\n",
    "        if values:\n",
    "            values = np.array(values)\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  Mean: {np.mean(values):.6f}\")\n",
    "            print(f\"  Median: {np.median(values):.6f}\")\n",
    "            print(f\"  Std: {np.std(values):.6f}\")\n",
    "            print(f\"  Max: {np.max(values):.6f}\")\n",
    "            print(f\"  95th pct: {np.percentile(values, 95):.6f}\")\n",
    "            print(f\"  # Values: {len(values)}\")\n",
    "        else:\n",
    "            print(f\"\\n{name}: No data collected\")\n",
    "\n",
    "    # Calculate and print ablation effect\n",
    "    if stats['pre_ablation_activations'] and stats['post_ablation_activations']:\n",
    "        pre_vals = np.array(stats['pre_ablation_activations'])\n",
    "        post_vals = np.array(stats['post_ablation_activations'])\n",
    "        print(\"\\nAblation Effect:\")\n",
    "        print(f\"  Average activation reduction: {(1 - np.mean(post_vals)/np.mean(pre_vals))*100:.2f}%\")\n",
    "        print(f\"  Median activation reduction: {(1 - np.median(post_vals)/np.median(pre_vals))*100:.2f}%\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = 'configs/lbl_model_20241016.pt'\n",
    "    stats = analyze_hooked_transformer(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Processing Global Model...\")\n",
    "global_model_path = 'global_model_20241017.pt'\n",
    "global_activation_file = 'activation_results/neuron_activations_global.json'\n",
    "\n",
    "# Load model\n",
    "global_model, global_config = load_model(\n",
    "    model_path=global_model_path,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# Initialize analyzer \n",
    "global_analyzer = NeuronAnalyzer(global_model)\n",
    "\n",
    "# Load activation data\n",
    "global_data = global_analyzer.load_activation_data(global_activation_file)\n",
    "\n",
    "# Process each layer\n",
    "global_graphs = {}\n",
    "for layer in range(global_config.num_layers):\n",
    "    print(f\"\\nAnalyzing layer {layer}\")\n",
    "    layer_graphs = global_analyzer.analyze_layer(\n",
    "        activation_data=global_data,\n",
    "        layer=layer,\n",
    "        save_graphs=True,\n",
    "        output_dir=f\"neuron_graphs/global_model/layer_{layer}\"\n",
    "    )\n",
    "    global_graphs[layer] = layer_graphs\n",
    "\n",
    "# Cleanup\n",
    "del global_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = \"neuron_graphs/base_model\"\n",
    "stats = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# For each layer (0-7)\n",
    "for layer in range(8):\n",
    "    layer_dir = os.path.join(base_dir, f\"layer_{layer}\")\n",
    "    \n",
    "    # For neurons 0-199 \n",
    "    for neuron in range(200):\n",
    "        graph_path = os.path.join(layer_dir, f\"l{layer}_n{neuron}_graph.json\")\n",
    "        \n",
    "        if os.path.exists(graph_path):\n",
    "            with open(graph_path, 'r') as f:\n",
    "                graph = json.load(f)\n",
    "                \n",
    "            # Collect statistics\n",
    "            stats[layer]['num_nodes'].append(len(graph['nodes']))\n",
    "            stats[layer]['num_edges'].append(len(graph.get('edges', {})))\n",
    "            \n",
    "            # Count activating nodes and get activation values\n",
    "            activating_nodes = 0\n",
    "            activation_values = []\n",
    "            importance_values = []\n",
    "            \n",
    "            for node, data in graph['nodes'].items():\n",
    "                if data.get('is_activating', False):\n",
    "                    activating_nodes += 1\n",
    "                    if 'activation' in data:\n",
    "                        activation_values.append(data['activation'])\n",
    "                elif 'importance' in data:\n",
    "                    importance_values.append(data['importance'])\n",
    "            \n",
    "            stats[layer]['num_activating'].append(activating_nodes)\n",
    "            if activation_values:\n",
    "                stats[layer]['avg_activation'].append(np.mean(activation_values))\n",
    "            if importance_values:\n",
    "                stats[layer]['avg_importance'].append(np.mean(importance_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Layer Statistics:\")\n",
    "for layer in sorted(stats.keys()):\n",
    "    print(f\"\\nLayer {layer}:\")\n",
    "    \n",
    "    # Basic stats\n",
    "    n_neurons = len(stats[layer]['num_nodes'])\n",
    "    print(f\"Neurons processed: {n_neurons}\")\n",
    "    \n",
    "    # Print averages with standard deviations\n",
    "    for metric in ['num_nodes', 'num_edges', 'num_activating']:\n",
    "        values = stats[layer][metric]\n",
    "        print(f\"Average {metric}: {np.mean(values):.2f} ± {np.std(values):.2f}\")\n",
    "    \n",
    "    # Print activation and importance stats\n",
    "    if stats[layer]['avg_activation']:\n",
    "        print(f\"Average activation: {np.mean(stats[layer]['avg_activation']):.4f}\")\n",
    "    if stats[layer]['avg_importance']:\n",
    "        print(f\"Average importance: {np.mean(stats[layer]['avg_importance']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set figure size\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Layer-wise Graph Statistics', fontsize=16)\n",
    "\n",
    "# Plot average nodes per layer\n",
    "layer_nums = sorted(stats.keys())\n",
    "avg_nodes = [np.mean(stats[layer]['num_nodes']) for layer in layer_nums]\n",
    "std_nodes = [np.std(stats[layer]['num_nodes']) for layer in layer_nums]\n",
    "axes[0,0].errorbar(layer_nums, avg_nodes, yerr=std_nodes, marker='o')\n",
    "axes[0,0].set_title('Average Number of Nodes per Layer')\n",
    "axes[0,0].set_xlabel('Layer')\n",
    "axes[0,0].set_ylabel('Number of Nodes')\n",
    "axes[0,0].grid(True)\n",
    "\n",
    "# Plot average edges per layer\n",
    "avg_edges = [np.mean(stats[layer]['num_edges']) for layer in layer_nums]\n",
    "std_edges = [np.std(stats[layer]['num_edges']) for layer in layer_nums]\n",
    "axes[0,1].errorbar(layer_nums, avg_edges, yerr=std_edges, marker='o')\n",
    "axes[0,1].set_title('Average Number of Edges per Layer')\n",
    "axes[0,1].set_xlabel('Layer')\n",
    "axes[0,1].set_ylabel('Number of Edges')\n",
    "axes[0,1].grid(True)\n",
    "\n",
    "# Plot average activating nodes\n",
    "avg_act = [np.mean(stats[layer]['num_activating']) for layer in layer_nums]\n",
    "std_act = [np.std(stats[layer]['num_activating']) for layer in layer_nums]\n",
    "axes[1,0].errorbar(layer_nums, avg_act, yerr=std_act, marker='o')\n",
    "axes[1,0].set_title('Average Number of Activating Nodes')\n",
    "axes[1,0].set_xlabel('Layer')\n",
    "axes[1,0].set_ylabel('Number of Activating Nodes')\n",
    "axes[1,0].grid(True)\n",
    "\n",
    "# Plot average activation values\n",
    "avg_actval = [np.mean(stats[layer]['avg_activation']) for layer in layer_nums]\n",
    "std_actval = [np.std(stats[layer]['avg_activation']) for layer in layer_nums]\n",
    "axes[1,1].errorbar(layer_nums, avg_actval, yerr=std_actval, marker='o')\n",
    "axes[1,1].set_title('Average Activation Values')\n",
    "axes[1,1].set_xlabel('Layer')\n",
    "axes[1,1].set_ylabel('Activation Value')\n",
    "axes[1,1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = \"neuron_graphs/lbl\"\n",
    "stats = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# For each layer (0-7)\n",
    "for layer in range(8):\n",
    "    layer_dir = os.path.join(base_dir, f\"layer_{layer}\")\n",
    "    \n",
    "    # For neurons 0-199 \n",
    "    for neuron in range(200):\n",
    "        graph_path = os.path.join(layer_dir, f\"l{layer}_n{neuron}_graph.json\")\n",
    "        \n",
    "        if os.path.exists(graph_path):\n",
    "            with open(graph_path, 'r') as f:\n",
    "                graph = json.load(f)\n",
    "                \n",
    "            # Collect statistics\n",
    "            stats[layer]['num_nodes'].append(len(graph['nodes']))\n",
    "            stats[layer]['num_edges'].append(len(graph.get('edges', {})))\n",
    "            \n",
    "            # Count activating nodes and get activation values\n",
    "            activating_nodes = 0\n",
    "            activation_values = []\n",
    "            importance_values = []\n",
    "            \n",
    "            for node, data in graph['nodes'].items():\n",
    "                if data.get('is_activating', False):\n",
    "                    activating_nodes += 1\n",
    "                    if 'activation' in data:\n",
    "                        activation_values.append(data['activation'])\n",
    "                elif 'importance' in data:\n",
    "                    importance_values.append(data['importance'])\n",
    "            \n",
    "            stats[layer]['num_activating'].append(activating_nodes)\n",
    "            if activation_values:\n",
    "                stats[layer]['avg_activation'].append(np.mean(activation_values))\n",
    "            if importance_values:\n",
    "                stats[layer]['avg_importance'].append(np.mean(importance_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Layer Statistics:\")\n",
    "for layer in sorted(stats.keys()):\n",
    "    print(f\"\\nLayer {layer}:\")\n",
    "    \n",
    "    # Basic stats\n",
    "    n_neurons = len(stats[layer]['num_nodes'])\n",
    "    print(f\"Neurons processed: {n_neurons}\")\n",
    "    \n",
    "    # Print averages with standard deviations\n",
    "    for metric in ['num_nodes', 'num_edges', 'num_activating']:\n",
    "        values = stats[layer][metric]\n",
    "        print(f\"Average {metric}: {np.mean(values):.2f} ± {np.std(values):.2f}\")\n",
    "    \n",
    "    # Print activation and importance stats\n",
    "    if stats[layer]['avg_activation']:\n",
    "        print(f\"Average activation: {np.mean(stats[layer]['avg_activation']):.4f}\")\n",
    "    if stats[layer]['avg_importance']:\n",
    "        print(f\"Average importance: {np.mean(stats[layer]['avg_importance']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set figure size\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Layer-wise Graph Statistics', fontsize=16)\n",
    "\n",
    "# Plot average nodes per layer\n",
    "layer_nums = sorted(stats.keys())\n",
    "avg_nodes = [np.mean(stats[layer]['num_nodes']) for layer in layer_nums]\n",
    "std_nodes = [np.std(stats[layer]['num_nodes']) for layer in layer_nums]\n",
    "axes[0,0].errorbar(layer_nums, avg_nodes, yerr=std_nodes, marker='o')\n",
    "axes[0,0].set_title('Average Number of Nodes per Layer')\n",
    "axes[0,0].set_xlabel('Layer')\n",
    "axes[0,0].set_ylabel('Number of Nodes')\n",
    "axes[0,0].grid(True)\n",
    "\n",
    "# Plot average edges per layer\n",
    "avg_edges = [np.mean(stats[layer]['num_edges']) for layer in layer_nums]\n",
    "std_edges = [np.std(stats[layer]['num_edges']) for layer in layer_nums]\n",
    "axes[0,1].errorbar(layer_nums, avg_edges, yerr=std_edges, marker='o')\n",
    "axes[0,1].set_title('Average Number of Edges per Layer')\n",
    "axes[0,1].set_xlabel('Layer')\n",
    "axes[0,1].set_ylabel('Number of Edges')\n",
    "axes[0,1].grid(True)\n",
    "\n",
    "# Plot average activating nodes\n",
    "avg_act = [np.mean(stats[layer]['num_activating']) for layer in layer_nums]\n",
    "std_act = [np.std(stats[layer]['num_activating']) for layer in layer_nums]\n",
    "axes[1,0].errorbar(layer_nums, avg_act, yerr=std_act, marker='o')\n",
    "axes[1,0].set_title('Average Number of Activating Nodes')\n",
    "axes[1,0].set_xlabel('Layer')\n",
    "axes[1,0].set_ylabel('Number of Activating Nodes')\n",
    "axes[1,0].grid(True)\n",
    "\n",
    "# Plot average activation values\n",
    "avg_actval = [np.mean(stats[layer]['avg_activation']) for layer in layer_nums]\n",
    "std_actval = [np.std(stats[layer]['avg_activation']) for layer in layer_nums]\n",
    "axes[1,1].errorbar(layer_nums, avg_actval, yerr=std_actval, marker='o')\n",
    "axes[1,1].set_title('Average Activation Values')\n",
    "axes[1,1].set_xlabel('Layer')\n",
    "axes[1,1].set_ylabel('Activation Value')\n",
    "axes[1,1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def collect_model_stats(model_dir):\n",
    "    stats = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    # For each layer (0-7)\n",
    "    for layer in range(8):\n",
    "        layer_dir = os.path.join(model_dir, f\"layer_{layer}\")\n",
    "        \n",
    "        # For neurons 0-199 \n",
    "        for neuron in range(200):\n",
    "            graph_path = os.path.join(layer_dir, f\"l{layer}_n{neuron}_graph.json\")\n",
    "            \n",
    "            if os.path.exists(graph_path):\n",
    "                with open(graph_path, 'r') as f:\n",
    "                    graph = json.load(f)\n",
    "                    \n",
    "                # Collect statistics\n",
    "                stats[layer]['num_nodes'].append(len(graph['nodes']))\n",
    "                stats[layer]['num_edges'].append(len(graph.get('edges', {})))\n",
    "                \n",
    "                # Count activating nodes and get activation values\n",
    "                activating_nodes = 0\n",
    "                activation_values = []\n",
    "                importance_values = []\n",
    "                \n",
    "                for node, data in graph['nodes'].items():\n",
    "                    if data.get('is_activating', False):\n",
    "                        activating_nodes += 1\n",
    "                        if 'activation' in data:\n",
    "                            activation_values.append(data['activation'])\n",
    "                    elif 'importance' in data:\n",
    "                        importance_values.append(data['importance'])\n",
    "                \n",
    "                stats[layer]['num_activating'].append(activating_nodes)\n",
    "                if activation_values:\n",
    "                    stats[layer]['avg_activation'].append(np.mean(activation_values))\n",
    "                if importance_values:\n",
    "                    stats[layer]['avg_importance'].append(np.mean(importance_values))\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Collect stats for both models\n",
    "lbl_stats = collect_model_stats(\"neuron_graphs/lbl\")\n",
    "base_stats = collect_model_stats(\"neuron_graphs/base_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure with 2x2 subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "fig.suptitle('Comparative Layer-wise Statistics: Base Model vs LBL Model', fontsize=16)\n",
    "\n",
    "# Plot data\n",
    "layer_nums = sorted(lbl_stats.keys())\n",
    "metrics = {\n",
    "    (0,0): ('num_nodes', 'Average Number of Nodes'),\n",
    "    (0,1): ('num_edges', 'Average Number of Edges'),\n",
    "    (1,0): ('num_activating', 'Average Number of Activating Nodes'),\n",
    "    (1,1): ('avg_activation', 'Average Activation Values')\n",
    "}\n",
    "\n",
    "for (i,j), (metric, title) in metrics.items():\n",
    "    # Base model\n",
    "    avg_base = [np.mean(base_stats[layer][metric]) for layer in layer_nums]\n",
    "    std_base = [np.std(base_stats[layer][metric]) for layer in layer_nums]\n",
    "    \n",
    "    # LBL model\n",
    "    avg_lbl = [np.mean(lbl_stats[layer][metric]) for layer in layer_nums]\n",
    "    std_lbl = [np.std(lbl_stats[layer][metric]) for layer in layer_nums]\n",
    "    \n",
    "    # Plot both models\n",
    "    axes[i,j].errorbar(layer_nums, avg_base, yerr=std_base, marker='o', label='Base Model', \n",
    "                      color='blue', capsize=5)\n",
    "    axes[i,j].errorbar(layer_nums, avg_lbl, yerr=std_lbl, marker='s', label='LBL Model', \n",
    "                      color='red', capsize=5)\n",
    "    \n",
    "    axes[i,j].set_title(title)\n",
    "    axes[i,j].set_xlabel('Layer')\n",
    "    axes[i,j].set_ylabel(title.split()[-1])\n",
    "    axes[i,j].grid(True)\n",
    "    axes[i,j].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create two separate figures\n",
    "fig_base, axes_base = plt.subplots(2, 2, figsize=(20, 16))\n",
    "fig_lbl, axes_lbl = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "fig_base.suptitle('Base Model Distribution Analysis', fontsize=16, y=1.02)\n",
    "fig_lbl.suptitle('LBL Model Distribution Analysis', fontsize=16, y=1.02)\n",
    "\n",
    "layer_nums = range(8)  # 8 layers (0-7)\n",
    "metrics = {\n",
    "    (0,0): ('num_nodes', 'Average Number of Nodes'),\n",
    "    (0,1): ('num_edges', 'Average Number of Edges'),\n",
    "    (1,0): ('num_activating', 'Average Number of Activating Nodes'),\n",
    "    (1,1): ('avg_activation', 'Average Activation Values')\n",
    "}\n",
    "\n",
    "# Helper function to create boxplot\n",
    "def create_model_boxplots(axes, stats, model_name, color):\n",
    "    for (i,j), (metric, title) in metrics.items():\n",
    "        # Prepare data\n",
    "        data = [stats[layer][metric] for layer in layer_nums]\n",
    "        \n",
    "        # Create boxplot\n",
    "        bp = axes[i,j].boxplot(data,\n",
    "                              patch_artist=True,\n",
    "                              showfliers=True,\n",
    "                              medianprops=dict(color=\"black\", linewidth=2),\n",
    "                              boxprops=dict(facecolor=color, alpha=0.8, linewidth=1.5),\n",
    "                              whiskerprops=dict(linewidth=1.5),\n",
    "                              capprops=dict(linewidth=1.5),\n",
    "                              flierprops=dict(marker='o', markerfacecolor=color, markersize=6, alpha=0.6))\n",
    "        \n",
    "        # Use log scale for nodes and edges\n",
    "        if metric in ['num_nodes', 'num_edges']:\n",
    "            axes[i,j].set_yscale('log')\n",
    "        \n",
    "        # Customize plot\n",
    "        axes[i,j].yaxis.grid(True, linestyle='--', alpha=0.7)\n",
    "        axes[i,j].set_axisbelow(True)\n",
    "        \n",
    "        axes[i,j].set_title(f\"{title} - {model_name}\", pad=20)\n",
    "        axes[i,j].set_xlabel('Layer', labelpad=10)\n",
    "        axes[i,j].set_ylabel(title.split()[-1], labelpad=10)\n",
    "        \n",
    "        # Set x-ticks\n",
    "        axes[i,j].set_xticks(range(1, len(layer_nums) + 1))\n",
    "        axes[i,j].set_xticklabels([f'Layer {l}' for l in layer_nums])\n",
    "        \n",
    "        # Add summary statistics as text\n",
    "        stats_text = f\"Mean: {np.mean([np.mean(d) for d in data]):.2f}\\n\"\n",
    "        stats_text += f\"Median: {np.mean([np.median(d) for d in data]):.2f}\\n\"\n",
    "        stats_text += f\"Max: {np.max([np.max(d) for d in data]):.2f}\"\n",
    "        axes[i,j].text(0.02, 0.98, stats_text,\n",
    "                      transform=axes[i,j].transAxes,\n",
    "                      verticalalignment='top',\n",
    "                      bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Create plots for each model\n",
    "create_model_boxplots(axes_base, base_stats, 'Base Model', 'lightblue')\n",
    "create_model_boxplots(axes_lbl, lbl_stats, 'LBL Model', 'lightcoral')\n",
    "\n",
    "# Adjust layout\n",
    "plt.figure(fig_base.number)\n",
    "plt.tight_layout(pad=2.0)\n",
    "\n",
    "plt.figure(fig_lbl.number)\n",
    "plt.tight_layout(pad=2.0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics for verification\n",
    "for model_name, stats in [(\"Base Model\", base_stats), (\"LBL Model\", lbl_stats)]:\n",
    "    print(f\"\\n{model_name} Statistics:\")\n",
    "    for metric in ['num_nodes', 'num_edges', 'num_activating', 'avg_activation']:\n",
    "        all_values = []\n",
    "        for layer in layer_nums:\n",
    "            all_values.extend(stats[layer][metric])\n",
    "        print(f\"\\n{metric}:\")\n",
    "        print(f\"Overall - Min: {np.min(all_values):.3f}, Max: {np.max(all_values):.3f}\")\n",
    "        print(f\"Overall - Mean: {np.mean(all_values):.3f}, Median: {np.median(all_values):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Initialize data structures for each model\n",
    "base_stats = defaultdict(lambda: defaultdict(list))\n",
    "lbl_stats = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Load and analyze graphs from each directory\n",
    "for model_dir, stats in [('neuron_graphs/base_model', base_stats), \n",
    "                        ('neuron_graphs/lbl', lbl_stats)]:\n",
    "    # Iterate through layer directories\n",
    "    for layer_dir in sorted(os.listdir(model_dir)):\n",
    "        if not layer_dir.startswith('layer'):\n",
    "            continue\n",
    "            \n",
    "        layer = int(layer_dir.split('_')[1])  # Extract layer number\n",
    "        layer_path = os.path.join(model_dir, layer_dir)\n",
    "        \n",
    "        # Process each graph in the layer\n",
    "        for graph_file in os.listdir(layer_path):\n",
    "            if not graph_file.endswith('_graph.json'):\n",
    "                continue\n",
    "                \n",
    "            with open(os.path.join(layer_path, graph_file), 'r') as f:\n",
    "                graph = json.load(f)\n",
    "            \n",
    "            # Calculate graph metrics\n",
    "            nodes = graph['nodes']\n",
    "            edges = graph['edges']\n",
    "            \n",
    "            # Basic counts\n",
    "            stats[layer]['total_nodes'].append(len(nodes))\n",
    "            stats[layer]['total_edges'].append(len(edges))\n",
    "            \n",
    "            # Activation metrics\n",
    "            activating_nodes = [n for n, data in nodes.items() \n",
    "                              if data.get('is_activating', False)]\n",
    "            stats[layer]['activating_nodes'].append(len(activating_nodes))\n",
    "            \n",
    "            # Get activation values for activating nodes\n",
    "            activation_values = [data.get('activation', 0) for data in nodes.values() \n",
    "                               if data.get('is_activating', False)]\n",
    "            avg_activation = np.mean(activation_values) if activation_values else 0\n",
    "            stats[layer]['avg_activation'].append(avg_activation)\n",
    "            \n",
    "            # Graph density\n",
    "            if len(nodes) > 1:\n",
    "                density = (2 * len(edges)) / (len(nodes) * (len(nodes) - 1))\n",
    "            else:\n",
    "                density = 0\n",
    "            stats[layer]['density'].append(density)\n",
    "\n",
    "# Calculate mean and std for each metric\n",
    "metrics = ['total_nodes', 'total_edges', 'activating_nodes', 'avg_activation', 'density']\n",
    "metric_names = {\n",
    "    'total_nodes': 'Total Nodes',\n",
    "    'total_edges': 'Total Edges', \n",
    "    'activating_nodes': 'Activating Nodes',\n",
    "    'avg_activation': 'Average Activation',\n",
    "    'density': 'Graph Density'\n",
    "}\n",
    "\n",
    "# Set up the figure\n",
    "sns.set_theme()\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Layer-wise Graph Analysis: Base vs LBL Model', fontsize=16)\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 3, idx % 3] if idx < 5 else None\n",
    "    if ax is None:\n",
    "        continue\n",
    "        \n",
    "    layers = sorted(base_stats.keys())\n",
    "    \n",
    "    # Calculate means and standard deviations\n",
    "    base_means = [np.mean(base_stats[l][metric]) for l in layers]\n",
    "    base_stds = [np.std(base_stats[l][metric]) for l in layers]\n",
    "    lbl_means = [np.mean(lbl_stats[l][metric]) for l in layers]\n",
    "    lbl_stds = [np.std(lbl_stats[l][metric]) for l in layers]\n",
    "    \n",
    "    # Plot with error bands\n",
    "    ax.plot(layers, base_means, 'b-o', label='Base Model', linewidth=2)\n",
    "    ax.fill_between(layers, \n",
    "                   np.array(base_means) - np.array(base_stds),\n",
    "                   np.array(base_means) + np.array(base_stds),\n",
    "                   alpha=0.2, color='blue')\n",
    "    \n",
    "    ax.plot(layers, lbl_means, 'r-o', label='LBL Model', linewidth=2)\n",
    "    ax.fill_between(layers,\n",
    "                   np.array(lbl_means) - np.array(lbl_stds),\n",
    "                   np.array(lbl_means) + np.array(lbl_stds),\n",
    "                   alpha=0.2, color='red')\n",
    "    \n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel(metric_names[metric])\n",
    "    ax.set_title(f'{metric_names[metric]} by Layer')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes.flatten()[-1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "for model_name, stats in [(\"Base Model\", base_stats), (\"LBL Model\", lbl_stats)]:\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for metric in metrics:\n",
    "        print(f\"\\n{metric_names[metric]}:\")\n",
    "        for layer in sorted(stats.keys()):\n",
    "            values = stats[layer][metric]\n",
    "            print(f\"  Layer {layer}:\")\n",
    "            print(f\"    Mean: {np.mean(values):.3f}\")\n",
    "            print(f\"    Std:  {np.std(values):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def collect_graph_stats(model_dir):\n",
    "    \"\"\"Collect node counts and their corresponding activation values by layer\"\"\"\n",
    "    layer_data = defaultdict(list)\n",
    "    \n",
    "    # Iterate through layer directories\n",
    "    for layer_dir in sorted(os.listdir(model_dir)):\n",
    "        if not layer_dir.startswith('layer'):\n",
    "            continue\n",
    "            \n",
    "        layer = int(layer_dir.split('_')[1])\n",
    "        layer_path = os.path.join(model_dir, layer_dir)\n",
    "        \n",
    "        # Process each graph in the layer\n",
    "        for graph_file in os.listdir(layer_path):\n",
    "            if not graph_file.endswith('_graph.json'):\n",
    "                continue\n",
    "                \n",
    "            with open(os.path.join(layer_path, graph_file), 'r') as f:\n",
    "                graph = json.load(f)\n",
    "            \n",
    "            activating_nodes = []\n",
    "            activation_values = []\n",
    "            \n",
    "            # Get activating nodes and their values\n",
    "            for node_id, node_data in graph['nodes'].items():\n",
    "                if node_data.get('is_activating', False):\n",
    "                    activating_nodes.append(node_id)\n",
    "                    activation_values.append(node_data.get('activation', 0))\n",
    "            \n",
    "            layer_data[layer].append({\n",
    "                'total_nodes': len(graph['nodes']),\n",
    "                'num_activating': len(activating_nodes),\n",
    "                'max_activation': max(activation_values) if activation_values else 0,\n",
    "                'mean_activation': np.mean(activation_values) if activation_values else 0\n",
    "            })\n",
    "    \n",
    "    return layer_data\n",
    "\n",
    "# Collect data\n",
    "print(\"Loading model data...\")\n",
    "base_data = collect_graph_stats('neuron_graphs/base_model')\n",
    "lbl_data = collect_graph_stats('neuron_graphs/lbl')\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Prepare data for plotting\n",
    "def prepare_plot_data(data):\n",
    "    total_nodes = []\n",
    "    num_activating = []\n",
    "    max_activations = []\n",
    "    \n",
    "    for layer_stats in data.values():\n",
    "        for graph in layer_stats:\n",
    "            total_nodes.append(graph['total_nodes'])\n",
    "            num_activating.append(graph['num_activating'])\n",
    "            max_activations.append(graph['max_activation'])\n",
    "    \n",
    "    return np.array(total_nodes), np.array(num_activating), np.array(max_activations)\n",
    "\n",
    "# Plot with size indicating max activation\n",
    "for model_name, data, color in [('Base Model', base_data, 'blue'),\n",
    "                               ('LBL Model', lbl_data, 'red')]:\n",
    "    total_nodes, num_activating, max_activations = prepare_plot_data(data)\n",
    "    \n",
    "    # Normalize sizes for visualization\n",
    "    sizes = (max_activations / max_activations.max() * 200) + 20\n",
    "    \n",
    "    plt.scatter(total_nodes, num_activating, \n",
    "               s=sizes, \n",
    "               alpha=0.5, \n",
    "               color=color, \n",
    "               label=model_name)\n",
    "    \n",
    "    # Add trend line if we have data\n",
    "    if len(total_nodes) > 0:\n",
    "        z = np.polyfit(total_nodes, num_activating, 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_trend = np.linspace(min(total_nodes), max(total_nodes), 100)\n",
    "        plt.plot(x_trend, p(x_trend), '--', color=color, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Total Nodes in Graph')\n",
    "plt.ylabel('Number of Activating Nodes')\n",
    "plt.title('Node Relationships\\n(Point size indicates maximum activation value)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "for model_name, data in [(\"Base Model\", base_data), (\"LBL Model\", lbl_data)]:\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    total_nodes, num_activating, max_activations = prepare_plot_data(data)\n",
    "    \n",
    "    print(f\"Average graph size: {np.mean(total_nodes):.2f} ± {np.std(total_nodes):.2f} nodes\")\n",
    "    print(f\"Average activating nodes: {np.mean(num_activating):.2f} ± {np.std(num_activating):.2f}\")\n",
    "    print(f\"Average max activation: {np.mean(max_activations):.2f} ± {np.std(max_activations):.2f}\")\n",
    "    \n",
    "    # Calculate correlations only if we have data\n",
    "    if len(total_nodes) > 0:\n",
    "        node_act_corr = np.corrcoef(total_nodes, num_activating)[0,1]\n",
    "        print(f\"Correlation (nodes vs activating): {node_act_corr:.3f}\")\n",
    "        \n",
    "        # Calculate correlation with max activation for graphs that have activating nodes\n",
    "        active_mask = num_activating > 0\n",
    "        if np.any(active_mask):\n",
    "            max_act_corr = np.corrcoef(total_nodes[active_mask], \n",
    "                                     max_activations[active_mask])[0,1]\n",
    "            print(f\"Correlation (nodes vs max activation): {max_act_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Initialize variables\n",
    "base_dir = 'neuron_graphs'\n",
    "models = ['base_model', 'lbl']\n",
    "num_layers = 8\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "gs = plt.GridSpec(3, 2, figure=fig)\n",
    "\n",
    "# Initialize data storage\n",
    "all_metrics = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Process graphs and collect data\n",
    "for model in models:\n",
    "    print(f\"\\nProcessing {model}\")\n",
    "    for layer in range(num_layers):\n",
    "        layer_dir = os.path.join(base_dir, model, f'layer_{layer}')\n",
    "        if not os.path.exists(layer_dir):\n",
    "            continue\n",
    "            \n",
    "        graph_files = [f for f in os.listdir(layer_dir) if f.endswith('_graph.json')]\n",
    "        for graph_file in tqdm(graph_files):\n",
    "            # Load graph\n",
    "            with open(os.path.join(layer_dir, graph_file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "            G = nx.DiGraph()\n",
    "            for node, attrs in data['nodes'].items():\n",
    "                G.add_node(node, **attrs)\n",
    "            for edge_str, attrs in data['edges'].items():\n",
    "                source, target = edge_str.split('->')\n",
    "                G.add_edge(source, target, **attrs)\n",
    "            \n",
    "            # Collect metrics\n",
    "            edge_weights = [d['weight'] for u, v, d in G.edges(data=True)]\n",
    "            activating_nodes = [n for n, d in G.nodes(data=True) if d.get('is_activating', False)]\n",
    "            \n",
    "            metrics = {\n",
    "                'avg_edge_weight': np.mean(edge_weights) if edge_weights else 0,\n",
    "                'avg_out_degree': np.mean([G.out_degree(n) for n in G.nodes()]),\n",
    "                'avg_in_degree': np.mean([G.in_degree(n) for n in G.nodes()]),\n",
    "                'edges_to_activating': sum(1 for u, v in G.edges() if v in activating_nodes),\n",
    "                'density': nx.density(G),\n",
    "                'transitivity': nx.transitivity(G)\n",
    "            }\n",
    "            \n",
    "            for metric_name, value in metrics.items():\n",
    "                all_metrics[model][f'layer_{layer}_{metric_name}'].append(value)\n",
    "\n",
    "# 1. Edge Weight Distribution\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "for model in models:\n",
    "    model_data = pd.DataFrame(all_metrics[model])\n",
    "    for layer in range(num_layers):\n",
    "        weights = [col for col in model_data.columns if f'layer_{layer}_avg_edge_weight' in col]\n",
    "        if weights:\n",
    "            sns.kdeplot(data=model_data[weights].mean(axis=1), \n",
    "                       label=f'{model} Layer {layer}', ax=ax1)\n",
    "ax1.set_title('Edge Weight Distribution by Layer and Model')\n",
    "ax1.set_xlabel('Average Edge Weight')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 2. Connectivity Patterns\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "metrics = ['avg_out_degree', 'avg_in_degree', 'edges_to_activating']\n",
    "for model in models:\n",
    "    model_data = pd.DataFrame(all_metrics[model])\n",
    "    layer_means = []\n",
    "    for layer in range(num_layers):\n",
    "        means = [model_data[col].mean() for col in model_data.columns \n",
    "                if any(f'layer_{layer}_{m}' in col for m in metrics)]\n",
    "        layer_means.append(means)\n",
    "    layer_means = np.array(layer_means)\n",
    "    ax2.plot(range(num_layers), layer_means, label=f'{model}', marker='o')\n",
    "ax2.set_title('Connectivity Patterns Across Layers')\n",
    "ax2.set_xlabel('Layer')\n",
    "ax2.set_ylabel('Average Value')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Network Structure Metrics\n",
    "data_for_plot = []\n",
    "for model in models:\n",
    "    for metric in ['density', 'transitivity']:\n",
    "        for layer in range(num_layers):\n",
    "            metric_vals = [v for k, v in all_metrics[model].items() \n",
    "                         if f'layer_{layer}_{metric}' in k]\n",
    "            if metric_vals:\n",
    "                mean_val = np.mean(metric_vals)\n",
    "                data_for_plot.append({\n",
    "                    'Model': model,\n",
    "                    'Layer': str(layer),\n",
    "                    'Metric': metric,\n",
    "                    'Value': mean_val\n",
    "                })\n",
    "\n",
    "plot_df = pd.DataFrame(data_for_plot)\n",
    "\n",
    "# Split into two subplots\n",
    "for i, metric in enumerate(['density', 'transitivity']):\n",
    "    metric_data = plot_df[plot_df['Metric'] == metric]\n",
    "    ax = fig.add_subplot(gs[1, i])\n",
    "    sns.barplot(data=metric_data, x='Layer', y='Value', hue='Model', ax=ax)\n",
    "    ax.set_title(f'{metric.capitalize()} by Layer')\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel('Value')\n",
    "    if i > 0:  # Only keep one legend\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "# 4. Edge Pattern Analysis\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "edge_patterns = []\n",
    "\n",
    "# Collect edge patterns\n",
    "for model in models:\n",
    "    print(f\"\\nCollecting edge patterns for {model}\")\n",
    "    for layer in range(num_layers):\n",
    "        layer_dir = os.path.join(base_dir, model, f'layer_{layer}')\n",
    "        if not os.path.exists(layer_dir):\n",
    "            continue\n",
    "            \n",
    "        for graph_file in os.listdir(layer_dir):\n",
    "            if not graph_file.endswith('_graph.json'):\n",
    "                continue\n",
    "                \n",
    "            with open(os.path.join(layer_dir, graph_file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            # Collect patterns for this graph\n",
    "            patterns = defaultdict(list)\n",
    "            for edge_str, attrs in data['edges'].items():\n",
    "                source, target = edge_str.split('->')\n",
    "                source_type = 'activating' if data['nodes'][source].get('is_activating', False) else 'context'\n",
    "                target_type = 'activating' if data['nodes'][target].get('is_activating', False) else 'context'\n",
    "                pattern_type = f'{source_type}->{target_type}'\n",
    "                patterns[pattern_type].append(attrs['weight'])\n",
    "            \n",
    "            # Store mean weights per pattern type\n",
    "            for pattern_type, weights in patterns.items():\n",
    "                edge_patterns.append({\n",
    "                    'model': model,\n",
    "                    'layer': layer,\n",
    "                    'pattern': pattern_type,\n",
    "                    'mean_weight': np.mean(weights)\n",
    "                })\n",
    "\n",
    "# Convert to DataFrame\n",
    "pattern_df = pd.DataFrame(edge_patterns)\n",
    "pivot_df = pattern_df.pivot_table(\n",
    "    values='mean_weight',\n",
    "    index=['model', 'pattern'],\n",
    "    aggfunc='mean'\n",
    ").reset_index()\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_data = []\n",
    "for m1 in models:\n",
    "    for m2 in models:\n",
    "        m1_patterns = pivot_df[pivot_df['model'] == m1]\n",
    "        m2_patterns = pivot_df[pivot_df['model'] == m2]\n",
    "        \n",
    "        # Match patterns\n",
    "        for p1 in m1_patterns['pattern'].unique():\n",
    "            for p2 in m2_patterns['pattern'].unique():\n",
    "                v1 = m1_patterns[m1_patterns['pattern'] == p1]['mean_weight'].values[0]\n",
    "                v2 = m2_patterns[m2_patterns['pattern'] == p2]['mean_weight'].values[0]\n",
    "                corr_data.append({\n",
    "                    'model1': f\"{m1}\\n{p1}\",\n",
    "                    'model2': f\"{m2}\\n{p2}\",\n",
    "                    'correlation': v1 * v2  # Simplified correlation measure\n",
    "                })\n",
    "\n",
    "corr_df = pd.DataFrame(corr_data)\n",
    "corr_matrix = corr_df.pivot(index='model1', columns='model2', values='correlation')\n",
    "\n",
    "# Plot correlation heatmap\n",
    "sns.heatmap(corr_matrix, \n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            annot=True,\n",
    "            fmt='.2f',\n",
    "            ax=ax4)\n",
    "ax4.set_title('Edge Pattern Correlations')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Graph pattern analysis: Load and pair graphs\n",
    "import json\n",
    "import os\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def load_paired_graphs(base_dir='neuron_graphs', layer_range=range(8)):\n",
    "    \"\"\"Load corresponding graphs from base and ablated models\"\"\"\n",
    "    paired_graphs = defaultdict(dict)\n",
    "    \n",
    "    for layer in layer_range:\n",
    "        base_dir_path = os.path.join(base_dir, 'base_model', f'layer_{layer}')\n",
    "        lbl_dir_path = os.path.join(base_dir, 'lbl', f'layer_{layer}')\n",
    "        \n",
    "        if not (os.path.exists(base_dir_path) and os.path.exists(lbl_dir_path)):\n",
    "            continue\n",
    "            \n",
    "        # Get all neuron IDs from filenames\n",
    "        base_neurons = {f.split('_')[1] for f in os.listdir(base_dir_path) if f.endswith('_graph.json')}\n",
    "        lbl_neurons = {f.split('_')[1] for f in os.listdir(lbl_dir_path) if f.endswith('_graph.json')}\n",
    "        \n",
    "        # Get intersection of neurons present in both models\n",
    "        common_neurons = base_neurons & lbl_neurons\n",
    "        \n",
    "        for neuron in common_neurons:\n",
    "            base_path = os.path.join(base_dir_path, f'l{layer}_{neuron}_graph.json')\n",
    "            lbl_path = os.path.join(lbl_dir_path, f'l{layer}_{neuron}_graph.json')\n",
    "            \n",
    "            if os.path.exists(base_path) and os.path.exists(lbl_path):\n",
    "                with open(base_path, 'r') as f:\n",
    "                    base_data = json.load(f)\n",
    "                with open(lbl_path, 'r') as f:\n",
    "                    lbl_data = json.load(f)\n",
    "                    \n",
    "                paired_graphs[layer][neuron] = {\n",
    "                    'base': base_data,\n",
    "                    'lbl': lbl_data\n",
    "                }\n",
    "    \n",
    "    return paired_graphs\n",
    "\n",
    "# Load paired graphs\n",
    "paired_graphs = load_paired_graphs()\n",
    "print(f\"Loaded graphs for {len(paired_graphs)} layers\")\n",
    "for layer, neurons in paired_graphs.items():\n",
    "    print(f\"Layer {layer}: {len(neurons)} paired neurons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Testing Refined sequence analysis\n",
    "def print_sequence_details(seqs, model_name):\n",
    "    \"\"\"Print detailed information about sequences\"\"\"\n",
    "    print(f\"\\n{model_name} Sequences:\")\n",
    "    for i, seq in enumerate(seqs, 1):\n",
    "        print(f\"\\nSequence {i}:\")\n",
    "        print(f\"Tokens: {' -> '.join(seq['path'])}\")\n",
    "        print(f\"Weights: {[f'{w:.3f}' for w in seq['weights']]}\")\n",
    "        print(f\"Mean weight: {seq['mean_weight']:.3f}\")\n",
    "        print(f\"Activating token: {seq['activating_token']}\")\n",
    "\n",
    "def analyze_activating_tokens(base_seqs, lbl_seqs):\n",
    "    \"\"\"Compare activating tokens between models\"\"\"\n",
    "    base_activating = {seq['activating_token'] for seq in base_seqs}\n",
    "    lbl_activating = {seq['activating_token'] for seq in lbl_seqs}\n",
    "    \n",
    "    print(\"\\nActivating Token Analysis:\")\n",
    "    print(f\"Base model unique activating tokens: {len(base_activating)}\")\n",
    "    print(f\"Ablated model unique activating tokens: {len(lbl_activating)}\")\n",
    "    print(f\"Common activating tokens: {len(base_activating & lbl_activating)}\")\n",
    "    \n",
    "    if base_activating & lbl_activating:\n",
    "        print(\"\\nCommon activating tokens:\")\n",
    "        for token in base_activating & lbl_activating:\n",
    "            print(f\"- {token}\")\n",
    "\n",
    "def find_similar_sequences(base_seqs, lbl_seqs, similarity_threshold=0.5):\n",
    "    \"\"\"Find sequences that share similar patterns even if not identical\"\"\"\n",
    "    similar_pairs = []\n",
    "    \n",
    "    for base_seq in base_seqs:\n",
    "        for lbl_seq in lbl_seqs:\n",
    "            # Check if they share the same activating token\n",
    "            if base_seq['activating_token'] == lbl_seq['activating_token']:\n",
    "                # Convert paths to sets for partial matching\n",
    "                base_set = set(base_seq['path'])\n",
    "                lbl_set = set(lbl_seq['path'])\n",
    "                \n",
    "                # Calculate Jaccard similarity\n",
    "                similarity = len(base_set & lbl_set) / len(base_set | lbl_set)\n",
    "                \n",
    "                if similarity >= similarity_threshold:\n",
    "                    similar_pairs.append({\n",
    "                        'base_seq': base_seq,\n",
    "                        'lbl_seq': lbl_seq,\n",
    "                        'similarity': similarity,\n",
    "                        'common_tokens': base_set & lbl_set\n",
    "                    })\n",
    "    \n",
    "    return similar_pairs\n",
    "\n",
    "# Analyze with lower threshold and more detail\n",
    "for layer in list(paired_graphs.keys())[:2]:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Analyzing Layer {layer}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    for neuron in list(paired_graphs[layer].keys())[:3]:\n",
    "        print(f\"\\nNeuron {neuron}:\")\n",
    "        \n",
    "        # Extract sequences with lower threshold\n",
    "        base_seqs = extract_token_sequences(\n",
    "            paired_graphs[layer][neuron]['base'],\n",
    "            min_weight=0.05,  # Lower threshold\n",
    "            max_length=3      # Shorter sequences\n",
    "        )\n",
    "        lbl_seqs = extract_token_sequences(\n",
    "            paired_graphs[layer][neuron]['lbl'],\n",
    "            min_weight=0.05,\n",
    "            max_length=3\n",
    "        )\n",
    "        \n",
    "        # Print sequence details\n",
    "        print_sequence_details(base_seqs, \"Base Model\")\n",
    "        print_sequence_details(lbl_seqs, \"Ablated Model\")\n",
    "        \n",
    "        # Analyze activating tokens\n",
    "        analyze_activating_tokens(base_seqs, lbl_seqs)\n",
    "        \n",
    "        # Find similar sequences\n",
    "        similar_pairs = find_similar_sequences(base_seqs, lbl_seqs)\n",
    "        if similar_pairs:\n",
    "            print(\"\\nSimilar Sequences Found:\")\n",
    "            for pair in similar_pairs:\n",
    "                print(f\"\\nSimilarity: {pair['similarity']:.3f}\")\n",
    "                print(f\"Base sequence: {' -> '.join(pair['base_seq']['path'])}\")\n",
    "                print(f\"Ablated sequence: {' -> '.join(pair['lbl_seq']['path'])}\")\n",
    "                print(f\"Common tokens: {pair['common_tokens']}\")\n",
    "        else:\n",
    "            print(\"\\nNo similar sequences found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "\n",
    "# Load spaCy for token analysis\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def load_paired_graphs(base_dir='neuron_graphs', layer_range=range(8)):\n",
    "    \"\"\"Load corresponding graphs from base and ablated models\"\"\"\n",
    "    paired_graphs = defaultdict(dict)\n",
    "    \n",
    "    for layer in layer_range:\n",
    "        base_dir_path = os.path.join(base_dir, 'base_model', f'layer_{layer}')\n",
    "        lbl_dir_path = os.path.join(base_dir, 'lbl', f'layer_{layer}')\n",
    "        \n",
    "        if not (os.path.exists(base_dir_path) and os.path.exists(lbl_dir_path)):\n",
    "            continue\n",
    "            \n",
    "        # Get all neuron IDs from filenames\n",
    "        base_neurons = {f.split('_')[1] for f in os.listdir(base_dir_path) if f.endswith('_graph.json')}\n",
    "        lbl_neurons = {f.split('_')[1] for f in os.listdir(lbl_dir_path) if f.endswith('_graph.json')}\n",
    "        \n",
    "        # Get intersection of neurons present in both models\n",
    "        common_neurons = base_neurons & lbl_neurons\n",
    "        \n",
    "        for neuron in common_neurons:\n",
    "            base_path = os.path.join(base_dir_path, f'l{layer}_{neuron}_graph.json')\n",
    "            lbl_path = os.path.join(lbl_dir_path, f'l{layer}_{neuron}_graph.json')\n",
    "            \n",
    "            if os.path.exists(base_path) and os.path.exists(lbl_path):\n",
    "                with open(base_path, 'r') as f:\n",
    "                    base_data = json.load(f)\n",
    "                with open(lbl_path, 'r') as f:\n",
    "                    lbl_data = json.load(f)\n",
    "                    \n",
    "                paired_graphs[layer][neuron] = {\n",
    "                    'base': base_data,\n",
    "                    'lbl': lbl_data\n",
    "                }\n",
    "    \n",
    "    return paired_graphs\n",
    "\n",
    "class SequenceAnalyzer:\n",
    "    def __init__(self, paired_graphs):\n",
    "        self.paired_graphs = paired_graphs\n",
    "        self.patterns_data = []\n",
    "        self.token_stats = defaultdict(lambda: defaultdict(int))\n",
    "        self.sequence_stats = defaultdict(lambda: defaultdict(list))\n",
    "        \n",
    "    def get_token_type(self, token: str) -> str:\n",
    "        \"\"\"Classify token type using spaCy\"\"\"\n",
    "        doc = nlp(token.strip())\n",
    "        if len(doc) == 0:\n",
    "            return 'UNKNOWN'\n",
    "        \n",
    "        token = doc[0]\n",
    "        if token.is_punct:\n",
    "            return 'PUNCT'\n",
    "        elif token.is_stop:\n",
    "            return 'STOP'\n",
    "        elif token.pos_ in ['VERB', 'NOUN', 'ADJ', 'ADV']:\n",
    "            return token.pos_\n",
    "        return 'OTHER'\n",
    "    \n",
    "    def extract_sequences(self, graph_data: Dict, min_weight: float = 0.05, max_length: int = 4) -> List[Dict]:\n",
    "        \"\"\"Extract sequences with enhanced metadata\"\"\"\n",
    "        G = nx.DiGraph()\n",
    "        for node, attrs in graph_data['nodes'].items():\n",
    "            G.add_node(node, **attrs)\n",
    "        for edge_str, attrs in graph_data['edges'].items():\n",
    "            source, target = edge_str.split('->')\n",
    "            if attrs['weight'] >= min_weight:\n",
    "                G.add_edge(source, target, **attrs)\n",
    "        \n",
    "        sequences = []\n",
    "        activating_nodes = [n for n, d in G.nodes(data=True) if d.get('is_activating', False)]\n",
    "        \n",
    "        for act_node in activating_nodes:\n",
    "            for node in G.nodes():\n",
    "                if node != act_node:\n",
    "                    for path in nx.all_simple_paths(G, node, act_node, cutoff=max_length):\n",
    "                        weights = []\n",
    "                        token_types = []\n",
    "                        for i in range(len(path)-1):\n",
    "                            weights.append(G.edges[path[i], path[i+1]]['weight'])\n",
    "                            token_types.append(self.get_token_type(path[i]))\n",
    "                        token_types.append(self.get_token_type(act_node))\n",
    "                        \n",
    "                        sequences.append({\n",
    "                            'path': path,\n",
    "                            'weights': weights,\n",
    "                            'token_types': token_types,\n",
    "                            'min_weight': min(weights),\n",
    "                            'mean_weight': np.mean(weights),\n",
    "                            'activating_token': act_node,\n",
    "                            'activating_type': token_types[-1],\n",
    "                            'length': len(path)\n",
    "                        })\n",
    "        return sequences\n",
    "    \n",
    "    def analyze_layer(self, layer: int, base_data: Dict, lbl_data: Dict) -> Dict:\n",
    "        \"\"\"Analyze sequence patterns for a layer\"\"\"\n",
    "        base_seqs = self.extract_sequences(base_data)\n",
    "        lbl_seqs = self.extract_sequences(lbl_data)\n",
    "        \n",
    "        # Collect stats\n",
    "        stats = {\n",
    "            'base_seq_count': len(base_seqs),\n",
    "            'lbl_seq_count': len(lbl_seqs),\n",
    "            'base_avg_length': np.mean([s['length'] for s in base_seqs]) if base_seqs else 0,\n",
    "            'lbl_avg_length': np.mean([s['length'] for s in lbl_seqs]) if lbl_seqs else 0,\n",
    "            'base_avg_weight': np.mean([s['mean_weight'] for s in base_seqs]) if base_seqs else 0,\n",
    "            'lbl_avg_weight': np.mean([s['mean_weight'] for s in lbl_seqs]) if lbl_seqs else 0\n",
    "        }\n",
    "        \n",
    "        # Analyze token types\n",
    "        for seq in base_seqs:\n",
    "            for t_type in seq['token_types']:\n",
    "                self.token_stats['base'][t_type] += 1\n",
    "                \n",
    "        for seq in lbl_seqs:\n",
    "            for t_type in seq['token_types']:\n",
    "                self.token_stats['lbl'][t_type] += 1\n",
    "        \n",
    "        # Collect sequence patterns\n",
    "        for seq in base_seqs:\n",
    "            pattern = '->'.join(seq['token_types'])\n",
    "            self.sequence_stats['base'][pattern].append(seq['mean_weight'])\n",
    "            \n",
    "        for seq in lbl_seqs:\n",
    "            pattern = '->'.join(seq['token_types'])\n",
    "            self.sequence_stats['lbl'][pattern].append(seq['mean_weight'])\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def analyze_all_layers(self):\n",
    "        \"\"\"Analyze all layers and collect statistics\"\"\"\n",
    "        layer_stats = []\n",
    "        \n",
    "        for layer in tqdm(list(self.paired_graphs.keys())):\n",
    "            for neuron, data in self.paired_graphs[layer].items():\n",
    "                stats = self.analyze_layer(layer, data['base'], data['lbl'])\n",
    "                stats.update({'layer': layer, 'neuron': neuron})\n",
    "                layer_stats.append(stats)\n",
    "        \n",
    "        self.layer_stats = pd.DataFrame(layer_stats)\n",
    "        \n",
    "    def plot_analysis(self):\n",
    "        \"\"\"Create comprehensive visualizations\"\"\"\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        gs = plt.GridSpec(3, 2)\n",
    "        \n",
    "        # 1. Sequence counts by layer\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        self.layer_stats.groupby('layer')[['base_seq_count', 'lbl_seq_count']].mean().plot(\n",
    "            kind='bar', ax=ax1)\n",
    "        ax1.set_title('Average Sequence Count by Layer')\n",
    "        ax1.set_ylabel('Count')\n",
    "        \n",
    "        # 2. Sequence lengths\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        self.layer_stats.groupby('layer')[['base_avg_length', 'lbl_avg_length']].mean().plot(\n",
    "            kind='bar', ax=ax2)\n",
    "        ax2.set_title('Average Sequence Length by Layer')\n",
    "        ax2.set_ylabel('Length')\n",
    "        \n",
    "        # 3. Token type distribution\n",
    "        ax3 = fig.add_subplot(gs[1, :])\n",
    "        token_df = pd.DataFrame(self.token_stats)\n",
    "        token_df.plot(kind='bar', ax=ax3)\n",
    "        ax3.set_title('Token Type Distribution')\n",
    "        ax3.set_ylabel('Count')\n",
    "        \n",
    "        # 4. Top sequence patterns\n",
    "        patterns_data = []\n",
    "        for model in ['base', 'lbl']:\n",
    "            for pattern, weights in self.sequence_stats[model].items():\n",
    "                patterns_data.append({\n",
    "                    'model': model,\n",
    "                    'pattern': pattern,\n",
    "                    'count': len(weights),\n",
    "                    'avg_weight': np.mean(weights)\n",
    "                })\n",
    "        \n",
    "        patterns_df = pd.DataFrame(patterns_data)\n",
    "        ax4 = fig.add_subplot(gs[2, :])\n",
    "        top_patterns = patterns_df.nlargest(10, 'count')\n",
    "        sns.barplot(data=top_patterns, x='pattern', y='count', hue='model', ax=ax4)\n",
    "        ax4.set_title('Top Sequence Patterns')\n",
    "        ax4.set_xticklabels(ax4.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return patterns_df\n",
    "\n",
    "# Load graphs and run analysis\n",
    "print(\"Loading paired graphs...\")\n",
    "graphs = load_paired_graphs()\n",
    "print(f\"Loaded {len(graphs)} layers of graphs\")\n",
    "\n",
    "print(\"\\nRunning sequence analysis...\")\n",
    "analyzer = SequenceAnalyzer(graphs)\n",
    "analyzer.analyze_all_layers()\n",
    "patterns_df = analyzer.plot_analysis()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"\\nAverage Sequence Counts by Layer:\")\n",
    "print(analyzer.layer_stats.groupby('layer')[['base_seq_count', 'lbl_seq_count']].mean())\n",
    "\n",
    "print(\"\\nMost Common Pattern Types:\")\n",
    "print(patterns_df.sort_values('count', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Substituion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from model_loader import load_model\n",
    "\n",
    "def examine_graph(graph_path):\n",
    "    \"\"\"Examine and display the structure of a neuron graph\"\"\"\n",
    "    print(f\"\\nExamining graph at: {graph_path}\")\n",
    "    \n",
    "    with open(graph_path) as f:\n",
    "        graph_data = json.load(f)\n",
    "    \n",
    "    print(\"\\nGraph Structure:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nNodes:\")\n",
    "    for node, data in graph_data['nodes'].items():\n",
    "        print(f\"\\nToken: {node}\")\n",
    "        for key, value in data.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "            \n",
    "    print(\"\\nEdges:\")\n",
    "    for edge, data in graph_data['edges'].items():\n",
    "        print(f\"\\n{edge}:\")\n",
    "        for key, value in data.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "            \n",
    "    # Find activating nodes\n",
    "    activating_nodes = []\n",
    "    for node, data in graph_data['nodes'].items():\n",
    "        if data.get('is_activating', False):\n",
    "            activating_nodes.append({\n",
    "                'token': node,\n",
    "                'activation': data.get('activation', 0),\n",
    "                'count': data.get('count', 0)\n",
    "            })\n",
    "    \n",
    "    print(\"\\nActivating Nodes Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    for node in activating_nodes:\n",
    "        print(f\"\\nToken: {node['token']}\")\n",
    "        print(f\"Activation: {node['activation']:.3f}\")\n",
    "        print(f\"Count: {node['count']}\")\n",
    "        \n",
    "        # Find connected nodes\n",
    "        connected = []\n",
    "        for edge in graph_data['edges'].keys():\n",
    "            source, target = edge.split('->')\n",
    "            if target == node['token']:\n",
    "                connected.append(source)\n",
    "            elif source == node['token']:\n",
    "                connected.append(target)\n",
    "                \n",
    "        if connected:\n",
    "            print(\"Connected tokens:\")\n",
    "            for token in connected:\n",
    "                print(f\"  - {token}\")\n",
    "\n",
    "# Load model and examine a specific neuron's graph\n",
    "print(\"Loading model...\")\n",
    "model_path = 'configs/base_model_best.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, config = load_model(model_path, device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# Let's look at layer 0, neuron 5 as before\n",
    "layer = 0\n",
    "neuron = 40\n",
    "graph_dir = Path('neuron_graphs/base_model')\n",
    "graph_path = graph_dir / f'layer_{layer}' / f'l{layer}_n{neuron}_graph.json'\n",
    "\n",
    "examine_graph(graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from model_loader import load_model\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import json\n",
    "import string\n",
    "\n",
    "class TokenSubstitutionAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes neuron activation patterns and finds token substitutions that maintain activation.\n",
    "    Uses BERT for suggesting replacement tokens and tests them in the original model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, device='cuda'):\n",
    "        \"\"\"Initialize with both GPT model for testing and BERT for suggestions\"\"\"\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize GPT-2 tokenizer for our base model\n",
    "        self.gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        \n",
    "        # Initialize BERT for masked token prediction\n",
    "        print(\"Loading BERT model for token suggestions...\")\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased').to(device)\n",
    "        self.bert_model.eval()\n",
    "        \n",
    "        # Create set of punctuation tokens in GPT-2's vocabulary\n",
    "        # This helps us handle punctuation substitutions appropriately\n",
    "        self.punct_tokens = set(self.gpt2_tokenizer.encode(p)[0] \n",
    "                              for p in string.punctuation \n",
    "                              if len(self.gpt2_tokenizer.encode(p)) == 1)\n",
    "    \n",
    "    def load_neuron_graph(self, graph_path):\n",
    "        \"\"\"Load and parse the neuron's activation graph from JSON\"\"\"\n",
    "        print(f\"Loading graph from {graph_path}\")\n",
    "        with open(graph_path) as f:\n",
    "            graph_data = json.load(f)\n",
    "        \n",
    "        # Create a directed graph to represent token relationships\n",
    "        graph = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes (tokens) with their properties\n",
    "        for node, data in graph_data['nodes'].items():\n",
    "            graph.add_node(node, **data)\n",
    "            \n",
    "        # Add edges (token transitions) with weights\n",
    "        for edge_str, data in graph_data['edges'].items():\n",
    "            source, target = edge_str.split('->')\n",
    "            graph.add_edge(source, target, **data)\n",
    "        \n",
    "        print(f\"Loaded graph with {len(graph.nodes)} nodes and {len(graph.edges)} edges\")\n",
    "        return graph\n",
    "    \n",
    "    def get_activation_patterns(self, graph):\n",
    "        \"\"\"Extract patterns of tokens that activate the neuron strongly\"\"\"\n",
    "        patterns = []\n",
    "        # Look through all nodes to find activating tokens\n",
    "        for node, data in graph.nodes(data=True):\n",
    "            if data.get('is_activating', False):\n",
    "                # Get the context before and after the activating token\n",
    "                pre_context = list(graph.predecessors(node))\n",
    "                post_context = list(graph.successors(node))\n",
    "                \n",
    "                patterns.append({\n",
    "                    'token': node,\n",
    "                    'activation': data['activation'],\n",
    "                    'pre_context': pre_context,\n",
    "                    'post_context': post_context\n",
    "                })\n",
    "                \n",
    "        print(f\"Found {len(patterns)} activation patterns\")\n",
    "        return patterns\n",
    "    \n",
    "    def get_bert_predictions(self, pre_context, post_context, original_token, k=10):\n",
    "        \"\"\"Use BERT to predict likely tokens that could replace the original\"\"\"\n",
    "        # Convert context tokens to text, handling empty contexts gracefully\n",
    "        pre_text = ' '.join([t.replace('Ġ', ' ').strip() for t in pre_context]) if pre_context else ''\n",
    "        post_text = ' '.join([t.replace('Ġ', ' ').strip() for t in post_context]) if post_context else ''\n",
    "        \n",
    "        # Create the masked input text based on available context\n",
    "        if pre_text and post_text:\n",
    "            masked_text = f\"{pre_text} [MASK] {post_text}\"\n",
    "        elif pre_text:\n",
    "            masked_text = f\"{pre_text} [MASK]\"\n",
    "        elif post_text:\n",
    "            masked_text = f\"[MASK] {post_text}\"\n",
    "        else:\n",
    "            masked_text = \"[MASK]\"\n",
    "            \n",
    "        print(f\"Getting BERT predictions for: {masked_text}\")\n",
    "        \n",
    "        # Get BERT's token predictions\n",
    "        inputs = self.bert_tokenizer(masked_text, return_tensors='pt').to(self.device)\n",
    "        mask_idx = torch.where(inputs['input_ids'] == self.bert_tokenizer.mask_token_id)[1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(**inputs)\n",
    "            logits = outputs.logits[0, mask_idx]\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Get extra predictions initially so we can filter them\n",
    "            top_k_mult = torch.topk(probs, k * 2)\n",
    "            \n",
    "            suggestions = []\n",
    "            for prob, token_id in zip(top_k_mult.values[0], top_k_mult.indices[0]):\n",
    "                token = self.bert_tokenizer.decode([token_id])\n",
    "                # Convert BERT's token to GPT-2 tokens\n",
    "                gpt2_tokens = self.gpt2_tokenizer.encode(token)\n",
    "                \n",
    "                # Only consider single-token predictions\n",
    "                if len(gpt2_tokens) == 1:\n",
    "                    is_punct = gpt2_tokens[0] in self.punct_tokens\n",
    "                    suggestions.append({\n",
    "                        'token': token,\n",
    "                        'gpt2_id': gpt2_tokens[0],\n",
    "                        'probability': prob.item(),\n",
    "                        'is_punct': is_punct\n",
    "                    })\n",
    "            \n",
    "            # Filter suggestions to match original token type (punctuation or not)\n",
    "            is_original_punct = original_token.strip() in string.punctuation\n",
    "            filtered_suggestions = [\n",
    "                s for s in suggestions \n",
    "                if s['is_punct'] == is_original_punct\n",
    "            ][:k]\n",
    "            \n",
    "            print(f\"Found {len(filtered_suggestions)} suitable predictions\")\n",
    "            for s in filtered_suggestions:\n",
    "                print(f\"  {s['token']} (prob: {s['probability']:.3f})\")\n",
    "            \n",
    "        return filtered_suggestions\n",
    "    \n",
    "    def test_activation(self, pre_context, token_id, post_context, layer, neuron):\n",
    "        \"\"\"Test how strongly a token activates the neuron in its context\"\"\"\n",
    "        # Handle pre-context tokens, converting strings if needed\n",
    "        if not pre_context:\n",
    "            pre_ids = []\n",
    "        elif isinstance(pre_context[0], str):\n",
    "            pre_ids = [self.gpt2_tokenizer.encode(t)[0] for t in pre_context]\n",
    "        else:\n",
    "            pre_ids = pre_context\n",
    "            \n",
    "        # Handle post-context tokens, converting strings if needed\n",
    "        if not post_context:\n",
    "            post_ids = []\n",
    "        elif isinstance(post_context[0], str):\n",
    "            post_ids = [self.gpt2_tokenizer.encode(t)[0] for t in post_context]\n",
    "        else:\n",
    "            post_ids = post_context\n",
    "        \n",
    "        # Create the complete sequence\n",
    "        sequence = pre_ids + [token_id] + post_ids\n",
    "        input_ids = torch.tensor([sequence], device=self.device)\n",
    "        \n",
    "        # Get neuron activations\n",
    "        with torch.no_grad():\n",
    "            outputs, cache = self.model(input_ids, return_cache=True)\n",
    "            \n",
    "        # Get activation at the target position (after pre_context)\n",
    "        target_pos = len(pre_ids)\n",
    "        activation = cache[f'transformer.h.{layer}.mlp'][0, target_pos, neuron].item()\n",
    "        \n",
    "        return activation\n",
    "\n",
    "    def analyze_substitutions(self, graph_path, layer, neuron, \n",
    "                            activation_threshold=0.5, \n",
    "                            relative_threshold=0.5):\n",
    "        \"\"\"Find and test token substitutions that maintain neuron activation\"\"\"\n",
    "        # Load and analyze the neuron's graph\n",
    "        graph = self.load_neuron_graph(graph_path)\n",
    "        patterns = self.get_activation_patterns(graph)\n",
    "        successful_substitutions = []\n",
    "        \n",
    "        print(f\"\\nAnalyzing {len(patterns)} activation patterns...\")\n",
    "        \n",
    "        # Process each activation pattern\n",
    "        for pattern in patterns:\n",
    "            print(f\"\\nAnalyzing pattern for token: {pattern['token']}\")\n",
    "            print(f\"Original activation: {pattern['activation']:.3f}\")\n",
    "            \n",
    "            # Log context information\n",
    "            pre_context_str = ' '.join(pattern['pre_context']) if pattern['pre_context'] else '(no pre-context)'\n",
    "            post_context_str = ' '.join(pattern['post_context']) if pattern['post_context'] else '(no post-context)'\n",
    "            print(f\"Pre-context: {pre_context_str}\")\n",
    "            print(f\"Post-context: {post_context_str}\")\n",
    "            \n",
    "            # Get BERT's suggestions for this context\n",
    "            predictions = self.get_bert_predictions(\n",
    "                pattern['pre_context'],\n",
    "                pattern['post_context'],\n",
    "                pattern['token']\n",
    "            )\n",
    "            \n",
    "            print(f\"Testing {len(predictions)} BERT suggestions...\")\n",
    "            \n",
    "            # Test each predicted token\n",
    "            for pred in predictions:\n",
    "                activation = self.test_activation(\n",
    "                    pattern['pre_context'],\n",
    "                    pred['gpt2_id'],\n",
    "                    pattern['post_context'],\n",
    "                    layer,\n",
    "                    neuron\n",
    "                )\n",
    "                \n",
    "                print(f\"Testing '{pred['token']}': activation = {activation:.3f}\")\n",
    "                \n",
    "                # Check if activation meets our thresholds\n",
    "                if (activation >= pattern['activation'] * relative_threshold and \n",
    "                    activation >= activation_threshold):\n",
    "                    successful_substitutions.append({\n",
    "                        'original_token': pattern['token'],\n",
    "                        'substitute_token': pred['token'],\n",
    "                        'bert_probability': pred['probability'],\n",
    "                        'original_activation': pattern['activation'],\n",
    "                        'new_activation': activation,\n",
    "                        'pre_context': pre_context_str,\n",
    "                        'post_context': post_context_str\n",
    "                    })\n",
    "                    print(f\"Found successful substitution: {pred['token']}\")\n",
    "                    print(f\"New activation: {activation:.3f}\")\n",
    "        \n",
    "        return successful_substitutions\n",
    "\n",
    "thresholds = [\n",
    "    (0.3, 0.3),  # Very permissive\n",
    "    (0.4, 0.4),  # Moderately permissive\n",
    "    (0.5, 0.5)   # Original threshold\n",
    "]\n",
    "\n",
    "print(\"Loading GPT model...\")\n",
    "model_path = 'configs/base_model_best.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, config = load_model(model_path, device)\n",
    "model.eval()\n",
    "\n",
    "analyzer = TokenSubstitutionAnalyzer(model, device)\n",
    "layer = 0\n",
    "neuron = 40\n",
    "graph_path = Path('neuron_graphs/base_model') / f'layer_{layer}' / f'l{layer}_n{neuron}_graph.json'\n",
    "\n",
    "print(f\"\\nAnalyzing Layer {layer} Neuron {neuron} with multiple thresholds\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for activation_threshold, relative_threshold in thresholds:\n",
    "    print(f\"\\nTesting with thresholds:\")\n",
    "    print(f\"Activation threshold: {activation_threshold:.1f}\")\n",
    "    print(f\"Relative threshold: {relative_threshold:.1f}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    substitutions = analyzer.analyze_substitutions(\n",
    "        graph_path, \n",
    "        layer, \n",
    "        neuron,\n",
    "        activation_threshold=activation_threshold,\n",
    "        relative_threshold=relative_threshold\n",
    "    )\n",
    "    \n",
    "    if substitutions:\n",
    "        print(\"\\nSuccessful substitutions:\")\n",
    "        for sub in substitutions:\n",
    "            print(f\"\\nOriginal: '{sub['original_token']}'\")\n",
    "            print(f\"Substitute: '{sub['substitute_token']}'\")\n",
    "            print(f\"Context: {sub['pre_context']}\")\n",
    "            print(f\"BERT probability: {sub['bert_probability']:.3f}\")\n",
    "            print(f\"Original activation: {sub['original_activation']:.3f}\")\n",
    "            print(f\"New activation: {sub['new_activation']:.3f}\")\n",
    "            print(f\"Activation ratio: {sub['new_activation']/sub['original_activation']:.3f}\")\n",
    "    else:\n",
    "        print(\"\\nNo successful substitutions found at these thresholds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Base Model Parallel Ver\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from model_loader import load_model\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import json\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "class BatchedTokenSubstitutionAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes neuron activation patterns and finds meaningful token substitutions.\n",
    "    Uses BERT for suggesting replacements and tests them in the original model,\n",
    "    while considering grammatical context and token types.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, device='cuda'):\n",
    "        \"\"\"Initialize analyzer with models and token patterns\"\"\"\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        \n",
    "        # Initialize BERT for contextual predictions\n",
    "        print(\"Loading BERT model for token suggestions...\")\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased').to(device)\n",
    "        self.bert_model.eval()\n",
    "        \n",
    "        # Define grammatical patterns for different token types\n",
    "        self.grammatical_patterns = {\n",
    "            'punctuation': [\n",
    "                \"In the sentence '{pre} [MASK] {post}', the punctuation\",\n",
    "                \"The text reads '{pre} [MASK] {post}' with proper punctuation\",\n",
    "                \"Complete with punctuation: {pre} [MASK] {post}\"\n",
    "            ],\n",
    "            'article': [\n",
    "                \"Consider '{pre} [MASK] {post}' as a phrase\",\n",
    "                \"The phrase '{pre} [MASK] {post}' uses an article\",\n",
    "                \"Fill in the article: {pre} [MASK] {post}\"\n",
    "            ],\n",
    "            'preposition': [\n",
    "                \"The words '{pre} [MASK] {post}' form a prepositional phrase\",\n",
    "                \"'{pre} [MASK] {post}' shows spatial relationship\",\n",
    "                \"Complete with a preposition: {pre} [MASK] {post}\"\n",
    "            ],\n",
    "            'conjunction': [\n",
    "                \"The sentence '{pre} [MASK] {post}' uses a conjunction\",\n",
    "                \"Join the phrases: {pre} [MASK] {post}\",\n",
    "                \"Connect with a conjunction: {pre} [MASK] {post}\"\n",
    "            ],\n",
    "            'verb': [\n",
    "                \"The action in '{pre} [MASK] {post}' is\",\n",
    "                \"Fill in the verb: {pre} [MASK] {post}\",\n",
    "                \"What happens in '{pre} [MASK] {post}'?\"\n",
    "            ],\n",
    "            'general': [\n",
    "                \"Complete the phrase: {pre} [MASK] {post}\",\n",
    "                \"Fill in: {pre} [MASK] {post}\",\n",
    "                \"What word fits here: {pre} [MASK] {post}\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Create sets of tokens by type for classification\n",
    "        self.punct_tokens = set(self.gpt2_tokenizer.encode(p)[0] \n",
    "                              for p in string.punctuation \n",
    "                              if len(self.gpt2_tokenizer.encode(p)) == 1)\n",
    "        \n",
    "        # Common word sets for token classification\n",
    "        self.articles = {'the', 'a', 'an'}\n",
    "        self.prepositions = {'in', 'on', 'at', 'to', 'from', 'by', 'with', 'above', 'below'}\n",
    "        self.conjunctions = {'and', 'but', 'or', 'nor', 'for', 'yet', 'so'}\n",
    "\n",
    "    def get_token_type(self, token):\n",
    "        \"\"\"\n",
    "        Determine the grammatical type of a token for appropriate prompting.\n",
    "        Uses both form and context to identify token types.\n",
    "        \"\"\"\n",
    "        # Clean the token for analysis\n",
    "        cleaned_token = token.strip().lower()\n",
    "        \n",
    "        # Check various token types\n",
    "        if cleaned_token in string.punctuation:\n",
    "            return 'punctuation'\n",
    "        if cleaned_token in self.articles:\n",
    "            return 'article'\n",
    "        if cleaned_token in self.prepositions:\n",
    "            return 'preposition'\n",
    "        if cleaned_token in self.conjunctions:\n",
    "            return 'conjunction'\n",
    "        \n",
    "        # Could add more sophisticated token type detection here\n",
    "        return 'general'\n",
    "\n",
    "    def load_neuron_graph(self, graph_path):\n",
    "        \"\"\"Load and parse the neuron's activation graph from JSON\"\"\"\n",
    "        with open(graph_path) as f:\n",
    "            graph_data = json.load(f)\n",
    "        \n",
    "        # Create a directed graph to represent token relationships\n",
    "        graph = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes with their properties\n",
    "        for node, data in graph_data['nodes'].items():\n",
    "            graph.add_node(node, **data)\n",
    "            \n",
    "        # Add edges with weights    \n",
    "        for edge_str, data in graph_data['edges'].items():\n",
    "            source, target = edge_str.split('->')\n",
    "            graph.add_edge(source, target, **data)\n",
    "            \n",
    "        return graph\n",
    "\n",
    "    def get_activation_patterns(self, graph):\n",
    "        \"\"\"Extract activating token patterns from the graph\"\"\"\n",
    "        patterns = []\n",
    "        for node, data in graph.nodes(data=True):\n",
    "            if data.get('is_activating', False):\n",
    "                # Get predecessor and successor nodes for context\n",
    "                pre_context = list(graph.predecessors(node))\n",
    "                post_context = list(graph.successors(node))\n",
    "                \n",
    "                patterns.append({\n",
    "                    'token': node,\n",
    "                    'activation': data['activation'],\n",
    "                    'pre_context': pre_context,\n",
    "                    'post_context': post_context\n",
    "                })\n",
    "        return patterns\n",
    "\n",
    "    def get_bert_predictions_with_grammar(self, pre_context, post_context, original_token, k=10):\n",
    "        \"\"\"\n",
    "        Get BERT predictions using grammatical prompting patterns.\n",
    "        Combines predictions from multiple grammatical contexts.\n",
    "        \"\"\"\n",
    "        # Determine token type and get appropriate patterns\n",
    "        token_type = self.get_token_type(original_token)\n",
    "        patterns = self.grammatical_patterns[token_type]\n",
    "        \n",
    "        # Process context text\n",
    "        pre_text = ' '.join([t.replace('Ġ', ' ').strip() for t in pre_context]) if pre_context else ''\n",
    "        post_text = ' '.join([t.replace('Ġ', ' ').strip() for t in post_context]) if post_context else ''\n",
    "        \n",
    "        combined_predictions = []\n",
    "        \n",
    "        # Try each grammatical pattern\n",
    "        for pattern in patterns:\n",
    "            masked_text = pattern.format(pre=pre_text, post=post_text).strip()\n",
    "            inputs = self.bert_tokenizer(masked_text, return_tensors='pt').to(self.device)\n",
    "            mask_idx = torch.where(inputs['input_ids'] == self.bert_tokenizer.mask_token_id)[1]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.bert_model(**inputs)\n",
    "                logits = outputs.logits[0, mask_idx]\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                \n",
    "                top_k = torch.topk(probs, k * 2)\n",
    "                \n",
    "                for prob, token_id in zip(top_k.values[0], top_k.indices[0]):\n",
    "                    token = self.bert_tokenizer.decode([token_id])\n",
    "                    gpt2_tokens = self.gpt2_tokenizer.encode(token)\n",
    "                    \n",
    "                    # Only keep single-token predictions\n",
    "                    if len(gpt2_tokens) == 1:\n",
    "                        combined_predictions.append({\n",
    "                            'token': token,\n",
    "                            'gpt2_id': gpt2_tokens[0],\n",
    "                            'probability': prob.item(),\n",
    "                            'pattern': pattern\n",
    "                        })\n",
    "        \n",
    "        # Sort by probability and take top k unique predictions\n",
    "        unique_predictions = {}\n",
    "        for pred in sorted(combined_predictions, key=lambda x: x['probability'], reverse=True):\n",
    "            if pred['token'] not in unique_predictions:\n",
    "                unique_predictions[pred['token']] = pred\n",
    "                if len(unique_predictions) == k:\n",
    "                    break\n",
    "                    \n",
    "        return list(unique_predictions.values())\n",
    "\n",
    "    def test_activation(self, pre_context, token_id, post_context, layer, neuron):\n",
    "        \"\"\"Test how strongly a token activates the neuron in context\"\"\"\n",
    "        # Handle pre-context tokens\n",
    "        if not pre_context:\n",
    "            pre_ids = []\n",
    "        elif isinstance(pre_context[0], str):\n",
    "            pre_ids = [self.gpt2_tokenizer.encode(t)[0] for t in pre_context]\n",
    "        else:\n",
    "            pre_ids = pre_context\n",
    "            \n",
    "        # Handle post-context tokens\n",
    "        if not post_context:\n",
    "            post_ids = []\n",
    "        elif isinstance(post_context[0], str):\n",
    "            post_ids = [self.gpt2_tokenizer.encode(t)[0] for t in post_context]\n",
    "        else:\n",
    "            post_ids = post_context\n",
    "        \n",
    "        # Create sequence and get activation\n",
    "        sequence = pre_ids + [token_id] + post_ids\n",
    "        input_ids = torch.tensor([sequence], device=self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs, cache = self.model(input_ids, return_cache=True)\n",
    "            activation = cache[f'transformer.h.{layer}.mlp'][0, len(pre_ids), neuron].item()\n",
    "        \n",
    "        return activation\n",
    "\n",
    "    def analyze_single_neuron(self, layer, neuron, graph_path, \n",
    "                            activation_threshold=0.3, relative_threshold=0.3):\n",
    "        \"\"\"Analyze substitutions for a single neuron\"\"\"\n",
    "        try:\n",
    "            graph = self.load_neuron_graph(graph_path)\n",
    "            patterns = self.get_activation_patterns(graph)\n",
    "            successful_substitutions = []\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                predictions = self.get_bert_predictions_with_grammar(\n",
    "                    pattern['pre_context'],\n",
    "                    pattern['post_context'],\n",
    "                    pattern['token']\n",
    "                )\n",
    "                \n",
    "                for pred in predictions:\n",
    "                    activation = self.test_activation(\n",
    "                        pattern['pre_context'],\n",
    "                        pred['gpt2_id'],\n",
    "                        pattern['post_context'],\n",
    "                        layer,\n",
    "                        neuron\n",
    "                    )\n",
    "                    \n",
    "                    # Check if activation meets thresholds\n",
    "                    if (activation >= pattern['activation'] * relative_threshold and \n",
    "                        activation >= activation_threshold):\n",
    "                        successful_substitutions.append({\n",
    "                            'original_token': pattern['token'],\n",
    "                            'substitute_token': pred['token'],\n",
    "                            'bert_probability': pred['probability'],\n",
    "                            'original_activation': pattern['activation'],\n",
    "                            'new_activation': activation,\n",
    "                            'pre_context': pattern['pre_context'],\n",
    "                            'post_context': pattern['post_context'],\n",
    "                            'pattern_used': pred['pattern']\n",
    "                        })\n",
    "            \n",
    "            return successful_substitutions\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing neuron {layer}_{neuron}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def analyze_neuron_batch(self, layer_range, neuron_range, batch_size=10):\n",
    "        \"\"\"Analyze neurons in batches for efficient processing\"\"\"\n",
    "        results = {}\n",
    "        neurons_to_analyze = []\n",
    "        \n",
    "        # Collect valid neurons\n",
    "        print(\"Collecting valid neurons...\")\n",
    "        for layer in layer_range:\n",
    "            for neuron in neuron_range:\n",
    "                graph_path = Path('neuron_graphs/base_model') / f'layer_{layer}' / f'l{layer}_n{neuron}_graph.json'\n",
    "                if graph_path.exists():\n",
    "                    neurons_to_analyze.append((layer, neuron, graph_path))\n",
    "\n",
    "        # Process in batches\n",
    "        total_batches = (len(neurons_to_analyze) + batch_size - 1) // batch_size\n",
    "        print(f\"\\nProcessing {len(neurons_to_analyze)} neurons in {total_batches} batches\")\n",
    "        \n",
    "        for batch_idx in tqdm(range(0, len(neurons_to_analyze), batch_size), desc=\"Processing batches\"):\n",
    "            batch = neurons_to_analyze[batch_idx:batch_idx + batch_size]\n",
    "            batch_results = {}\n",
    "            \n",
    "            for layer, neuron, graph_path in batch:\n",
    "                try:\n",
    "                    substitutions = self.analyze_single_neuron(layer, neuron, graph_path)\n",
    "                    if substitutions:\n",
    "                        batch_results[f\"l{layer}_n{neuron}\"] = substitutions\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError analyzing layer {layer} neuron {neuron}: {str(e)}\")\n",
    "            \n",
    "            # Update and save results\n",
    "            results.update(batch_results)\n",
    "            if (batch_idx + batch_size) % 50 == 0:\n",
    "                self._save_intermediate_results(results, batch_idx + batch_size)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _save_intermediate_results(self, results, neurons_processed):\n",
    "        \"\"\"Save checkpoint of results during processing\"\"\"\n",
    "        output_path = Path(f'substitution_results_checkpoint_{neurons_processed}.json')\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"\\nSaved checkpoint after processing {neurons_processed} neurons\")\n",
    "\n",
    "    def get_substitution_statistics(self, results):\n",
    "        \"\"\"Calculate comprehensive statistics about substitution patterns\"\"\"\n",
    "        stats = {\n",
    "            'total_neurons_analyzed': 0,\n",
    "            'neurons_with_substitutions': 0,\n",
    "            'substitutions_by_type': defaultdict(int),\n",
    "            'average_activation_ratio': [],\n",
    "            'substitutions_by_layer': defaultdict(int),\n",
    "            'bert_probability_distribution': [],\n",
    "            'pattern_effectiveness': defaultdict(list)\n",
    "        }\n",
    "        \n",
    "        for neuron_id, substitutions in results.items():\n",
    "            layer = int(neuron_id.split('_')[0][1:])\n",
    "            stats['total_neurons_analyzed'] += 1\n",
    "            \n",
    "            if substitutions:\n",
    "                stats['neurons_with_substitutions'] += 1\n",
    "                stats['substitutions_by_layer'][layer] += len(substitutions)\n",
    "                \n",
    "                for sub in substitutions:\n",
    "                    token_type = self.get_token_type(sub['original_token'])\n",
    "                    stats['substitutions_by_type'][token_type] += 1\n",
    "                    \n",
    "                    activation_ratio = sub['new_activation'] / sub['original_activation']\n",
    "                    stats['average_activation_ratio'].append(activation_ratio)\n",
    "                    stats['bert_probability_distribution'].append(sub['bert_probability'])\n",
    "                    stats['pattern_effectiveness'][sub['pattern_used']].append(activation_ratio)\n",
    "        \n",
    "        # Calculate averages\n",
    "        if stats['average_activation_ratio']:\n",
    "            stats['average_activation_ratio'] = np.mean(stats['average_activation_ratio'])\n",
    "            stats['bert_probability_mean'] = np.mean(stats['bert_probability_distribution'])\n",
    "            \n",
    "            # Calculate effectiveness of each pattern\n",
    "            stats['pattern_effectiveness'] = {\n",
    "                pattern: np.mean(ratios)\n",
    "                for pattern, ratios in stats['pattern_effectiveness'].items()\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "def run_batch_analysis():\n",
    "    \"\"\"Run complete analysis for neurons 0-199 in each layer\"\"\"\n",
    "    print(\"Loading GPT model...\")\n",
    "    model_path = 'configs/base_model_best.pt'\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, config = load_model(model_path, device)\n",
    "    model.eval()\n",
    "\n",
    "    analyzer = BatchedTokenSubstitutionAnalyzer(model, device)\n",
    "    \n",
    "    # Analyze neurons 0-199 for each layer\n",
    "    layer_range = range(8)\n",
    "    neuron_range = range(200)\n",
    "    \n",
    "    print(\"Starting batched analysis...\")\n",
    "    results = analyzer.analyze_neuron_batch(layer_range, neuron_range)\n",
    "    \n",
    "    # Calculate and display statistics\n",
    "    stats = analyzer.get_substitution_statistics(results)\n",
    "    \n",
    "    print(\"\\nAnalysis Results:\")\n",
    "    print(f\"Total neurons analyzed: {stats['total_neurons_analyzed']}\")\n",
    "    print(f\"Neurons with substitutions: {stats['neurons_with_substitutions']}\")\n",
    "    print(f\"Success rate: {(stats['neurons_with_substitutions']/stats['total_neurons_analyzed'])*100:.2f}%\")\n",
    "    \n",
    "    print(\"\\nSubstitutions by layer:\")\n",
    "    for layer, count in sorted(stats['substitutions_by_layer'].items()):\n",
    "        print(f\"Layer {layer}: {count} substitutions\")\n",
    "    \n",
    "    print(\"\\nSubstitutions by token type:\")\n",
    "    for token_type, count in stats['substitutions_by_type'].items():\n",
    "        print(f\"{token_type}: {count} substitutions\")\n",
    "    \n",
    "    print(f\"\\nAverage activation ratio: {stats['average_activation_ratio']:.3f}\")\n",
    "    \n",
    "    # Save final results\n",
    "    output_path = Path('substitution_analysis_results.json')\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'results': results,\n",
    "            'statistics': stats\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nFull results saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_batch_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed modules\n",
    "from neuron_graph_builder import NeuronAnalyzer\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import json\n",
    "import string\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Optional\n",
    "from graph_utils import fast_measure_importance_ablated\n",
    "\n",
    "class AblatedBatchedTokenSubstitutionAnalyzer(NeuronAnalyzer):\n",
    "    \"\"\"\n",
    "    Analyzes neuron activation patterns for ablated models, suggesting token substitutions\n",
    "    that maintain activation patterns. Inherits from NeuronAnalyzer to reuse graph and\n",
    "    activation analysis code.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, device='cuda'):\n",
    "        \"\"\"Initialize analyzer with both GPT2 and BERT tokenization\"\"\"\n",
    "        super().__init__(model, device)\n",
    "        \n",
    "        # Initialize BERT for token suggestions\n",
    "        print(\"Loading BERT model for token suggestions...\")\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased').to(device)\n",
    "        self.bert_model.eval()\n",
    "        \n",
    "        # Rename base class tokenizer for clarity\n",
    "        self.gpt2_tokenizer = self.tokenizer\n",
    "        \n",
    "        # Define grammatical patterns for prompting\n",
    "        self.grammatical_patterns = {\n",
    "            'punctuation': [\n",
    "                \"In the sentence '{pre} [MASK] {post}', the punctuation\",\n",
    "                \"The text reads '{pre} [MASK] {post}' with proper punctuation\",\n",
    "            ],\n",
    "            'article': [\n",
    "                \"Consider '{pre} [MASK] {post}' as a phrase\",\n",
    "                \"The phrase '{pre} [MASK] {post}' uses an article\",\n",
    "            ],\n",
    "            'preposition': [\n",
    "                \"The words '{pre} [MASK] {post}' form a prepositional phrase\",\n",
    "                \"'{pre} [MASK] {post}' shows spatial relationship\",\n",
    "            ],\n",
    "            'conjunction': [\n",
    "                \"The sentence '{pre} [MASK] {post}' uses a conjunction\",\n",
    "                \"Join the phrases: {pre} [MASK] {post}\",\n",
    "            ],\n",
    "            'verb': [\n",
    "                \"The action in '{pre} [MASK] {post}' is\",\n",
    "                \"Fill in the verb: {pre} [MASK] {post}\",\n",
    "            ],\n",
    "            'general': [\n",
    "                \"Complete the phrase: {pre} [MASK] {post}\",\n",
    "                \"Fill in: {pre} [MASK] {post}\",\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Define token type sets\n",
    "        self.articles = {'the', 'a', 'an'}\n",
    "        self.prepositions = {'in', 'on', 'at', 'to', 'from', 'by', 'with'}\n",
    "        self.conjunctions = {'and', 'but', 'or', 'nor', 'for', 'yet', 'so'}\n",
    "\n",
    "    def get_token_type(self, token: str) -> str:\n",
    "        \"\"\"Determine the grammatical type of a token\"\"\"\n",
    "        cleaned = token.strip().lower()\n",
    "        if cleaned in string.punctuation:\n",
    "            return 'punctuation'\n",
    "        if cleaned in self.articles:\n",
    "            return 'article'\n",
    "        if cleaned in self.prepositions:\n",
    "            return 'preposition'\n",
    "        if cleaned in self.conjunctions:\n",
    "            return 'conjunction'\n",
    "        return 'general'\n",
    "\n",
    "    def test_activation(self, pre_context, token_id, post_context, layer, neuron, \n",
    "                           min_activation=1e-4, relative_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Test neuron activation using graph_utils infrastructure with better activation handling\n",
    "        \n",
    "        Args:\n",
    "            min_activation: Minimum absolute activation value to consider valid\n",
    "            relative_threshold: Minimum ratio relative to max activation to consider valid\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Process context tokens\n",
    "            if not pre_context:\n",
    "                pre_ids = []\n",
    "            elif isinstance(pre_context[0], str):\n",
    "                pre_ids = [self.gpt2_tokenizer.encode(t)[0] for t in pre_context]\n",
    "            else:\n",
    "                pre_ids = pre_context\n",
    "                \n",
    "            if not post_context:\n",
    "                post_ids = []\n",
    "            elif isinstance(post_context[0], str):\n",
    "                post_ids = [self.gpt2_tokenizer.encode(t)[0] for t in post_context]\n",
    "            else:\n",
    "                post_ids = post_context\n",
    "            \n",
    "            # Convert to full sequence\n",
    "            sequence = pre_ids + [token_id] + post_ids\n",
    "            text_input = self.gpt2_tokenizer.decode(sequence)\n",
    "            position = len(pre_ids)\n",
    "            \n",
    "            # Use our existing activation testing\n",
    "            _, initial_max, _, token_activations, actual_max_pos = fast_measure_importance_ablated(\n",
    "                self, layer, neuron, text_input, \n",
    "                initial_argmax=position\n",
    "            )\n",
    "            \n",
    "            # Get activation at our target position\n",
    "            position_activation = None\n",
    "            max_activation = None\n",
    "            for token, act in token_activations:\n",
    "                if position_activation is None and token == self.gpt2_tokenizer.decode([token_id]):\n",
    "                    position_activation = act\n",
    "                max_activation = max(max_activation or act, act)\n",
    "            \n",
    "            # Validation criteria:\n",
    "            # 1. Must have some meaningful activation\n",
    "            # 2. Must be reasonably close to max activation found\n",
    "            if position_activation is not None:\n",
    "                meets_min = abs(position_activation) >= min_activation\n",
    "                ratio_to_max = abs(position_activation) / abs(max_activation) if abs(max_activation) > min_activation else 0\n",
    "                meets_relative = ratio_to_max >= relative_threshold\n",
    "                \n",
    "                if meets_min or meets_relative:\n",
    "                    return {\n",
    "                        'activation': position_activation,\n",
    "                        'token_activations': token_activations,\n",
    "                        'position': position,\n",
    "                        'sequence': text_input,\n",
    "                        'meets_min': meets_min,\n",
    "                        'meets_relative': meets_relative,\n",
    "                        'ratio_to_max': ratio_to_max\n",
    "                    }\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in test_activation: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_bert_predictions_with_grammar(self, pre_context, post_context, original_token, k=10):\n",
    "        \"\"\"Get BERT predictions using grammatical prompting patterns\"\"\"\n",
    "        token_type = self.get_token_type(original_token)\n",
    "        patterns = self.grammatical_patterns[token_type]\n",
    "        \n",
    "        # Prepare context\n",
    "        pre_text = ' '.join([t.replace('Ġ', ' ').strip() for t in pre_context]) if pre_context else ''\n",
    "        post_text = ' '.join([t.replace('Ġ', ' ').strip() for t in post_context]) if post_context else ''\n",
    "        \n",
    "        predictions = []\n",
    "        for pattern in patterns:\n",
    "            masked_text = pattern.format(pre=pre_text, post=post_text).strip()\n",
    "            inputs = self.bert_tokenizer(masked_text, return_tensors='pt').to(self.device)\n",
    "            mask_idx = torch.where(inputs['input_ids'] == self.bert_tokenizer.mask_token_id)[1]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.bert_model(**inputs)\n",
    "                logits = outputs.logits[0, mask_idx]\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                \n",
    "                top_k = torch.topk(probs, k * 2)\n",
    "                \n",
    "                for prob, token_id in zip(top_k.values[0], top_k.indices[0]):\n",
    "                    token = self.bert_tokenizer.decode([token_id])\n",
    "                    gpt2_tokens = self.gpt2_tokenizer.encode(token)\n",
    "                    \n",
    "                    if len(gpt2_tokens) == 1:\n",
    "                        predictions.append({\n",
    "                            'token': token,\n",
    "                            'gpt2_id': gpt2_tokens[0],\n",
    "                            'probability': prob.item(),\n",
    "                            'pattern': pattern\n",
    "                        })\n",
    "        \n",
    "        # Get unique predictions\n",
    "        unique_predictions = {}\n",
    "        for pred in sorted(predictions, key=lambda x: x['probability'], reverse=True):\n",
    "            if pred['token'] not in unique_predictions:\n",
    "                unique_predictions[pred['token']] = pred\n",
    "                if len(unique_predictions) == k:\n",
    "                    break\n",
    "        \n",
    "        return list(unique_predictions.values())\n",
    "\n",
    "    def analyze_neuron_substitutions(self, layer: int, neuron: int, graph_path: Path, \n",
    "                                   debug_mode: bool = True) -> Optional[List[Dict]]:\n",
    "        \"\"\"Analyze possible token substitutions for a neuron\"\"\"\n",
    "        print(f\"\\nAnalyzing L{layer}N{neuron}\")\n",
    "        \n",
    "        try:\n",
    "            # Load graph\n",
    "            with open(graph_path) as f:\n",
    "                graph_data = json.load(f)\n",
    "            \n",
    "            # Find activating nodes\n",
    "            activating_nodes = [n for n, data in graph_data['nodes'].items() \n",
    "                              if data.get('is_activating', False)]\n",
    "            \n",
    "            if not activating_nodes:\n",
    "                print(\"No activating nodes found\")\n",
    "                return None\n",
    "            \n",
    "            print(f\"Found {len(activating_nodes)} activating patterns\")\n",
    "            results = []\n",
    "            \n",
    "            # Test each activating pattern\n",
    "            for node in activating_nodes:\n",
    "                if debug_mode:\n",
    "                    print(f\"\\nTesting pattern: {node}\")\n",
    "                \n",
    "                # Get context\n",
    "                pre_edges = [e.split('->')[0] for e in graph_data['edges'] \n",
    "                           if e.split('->')[1] == node]\n",
    "                \n",
    "                # Get predictions\n",
    "                predictions = self.get_bert_predictions_with_grammar(\n",
    "                    pre_edges, [], node, k=5\n",
    "                )\n",
    "                \n",
    "                # Test each prediction\n",
    "                for pred in predictions:\n",
    "                    try:\n",
    "                        result = self.test_activation(\n",
    "                            pre_context=pre_edges,\n",
    "                            token_id=pred['gpt2_id'],\n",
    "                            post_context=[],\n",
    "                            layer=layer,\n",
    "                            neuron=neuron\n",
    "                        )\n",
    "                        \n",
    "                        if result:  # Only add successful tests\n",
    "                            substitution = {\n",
    "                                'original_token': node,\n",
    "                                'substitute_token': pred['token'],\n",
    "                                'activation': result['activation'],\n",
    "                                'token_activations': result['token_activations'],\n",
    "                                'bert_probability': pred['probability'],\n",
    "                                'pre_context': pre_edges,\n",
    "                                'activation_quality': {\n",
    "                                    'meets_min_threshold': result['meets_min'],\n",
    "                                    'meets_relative_threshold': result['meets_relative'],\n",
    "                                    'ratio_to_max': result['ratio_to_max']\n",
    "                                }\n",
    "                            }\n",
    "                            \n",
    "                            if debug_mode:\n",
    "                                print(f\"\\nSubstitution: {node} -> {pred['token']}\")\n",
    "                                print(f\"Activation: {result['activation']:.4f}\")\n",
    "                                print(f\"Ratio to max: {result['ratio_to_max']:.4f}\")\n",
    "                                if result['meets_min']:\n",
    "                                    print(\"Meets minimum threshold\")\n",
    "                                if result['meets_relative']:\n",
    "                                    print(\"Meets relative threshold\")\n",
    "                            \n",
    "                            results.append(substitution)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error testing substitution: {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            return results if results else None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing neuron: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def analyze_layer_range(self, start_layer: int, end_layer: int, neurons_per_layer: int = 5):\n",
    "        \"\"\"Analyze a range of layers for substitution patterns\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for layer in range(start_layer, end_layer + 1):\n",
    "            layer_results = []\n",
    "            print(f\"\\nAnalyzing Layer {layer}\")\n",
    "            \n",
    "            for neuron in range(neurons_per_layer):\n",
    "                graph_path = Path('neuron_graphs/lbl') / f'layer_{layer}' / f'l{layer}_n{neuron}_graph.json'\n",
    "                \n",
    "                if graph_path.exists():\n",
    "                    substitutions = self.analyze_neuron_substitutions(\n",
    "                        layer=layer,\n",
    "                        neuron=neuron,\n",
    "                        graph_path=graph_path\n",
    "                    )\n",
    "                    if substitutions:\n",
    "                        layer_results.extend(substitutions)\n",
    "            \n",
    "            if layer_results:\n",
    "                results[layer] = layer_results\n",
    "                print(f\"Found {len(layer_results)} substitutions in layer {layer}\")\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Analysis runner for ablated models\n",
    "def run_ablated_batch_analysis():\n",
    "    \"\"\"Run complete analysis for neurons 0-199 in each layer of the ablated model\"\"\"\n",
    "    print(\"Loading ablated GPT model...\")\n",
    "    model_path = 'configs/lbl_model_20241016.pt'  # Using the layer-by-layer ablated model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, config = load_model(model_path, device)\n",
    "    model.eval()\n",
    "\n",
    "    analyzer = AblatedBatchedTokenSubstitutionAnalyzer(model, device)\n",
    "    \n",
    "    # Analyze neurons 0-199 for each layer\n",
    "    layer_range = range(8)\n",
    "    neuron_range = range(200)\n",
    "    \n",
    "    print(\"Starting batched analysis...\")\n",
    "    results = analyzer.analyze_neuron_batch(layer_range, neuron_range)\n",
    "    \n",
    "    # Calculate and display statistics\n",
    "    stats = analyzer.get_substitution_statistics(results)\n",
    "    \n",
    "    # Print analysis results\n",
    "    print(\"\\nAnalysis Results:\")\n",
    "    print(f\"Total neurons analyzed: {stats['total_neurons_analyzed']}\")\n",
    "    print(f\"Neurons with substitutions: {stats['neurons_with_substitutions']}\")\n",
    "    print(f\"Success rate: {(stats['neurons_with_substitutions']/stats['total_neurons_analyzed'])*100:.2f}%\")\n",
    "    \n",
    "    print(\"\\nSubstitutions by layer:\")\n",
    "    for layer, count in sorted(stats['substitutions_by_layer'].items()):\n",
    "        print(f\"Layer {layer}: {count} substitutions\")\n",
    "    \n",
    "    print(\"\\nSubstitutions by token type:\")\n",
    "    for token_type, count in stats['substitutions_by_type'].items():\n",
    "        print(f\"{token_type}: {count} substitutions\")\n",
    "    \n",
    "    print(f\"\\nAverage activation ratio: {stats['average_activation_ratio']:.3f}\")\n",
    "    \n",
    "    # Save final results with timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = Path(f'ablated_substitution_analysis_{timestamp}.json')\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'results': results,\n",
    "            'statistics': stats,\n",
    "            'model_info': {\n",
    "                'path': model_path,\n",
    "                'type': 'ablated',\n",
    "                'config': config.__dict__\n",
    "            }\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nFull results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Third cell - Execution\n",
    "if __name__ == \"__main__\":\n",
    "    run_ablated_batch_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Get the existing analyzer instance and results\n",
    "model_path = 'configs/lbl_model_20241016.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, config = load_model(model_path, device)\n",
    "analyzer = AblatedBatchedTokenSubstitutionAnalyzer(model, device)\n",
    "\n",
    "# Load your most recent results file\n",
    "results_path = \"substitution/Ablated/ablated_substitution_analysis_20250116_235410.json\"  # Update with your latest results file\n",
    "with open(results_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    results = data['results']\n",
    "\n",
    "# Run the specialization analysis\n",
    "analyzer.analyze_neuron_specialization(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Load model and create analyzer\n",
    "model_path = 'configs/lbl_model_20241016.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, config = load_model(model_path, device)\n",
    "analyzer = AblatedBatchedTokenSubstitutionAnalyzer(model, device)\n",
    "\n",
    "# Define ranges for all layers\n",
    "layer_range = range(8)  # All 8 layers\n",
    "neuron_range = range(200)  # 200 neurons per layer\n",
    "\n",
    "# Run analysis\n",
    "print(\"\\nStarting batch analysis with layer statistics...\")\n",
    "results = analyzer.analyze_neuron_batch(layer_range, neuron_range)\n",
    "\n",
    "# Load most recent results for comparison\n",
    "latest_results_path = \"ablated_substitution_analysis_20250117_091755.json\"\n",
    "with open(latest_results_path, 'r') as f:\n",
    "    previous_data = json.load(f)\n",
    "    previous_results = previous_data['results']\n",
    "\n",
    "print(\"\\nComparison of Results:\")\n",
    "print(\"Previous analysis found neurons in layers:\", \n",
    "      set(int(k.split('_')[0][1:]) for k in previous_results.keys()))\n",
    "print(\"Current analysis found neurons in layers:\", \n",
    "      set(int(k.split('_')[0][1:]) for k in results.keys()))\n",
    "\n",
    "# Run specialization analysis on new results\n",
    "print(\"\\nSpecialization Analysis of New Results:\")\n",
    "analyzer.analyze_neuron_specialization(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from configs/lbl_model_20241016.pt\n",
      "Model type: ablated (layer-by-layer)\n",
      "Number of parameters: 8709504\n",
      "Initializing NeuronAnalyzer...\n",
      "Detected ablated model - initializing HookedTransformer\n",
      "HookedTransformer initialized successfully\n",
      "NeuronAnalyzer initialized (Model type: ablated)\n",
      "Loading BERT model for token suggestions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing activation patterns across layers...\n",
      "\n",
      "Layer 0:\n",
      "Error processing neuron 161 in layer 0: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 87 in layer 0: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 159 in layer 0: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 163 in layer 0: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 89 in layer 0: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 112 in layer 0: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 135 in layer 0: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 107 in layer 0: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 77 in layer 0: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 140 in layer 0: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Layer 0 Summary:\n",
      "Successful neurons: 0/10\n",
      "Total patterns found: 0\n",
      "\n",
      "Layer 1:\n",
      "Error processing neuron 52 in layer 1: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 15 in layer 1: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 114 in layer 1: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 50 in layer 1: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 12 in layer 1: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 140 in layer 1: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 78 in layer 1: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 67 in layer 1: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 48 in layer 1: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 62 in layer 1: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Layer 1 Summary:\n",
      "Successful neurons: 0/10\n",
      "Total patterns found: 0\n",
      "\n",
      "Layer 2:\n",
      "Error processing neuron 37 in layer 2: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 34 in layer 2: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 127 in layer 2: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 177 in layer 2: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 187 in layer 2: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 67 in layer 2: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 100 in layer 2: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 189 in layer 2: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 86 in layer 2: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 129 in layer 2: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Layer 2 Summary:\n",
      "Successful neurons: 0/10\n",
      "Total patterns found: 0\n",
      "\n",
      "Layer 3:\n",
      "Error processing neuron 23 in layer 3: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 52 in layer 3: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 49 in layer 3: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 29 in layer 3: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 92 in layer 3: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 164 in layer 3: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 134 in layer 3: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 75 in layer 3: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 47 in layer 3: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 102 in layer 3: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Layer 3 Summary:\n",
      "Successful neurons: 0/10\n",
      "Total patterns found: 0\n",
      "\n",
      "Layer 4:\n",
      "Error processing neuron 155 in layer 4: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 43 in layer 4: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 102 in layer 4: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 149 in layer 4: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 165 in layer 4: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 14 in layer 4: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 194 in layer 4: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 113 in layer 4: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 2 in layer 4: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 59 in layer 4: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Layer 4 Summary:\n",
      "Successful neurons: 0/10\n",
      "Total patterns found: 0\n",
      "\n",
      "Layer 5:\n",
      "Error processing neuron 13 in layer 5: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 182 in layer 5: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 93 in layer 5: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 17 in layer 5: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 76 in layer 5: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 41 in layer 5: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 162 in layer 5: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 102 in layer 5: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 145 in layer 5: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 55 in layer 5: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Layer 5 Summary:\n",
      "Successful neurons: 0/10\n",
      "Total patterns found: 0\n",
      "\n",
      "Layer 6:\n",
      "Error processing neuron 127 in layer 6: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 109 in layer 6: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 140 in layer 6: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 102 in layer 6: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 64 in layer 6: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 189 in layer 6: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 132 in layer 6: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 10 in layer 6: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 148 in layer 6: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 192 in layer 6: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Layer 6 Summary:\n",
      "Successful neurons: 0/10\n",
      "Total patterns found: 0\n",
      "\n",
      "Layer 7:\n",
      "Error processing neuron 11 in layer 7: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 78 in layer 7: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 149 in layer 7: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 146 in layer 7: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 89 in layer 7: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 1 in layer 7: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 176 in layer 7: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 150 in layer 7: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 58 in layer 7: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Error processing neuron 170 in layer 7: 'AblatedBatchedTokenSubstitutionAnalyzer' object has no attribute 'load_neuron_graph'\n",
      "Layer 7 Summary:\n",
      "Successful neurons: 0/10\n",
      "Total patterns found: 0\n",
      "\n",
      "Layer-wise Activation Statistics:\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Load most recent results and model\n",
    "model_path = 'configs/lbl_model_20241016.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, config = load_model(model_path, device)\n",
    "analyzer = AblatedBatchedTokenSubstitutionAnalyzer(model, device)\n",
    "\n",
    "# Sample a few neurons from each layer to analyze activation patterns\n",
    "sample_size = 10\n",
    "activation_stats = defaultdict(list)\n",
    "\n",
    "print(\"\\nAnalyzing activation patterns across layers...\")\n",
    "for layer in range(8):  # All 8 layers\n",
    "    print(f\"\\nLayer {layer}:\")\n",
    "    neurons = random.sample(range(200), sample_size)\n",
    "    total_patterns = 0\n",
    "    successful_neurons = 0\n",
    "    \n",
    "    for neuron in neurons:\n",
    "        graph_path = Path('neuron_graphs/lbl') / f'layer_{layer}' / f'l{layer}_n{neuron}_graph.json'\n",
    "        try:\n",
    "            if graph_path.exists():\n",
    "                graph = analyzer.load_neuron_graph(graph_path)\n",
    "                patterns = analyzer.get_activation_patterns(graph)\n",
    "                \n",
    "                if patterns:\n",
    "                    total_patterns += len(patterns)\n",
    "                    successful_neurons += 1\n",
    "                    activations = [p['activation'] for p in patterns]\n",
    "                    stats = {\n",
    "                        'mean': float(np.mean(activations)),\n",
    "                        'max': float(np.max(activations)),\n",
    "                        'std': float(np.std(activations)),\n",
    "                        'pattern_count': len(patterns)\n",
    "                    }\n",
    "                    activation_stats[layer].append(stats)\n",
    "                    print(f\"  Neuron {neuron}: {len(patterns)} patterns, \"\n",
    "                          f\"max activation: {stats['max']:.4f}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing neuron {neuron} in layer {layer}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Layer {layer} Summary:\")\n",
    "    print(f\"Successful neurons: {successful_neurons}/{sample_size}\")\n",
    "    print(f\"Total patterns found: {total_patterns}\")\n",
    "\n",
    "# Print comprehensive statistics for each layer\n",
    "print(\"\\nLayer-wise Activation Statistics:\")\n",
    "for layer in range(8):\n",
    "    if activation_stats[layer]:\n",
    "        layer_means = [s['mean'] for s in activation_stats[layer]]\n",
    "        layer_maxes = [s['max'] for s in activation_stats[layer]]\n",
    "        layer_patterns = [s['pattern_count'] for s in activation_stats[layer]]\n",
    "        \n",
    "        print(f\"\\nLayer {layer}:\")\n",
    "        print(f\"Average activation: {np.mean(layer_means):.4f} (±{np.std(layer_means):.4f})\")\n",
    "        print(f\"Average max activation: {np.mean(layer_maxes):.4f} (±{np.std(layer_maxes):.4f})\")\n",
    "        print(f\"Average pattern count: {np.mean(layer_patterns):.1f}\")\n",
    "        print(f\"Number of neurons with patterns: {len(activation_stats[layer])}/{sample_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Let's examine the actual graph files for deeper layers\n",
    "print(\"\\nAnalyzing graph structure across layers...\")\n",
    "graph_stats = defaultdict(lambda: {'total_graphs': 0, 'graphs_with_activations': 0, 'total_nodes': 0})\n",
    "\n",
    "for layer in range(8):\n",
    "    print(f\"\\nLayer {layer}:\")\n",
    "    for neuron in range(200):  # Check all neurons\n",
    "        graph_path = Path('neuron_graphs/lbl') / f'layer_{layer}' / f'l{layer}_n{neuron}_graph.json'\n",
    "        if graph_path.exists():\n",
    "            graph_stats[layer]['total_graphs'] += 1\n",
    "            try:\n",
    "                with open(graph_path, 'r') as f:\n",
    "                    graph_data = json.load(f)\n",
    "                    # Count nodes with is_activating=True\n",
    "                    activating_nodes = sum(1 for node, data in graph_data['nodes'].items() \n",
    "                                         if data.get('is_activating', False))\n",
    "                    if activating_nodes > 0:\n",
    "                        graph_stats[layer]['graphs_with_activations'] += 1\n",
    "                        graph_stats[layer]['total_nodes'] += activating_nodes\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading graph {neuron}: {str(e)}\")\n",
    "    \n",
    "    stats = graph_stats[layer]\n",
    "    if stats['total_graphs'] > 0:\n",
    "        print(f\"Total graphs: {stats['total_graphs']}\")\n",
    "        print(f\"Graphs with activations: {stats['graphs_with_activations']}\")\n",
    "        print(f\"Average activating nodes per graph: {stats['total_nodes']/stats['total_graphs']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nAnalyzing activation strengths in graphs with activations...\")\n",
    "activation_strengths = defaultdict(list)\n",
    "\n",
    "for layer in range(8):\n",
    "    print(f\"\\nLayer {layer}:\")\n",
    "    for neuron in range(200):\n",
    "        graph_path = Path('neuron_graphs/lbl') / f'layer_{layer}' / f'l{layer}_n{neuron}_graph.json'\n",
    "        if graph_path.exists():\n",
    "            with open(graph_path, 'r') as f:\n",
    "                graph_data = json.load(f)\n",
    "                # Get activation values for activating nodes\n",
    "                activations = [data.get('activation', 0) \n",
    "                             for data in graph_data['nodes'].values() \n",
    "                             if data.get('is_activating', False)]\n",
    "                if activations:\n",
    "                    activation_strengths[layer].extend(activations)\n",
    "    \n",
    "    if activation_strengths[layer]:\n",
    "        print(f\"Number of activating patterns: {len(activation_strengths[layer])}\")\n",
    "        print(f\"Mean activation: {np.mean(activation_strengths[layer]):.4f}\")\n",
    "        print(f\"Max activation: {np.max(activation_strengths[layer]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model and analyzer\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "\n",
    "model_path = 'configs/lbl_model_20241016.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, config = load_model(model_path, device)\n",
    "analyzer = AblatedBatchedTokenSubstitutionAnalyzer(model, device)\n",
    "\n",
    "# Run analysis with updated method\n",
    "print(\"Starting analysis with layer-adaptive thresholds...\")\n",
    "layer_range = range(8)\n",
    "neuron_range = range(200)\n",
    "results = analyzer.analyze_neuron_batch(layer_range, neuron_range)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nResults by layer:\")\n",
    "layer_counts = defaultdict(int)\n",
    "for neuron_id in results:\n",
    "    layer = int(neuron_id.split('_')[0][1:])\n",
    "    layer_counts[layer] += len(results[neuron_id])\n",
    "\n",
    "for layer in sorted(layer_counts.keys()):\n",
    "    print(f\"Layer {layer}: {layer_counts[layer]} substitutions\")\n",
    "\n",
    "# Run specialization analysis\n",
    "print(\"\\nNeuron specialization analysis:\")\n",
    "analyzer.analyze_neuron_specialization(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model and analyzer\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "\n",
    "model_path = 'configs/lbl_model_20241016.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, config = load_model(model_path, device)\n",
    "analyzer = AblatedBatchedTokenSubstitutionAnalyzer(model, device)\n",
    "\n",
    "# Run focused analysis on a few neurons from each layer\n",
    "print(\"\\nDetailed Layer Analysis:\")\n",
    "for layer in range(8):\n",
    "    print(f\"\\n=== Layer {layer} ===\")\n",
    "    # Take first 5 neurons that have activation patterns\n",
    "    neurons_checked = 0\n",
    "    neurons_with_patterns = 0\n",
    "    \n",
    "    for neuron in range(200):\n",
    "        if neurons_with_patterns >= 5 and neurons_checked >= 20:  # Stop after finding 5 or checking 20\n",
    "            break\n",
    "            \n",
    "        graph_path = Path('neuron_graphs/lbl') / f'layer_{layer}' / f'l{layer}_n{neuron}_graph.json'\n",
    "        if graph_path.exists():\n",
    "            neurons_checked += 1\n",
    "            \n",
    "            # Load and analyze graph\n",
    "            graph = analyzer.load_neuron_graph(graph_path)\n",
    "            patterns = analyzer.get_activation_patterns(graph)\n",
    "            \n",
    "            if patterns:\n",
    "                neurons_with_patterns += 1\n",
    "                print(f\"\\nNeuron {neuron}:\")\n",
    "                print(f\"Number of patterns: {len(patterns)}\")\n",
    "                \n",
    "                # Show activation values\n",
    "                activations = [p['activation'] for p in patterns]\n",
    "                print(f\"Activation stats:\")\n",
    "                print(f\"- Mean: {np.mean(activations):.4f}\")\n",
    "                print(f\"- Max: {np.max(activations):.4f}\")\n",
    "                print(f\"- Min: {np.min(activations):.4f}\")\n",
    "                \n",
    "                # Show pattern details for first pattern\n",
    "                if patterns:\n",
    "                    pattern = patterns[0]\n",
    "                    print(f\"\\nFirst pattern details:\")\n",
    "                    print(f\"Token: {pattern['token']}\")\n",
    "                    print(f\"Activation: {pattern['activation']:.4f}\")\n",
    "                    print(f\"Pre-context length: {len(pattern['pre_context'])}\")\n",
    "                    print(f\"Post-context length: {len(pattern['post_context'])}\")\n",
    "                \n",
    "                # Try to get predictions and test activations\n",
    "                if patterns:\n",
    "                    pattern = patterns[0]\n",
    "                    predictions = analyzer.get_bert_predictions_with_grammar(\n",
    "                        pattern['pre_context'],\n",
    "                        pattern['post_context'],\n",
    "                        pattern['token']\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"\\nTesting first 3 predictions:\")\n",
    "                    for pred in predictions[:3]:\n",
    "                        activation = analyzer.test_activation(\n",
    "                            pattern['pre_context'],\n",
    "                            pred['gpt2_id'],\n",
    "                            pattern['post_context'],\n",
    "                            layer,\n",
    "                            neuron\n",
    "                        )\n",
    "                        print(f\"'{pred['token']}': {activation:.4f}\")\n",
    "    \n",
    "    print(f\"\\nLayer {layer} Summary:\")\n",
    "    print(f\"Neurons checked: {neurons_checked}\")\n",
    "    print(f\"Neurons with patterns: {neurons_with_patterns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Test cell for specific neuron activation diagnostics\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "\n",
    "model_path = 'configs/lbl_model_20241016.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, config = load_model(model_path, device)\n",
    "analyzer = AblatedBatchedTokenSubstitutionAnalyzer(model, device)\n",
    "\n",
    "# Let's test some of the neurons from your examples\n",
    "test_cases = [\n",
    "   (7, 87),  # Layer 7 neuron with 'said' pattern\n",
    "   (6, 98),  # Layer 6 neuron with 'Tim' pattern\n",
    "   (5, 114), # Layer 5 neuron with high activation (10.3284)\n",
    "   (4, 87)   # Layer 4 neuron with low activation\n",
    "]\n",
    "\n",
    "print(\"Testing specific neurons with detailed activation tracking...\")\n",
    "for layer, neuron in test_cases:\n",
    "   graph_path = Path('neuron_graphs/lbl') / f'layer_{layer}' / f'l{layer}_n{neuron}_graph.json'\n",
    "   if graph_path.exists():\n",
    "       analyzer.test_specific_neuron(layer, neuron, graph_path)\n",
    "       print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Loading model...\n",
      "Model loaded successfully from configs/lbl_model_20241016.pt\n",
      "Model type: ablated (layer-by-layer)\n",
      "Number of parameters: 8709504\n",
      "Model loaded successfully!\n",
      "\n",
      "Initializing analyzer...\n",
      "Initializing NeuronAnalyzer...\n",
      "Detected ablated model - initializing HookedTransformer\n",
      "HookedTransformer initialized successfully\n",
      "NeuronAnalyzer initialized (Model type: ablated)\n",
      "Loading BERT model for token suggestions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzer initialized!\n",
      "\n",
      "Results will be saved to: substitution_results_20250117_150955\n",
      "\n",
      "Test configuration:\n",
      "start_layer: 3\n",
      "end_layer: 7\n",
      "neurons_per_layer: 3\n",
      "debug_mode: True\n",
      "\n",
      "Starting analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layer 4:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing L3N0\n",
      "\n",
      "Analyzing L3N0\n",
      "No activating nodes found\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L3N1\n",
      "\n",
      "Analyzing L3N1\n",
      "No activating nodes found\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L3N2\n",
      "\n",
      "Analyzing L3N2\n",
      "No activating nodes found\n",
      "No substitutions found\n",
      "\n",
      "Layer 3 complete: No substitutions found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layer 5:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing L4N0\n",
      "\n",
      "Analyzing L4N0\n",
      "No activating nodes found\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L4N1\n",
      "\n",
      "Analyzing L4N1\n",
      "No activating nodes found\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L4N2\n",
      "\n",
      "Analyzing L4N2\n",
      "No activating nodes found\n",
      "No substitutions found\n",
      "\n",
      "Layer 4 complete: No substitutions found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layer 6:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing L5N0\n",
      "\n",
      "Analyzing L5N0\n",
      "No activating nodes found\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L5N1\n",
      "\n",
      "Analyzing L5N1\n",
      "No activating nodes found\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L5N2\n",
      "\n",
      "Analyzing L5N2\n",
      "No activating nodes found\n",
      "No substitutions found\n",
      "\n",
      "Layer 5 complete: No substitutions found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layer 7:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing L6N0\n",
      "\n",
      "Analyzing L6N0\n",
      "No activating nodes found\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L6N1\n",
      "\n",
      "Analyzing L6N1\n",
      "No activating nodes found\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L6N2\n",
      "\n",
      "Analyzing L6N2\n",
      "No activating nodes found\n",
      "No substitutions found\n",
      "\n",
      "Layer 6 complete: No substitutions found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layer 7: 100%|██████████| 5/5 [00:00<00:00, 88.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing L7N0\n",
      "\n",
      "Analyzing L7N0\n",
      "No activating nodes found\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L7N1\n",
      "\n",
      "Analyzing L7N1\n",
      "No activating nodes found\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L7N2\n",
      "\n",
      "Analyzing L7N2\n",
      "No activating nodes found\n",
      "No substitutions found\n",
      "\n",
      "Layer 7 complete: No substitutions found\n",
      "\n",
      "No results found in any layer\n",
      "\n",
      "Cleaning up...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from model_loader import load_model\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Set up device and paths\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nLoading model...\")\n",
    "model_path = 'configs/lbl_model_20241016.pt'\n",
    "model, config = load_model(model_path, device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Initialize analyzer\n",
    "print(\"\\nInitializing analyzer...\")\n",
    "analyzer = AblatedBatchedTokenSubstitutionAnalyzer(model, device)\n",
    "print(\"Analyzer initialized!\")\n",
    "\n",
    "# Create output directory for results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = Path(f'substitution_results_{timestamp}')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "print(f\"\\nResults will be saved to: {output_dir}\")\n",
    "\n",
    "# Test parameters\n",
    "test_config = {\n",
    "    'start_layer': 3,\n",
    "    'end_layer': 7,\n",
    "    'neurons_per_layer': 3,\n",
    "    'debug_mode': True\n",
    "}\n",
    "\n",
    "print(\"\\nTest configuration:\")\n",
    "for key, value in test_config.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Run analysis with detailed progress tracking\n",
    "print(\"\\nStarting analysis...\")\n",
    "results = {}\n",
    "try:\n",
    "    # Iterate through layers with progress bar\n",
    "    layer_pbar = tqdm(range(test_config['start_layer'], test_config['end_layer'] + 1), \n",
    "                     desc=\"Processing layers\")\n",
    "    \n",
    "    for layer in layer_pbar:\n",
    "        layer_results = []\n",
    "        layer_pbar.set_description(f\"Processing layer {layer}\")\n",
    "        \n",
    "        # Iterate through neurons with progress bar\n",
    "        neuron_pbar = tqdm(range(test_config['neurons_per_layer']), \n",
    "                          desc=f\"Layer {layer} neurons\",\n",
    "                          leave=False)\n",
    "        \n",
    "        for neuron in neuron_pbar:\n",
    "            try:\n",
    "                neuron_pbar.set_description(f\"Layer {layer} Neuron {neuron}\")\n",
    "                \n",
    "                # Check if graph exists\n",
    "                graph_path = Path('neuron_graphs/lbl') / f'layer_{layer}' / f'l{layer}_n{neuron}_graph.json'\n",
    "                if not graph_path.exists():\n",
    "                    print(f\"\\nWarning: No graph file found at {graph_path}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Analyze neuron\n",
    "                print(f\"\\nAnalyzing L{layer}N{neuron}\")\n",
    "                substitutions = analyzer.analyze_neuron_substitutions(\n",
    "                    layer=layer,\n",
    "                    neuron=neuron,\n",
    "                    graph_path=graph_path,\n",
    "                    debug_mode=test_config['debug_mode']\n",
    "                )\n",
    "                \n",
    "                # Process results\n",
    "                if substitutions:\n",
    "                    print(f\"Found {len(substitutions)} substitutions\")\n",
    "                    layer_results.extend(substitutions)\n",
    "                else:\n",
    "                    print(\"No substitutions found\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing neuron {neuron} in layer {layer}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        # Store layer results if any found\n",
    "        if layer_results:\n",
    "            results[layer] = layer_results\n",
    "            print(f\"\\nLayer {layer} complete: {len(layer_results)} total substitutions\")\n",
    "        else:\n",
    "            print(f\"\\nLayer {layer} complete: No substitutions found\")\n",
    "    \n",
    "    # Save results\n",
    "    if results:\n",
    "        print(\"\\nSaving results...\")\n",
    "        summary_data = {\n",
    "            'timestamp': timestamp,\n",
    "            'config': test_config,\n",
    "            'results': results\n",
    "        }\n",
    "        \n",
    "        output_file = output_dir / 'substitution_analysis.json'\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(summary_data, f, indent=2)\n",
    "            \n",
    "        print(f\"Results saved to: {output_file}\")\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        total_layers = len(range(test_config['start_layer'], test_config['end_layer'] + 1))\n",
    "        layers_with_results = len(results)\n",
    "        total_substitutions = sum(len(layer_results) for layer_results in results.values())\n",
    "        avg_substitutions = total_substitutions / layers_with_results if layers_with_results else 0\n",
    "        \n",
    "        print(f\"Total layers analyzed: {total_layers}\")\n",
    "        print(f\"Layers with results: {layers_with_results}\")\n",
    "        print(f\"Total substitutions found: {total_substitutions}\")\n",
    "        print(f\"Average substitutions per layer with results: {avg_substitutions:.2f}\")\n",
    "        \n",
    "        # Print example substitutions\n",
    "        print(\"\\nExample substitutions from first layer with results:\")\n",
    "        if results:\n",
    "            first_layer = min(results.keys())\n",
    "            if results[first_layer]:\n",
    "                for i, sub in enumerate(results[first_layer][:3]):  # Show first 3 substitutions\n",
    "                    print(f\"\\nSubstitution {i+1}:\")\n",
    "                    print(f\"Original token: {sub['original_token']}\")\n",
    "                    print(f\"Substitute token: {sub['substitute_token']}\")\n",
    "                    print(f\"Activation: {sub['activation']:.4f}\")\n",
    "                    print(f\"BERT probability: {sub['bert_probability']:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nNo results found in any layer\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nError during analysis: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # Cleanup\n",
    "    print(\"\\nCleaning up...\")\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer 3 Summary:\n",
      "Files found: 200\n",
      "Files with activating nodes: 18\n",
      "Total activating nodes: 20\n",
      "\n",
      "Detailed status:\n",
      "Neuron 0: No activating nodes\n",
      "Neuron 1: No activating nodes\n",
      "Neuron 2: No activating nodes\n",
      "Neuron 3: No activating nodes\n",
      "Neuron 4: No activating nodes\n",
      "Neuron 5: No activating nodes\n",
      "Neuron 6: No activating nodes\n",
      "Neuron 7: No activating nodes\n",
      "Neuron 8: No activating nodes\n",
      "Neuron 9: No activating nodes\n",
      "Neuron 10: No activating nodes\n",
      "Neuron 11: No activating nodes\n",
      "Neuron 12: No activating nodes\n",
      "Neuron 13: No activating nodes\n",
      "Neuron 14: No activating nodes\n",
      "Neuron 15: No activating nodes\n",
      "Neuron 16: No activating nodes\n",
      "Neuron 17: No activating nodes\n",
      "Neuron 18: Found 1 activating nodes\n",
      "Neuron 19: No activating nodes\n",
      "Neuron 20: No activating nodes\n",
      "Neuron 21: Found 1 activating nodes\n",
      "Neuron 22: No activating nodes\n",
      "Neuron 23: No activating nodes\n",
      "Neuron 24: No activating nodes\n",
      "Neuron 25: No activating nodes\n",
      "Neuron 26: No activating nodes\n",
      "Neuron 27: No activating nodes\n",
      "Neuron 28: No activating nodes\n",
      "Neuron 29: No activating nodes\n",
      "Neuron 30: No activating nodes\n",
      "Neuron 31: No activating nodes\n",
      "Neuron 32: No activating nodes\n",
      "Neuron 33: No activating nodes\n",
      "Neuron 34: No activating nodes\n",
      "Neuron 35: No activating nodes\n",
      "Neuron 36: No activating nodes\n",
      "Neuron 37: No activating nodes\n",
      "Neuron 38: No activating nodes\n",
      "Neuron 39: Found 1 activating nodes\n",
      "Neuron 40: No activating nodes\n",
      "Neuron 41: No activating nodes\n",
      "Neuron 42: No activating nodes\n",
      "Neuron 43: No activating nodes\n",
      "Neuron 44: No activating nodes\n",
      "Neuron 45: No activating nodes\n",
      "Neuron 46: No activating nodes\n",
      "Neuron 47: No activating nodes\n",
      "Neuron 48: No activating nodes\n",
      "Neuron 49: No activating nodes\n",
      "Neuron 50: No activating nodes\n",
      "Neuron 51: No activating nodes\n",
      "Neuron 52: No activating nodes\n",
      "Neuron 53: Found 1 activating nodes\n",
      "Neuron 54: No activating nodes\n",
      "Neuron 55: No activating nodes\n",
      "Neuron 56: No activating nodes\n",
      "Neuron 57: No activating nodes\n",
      "Neuron 58: No activating nodes\n",
      "Neuron 59: No activating nodes\n",
      "Neuron 60: Found 1 activating nodes\n",
      "Neuron 61: No activating nodes\n",
      "Neuron 62: No activating nodes\n",
      "Neuron 63: No activating nodes\n",
      "Neuron 64: No activating nodes\n",
      "Neuron 65: No activating nodes\n",
      "Neuron 66: No activating nodes\n",
      "Neuron 67: No activating nodes\n",
      "Neuron 68: No activating nodes\n",
      "Neuron 69: No activating nodes\n",
      "Neuron 70: No activating nodes\n",
      "Neuron 71: No activating nodes\n",
      "Neuron 72: No activating nodes\n",
      "Neuron 73: No activating nodes\n",
      "Neuron 74: No activating nodes\n",
      "Neuron 75: No activating nodes\n",
      "Neuron 76: No activating nodes\n",
      "Neuron 77: No activating nodes\n",
      "Neuron 78: No activating nodes\n",
      "Neuron 79: No activating nodes\n",
      "Neuron 80: Found 1 activating nodes\n",
      "Neuron 81: No activating nodes\n",
      "Neuron 82: No activating nodes\n",
      "Neuron 83: No activating nodes\n",
      "Neuron 84: No activating nodes\n",
      "Neuron 85: No activating nodes\n",
      "Neuron 86: No activating nodes\n",
      "Neuron 87: No activating nodes\n",
      "Neuron 88: No activating nodes\n",
      "Neuron 89: No activating nodes\n",
      "Neuron 90: No activating nodes\n",
      "Neuron 91: No activating nodes\n",
      "Neuron 92: No activating nodes\n",
      "Neuron 93: No activating nodes\n",
      "Neuron 94: No activating nodes\n",
      "Neuron 95: No activating nodes\n",
      "Neuron 96: No activating nodes\n",
      "Neuron 97: No activating nodes\n",
      "Neuron 98: No activating nodes\n",
      "Neuron 99: No activating nodes\n",
      "Neuron 100: No activating nodes\n",
      "Neuron 101: Found 1 activating nodes\n",
      "Neuron 102: No activating nodes\n",
      "Neuron 103: No activating nodes\n",
      "Neuron 104: No activating nodes\n",
      "Neuron 105: No activating nodes\n",
      "Neuron 106: No activating nodes\n",
      "Neuron 107: No activating nodes\n",
      "Neuron 108: No activating nodes\n",
      "Neuron 109: No activating nodes\n",
      "Neuron 110: No activating nodes\n",
      "Neuron 111: No activating nodes\n",
      "Neuron 112: No activating nodes\n",
      "Neuron 113: Found 1 activating nodes\n",
      "Neuron 114: No activating nodes\n",
      "Neuron 115: No activating nodes\n",
      "Neuron 116: Found 1 activating nodes\n",
      "Neuron 117: No activating nodes\n",
      "Neuron 118: No activating nodes\n",
      "Neuron 119: Found 1 activating nodes\n",
      "Neuron 120: Found 1 activating nodes\n",
      "Neuron 121: No activating nodes\n",
      "Neuron 122: No activating nodes\n",
      "Neuron 123: No activating nodes\n",
      "Neuron 124: No activating nodes\n",
      "Neuron 125: No activating nodes\n",
      "Neuron 126: Found 1 activating nodes\n",
      "Neuron 127: No activating nodes\n",
      "Neuron 128: No activating nodes\n",
      "Neuron 129: No activating nodes\n",
      "Neuron 130: No activating nodes\n",
      "Neuron 131: No activating nodes\n",
      "Neuron 132: No activating nodes\n",
      "Neuron 133: No activating nodes\n",
      "Neuron 134: No activating nodes\n",
      "Neuron 135: No activating nodes\n",
      "Neuron 136: No activating nodes\n",
      "Neuron 137: No activating nodes\n",
      "Neuron 138: No activating nodes\n",
      "Neuron 139: No activating nodes\n",
      "Neuron 140: No activating nodes\n",
      "Neuron 141: No activating nodes\n",
      "Neuron 142: No activating nodes\n",
      "Neuron 143: No activating nodes\n",
      "Neuron 144: No activating nodes\n",
      "Neuron 145: Found 1 activating nodes\n",
      "Neuron 146: No activating nodes\n",
      "Neuron 147: No activating nodes\n",
      "Neuron 148: Found 1 activating nodes\n",
      "Neuron 149: No activating nodes\n",
      "Neuron 150: No activating nodes\n",
      "Neuron 151: No activating nodes\n",
      "Neuron 152: No activating nodes\n",
      "Neuron 153: Found 1 activating nodes\n",
      "Neuron 154: No activating nodes\n",
      "Neuron 155: No activating nodes\n",
      "Neuron 156: No activating nodes\n",
      "Neuron 157: No activating nodes\n",
      "Neuron 158: No activating nodes\n",
      "Neuron 159: No activating nodes\n",
      "Neuron 160: No activating nodes\n",
      "Neuron 161: No activating nodes\n",
      "Neuron 162: No activating nodes\n",
      "Neuron 163: Found 1 activating nodes\n",
      "Neuron 164: Found 2 activating nodes\n",
      "Neuron 165: No activating nodes\n",
      "Neuron 166: No activating nodes\n",
      "Neuron 167: No activating nodes\n",
      "Neuron 168: No activating nodes\n",
      "Neuron 169: No activating nodes\n",
      "Neuron 170: No activating nodes\n",
      "Neuron 171: No activating nodes\n",
      "Neuron 172: No activating nodes\n",
      "Neuron 173: No activating nodes\n",
      "Neuron 174: No activating nodes\n",
      "Neuron 175: No activating nodes\n",
      "Neuron 176: No activating nodes\n",
      "Neuron 177: No activating nodes\n",
      "Neuron 178: No activating nodes\n",
      "Neuron 179: No activating nodes\n",
      "Neuron 180: No activating nodes\n",
      "Neuron 181: No activating nodes\n",
      "Neuron 182: No activating nodes\n",
      "Neuron 183: No activating nodes\n",
      "Neuron 184: No activating nodes\n",
      "Neuron 185: No activating nodes\n",
      "Neuron 186: No activating nodes\n",
      "Neuron 187: No activating nodes\n",
      "Neuron 188: Found 2 activating nodes\n",
      "Neuron 189: No activating nodes\n",
      "Neuron 190: No activating nodes\n",
      "Neuron 191: No activating nodes\n",
      "Neuron 192: No activating nodes\n",
      "Neuron 193: No activating nodes\n",
      "Neuron 194: No activating nodes\n",
      "Neuron 195: No activating nodes\n",
      "Neuron 196: No activating nodes\n",
      "Neuron 197: No activating nodes\n",
      "Neuron 198: No activating nodes\n",
      "Neuron 199: No activating nodes\n",
      "\n",
      "Layer 4 Summary:\n",
      "Files found: 200\n",
      "Files with activating nodes: 11\n",
      "Total activating nodes: 13\n",
      "\n",
      "Detailed status:\n",
      "Neuron 0: No activating nodes\n",
      "Neuron 1: No activating nodes\n",
      "Neuron 2: No activating nodes\n",
      "Neuron 3: No activating nodes\n",
      "Neuron 4: No activating nodes\n",
      "Neuron 5: No activating nodes\n",
      "Neuron 6: Found 1 activating nodes\n",
      "Neuron 7: No activating nodes\n",
      "Neuron 8: No activating nodes\n",
      "Neuron 9: No activating nodes\n",
      "Neuron 10: No activating nodes\n",
      "Neuron 11: No activating nodes\n",
      "Neuron 12: No activating nodes\n",
      "Neuron 13: No activating nodes\n",
      "Neuron 14: No activating nodes\n",
      "Neuron 15: No activating nodes\n",
      "Neuron 16: No activating nodes\n",
      "Neuron 17: No activating nodes\n",
      "Neuron 18: No activating nodes\n",
      "Neuron 19: Found 1 activating nodes\n",
      "Neuron 20: No activating nodes\n",
      "Neuron 21: No activating nodes\n",
      "Neuron 22: No activating nodes\n",
      "Neuron 23: No activating nodes\n",
      "Neuron 24: No activating nodes\n",
      "Neuron 25: No activating nodes\n",
      "Neuron 26: No activating nodes\n",
      "Neuron 27: No activating nodes\n",
      "Neuron 28: No activating nodes\n",
      "Neuron 29: No activating nodes\n",
      "Neuron 30: No activating nodes\n",
      "Neuron 31: No activating nodes\n",
      "Neuron 32: No activating nodes\n",
      "Neuron 33: No activating nodes\n",
      "Neuron 34: No activating nodes\n",
      "Neuron 35: No activating nodes\n",
      "Neuron 36: No activating nodes\n",
      "Neuron 37: No activating nodes\n",
      "Neuron 38: No activating nodes\n",
      "Neuron 39: No activating nodes\n",
      "Neuron 40: No activating nodes\n",
      "Neuron 41: No activating nodes\n",
      "Neuron 42: No activating nodes\n",
      "Neuron 43: No activating nodes\n",
      "Neuron 44: No activating nodes\n",
      "Neuron 45: No activating nodes\n",
      "Neuron 46: No activating nodes\n",
      "Neuron 47: No activating nodes\n",
      "Neuron 48: No activating nodes\n",
      "Neuron 49: Found 1 activating nodes\n",
      "Neuron 50: No activating nodes\n",
      "Neuron 51: No activating nodes\n",
      "Neuron 52: No activating nodes\n",
      "Neuron 53: No activating nodes\n",
      "Neuron 54: No activating nodes\n",
      "Neuron 55: No activating nodes\n",
      "Neuron 56: No activating nodes\n",
      "Neuron 57: No activating nodes\n",
      "Neuron 58: No activating nodes\n",
      "Neuron 59: No activating nodes\n",
      "Neuron 60: No activating nodes\n",
      "Neuron 61: No activating nodes\n",
      "Neuron 62: Found 1 activating nodes\n",
      "Neuron 63: No activating nodes\n",
      "Neuron 64: No activating nodes\n",
      "Neuron 65: No activating nodes\n",
      "Neuron 66: No activating nodes\n",
      "Neuron 67: No activating nodes\n",
      "Neuron 68: No activating nodes\n",
      "Neuron 69: No activating nodes\n",
      "Neuron 70: No activating nodes\n",
      "Neuron 71: No activating nodes\n",
      "Neuron 72: No activating nodes\n",
      "Neuron 73: No activating nodes\n",
      "Neuron 74: No activating nodes\n",
      "Neuron 75: No activating nodes\n",
      "Neuron 76: No activating nodes\n",
      "Neuron 77: No activating nodes\n",
      "Neuron 78: No activating nodes\n",
      "Neuron 79: No activating nodes\n",
      "Neuron 80: No activating nodes\n",
      "Neuron 81: No activating nodes\n",
      "Neuron 82: No activating nodes\n",
      "Neuron 83: No activating nodes\n",
      "Neuron 84: No activating nodes\n",
      "Neuron 85: No activating nodes\n",
      "Neuron 86: No activating nodes\n",
      "Neuron 87: Found 2 activating nodes\n",
      "Neuron 88: No activating nodes\n",
      "Neuron 89: No activating nodes\n",
      "Neuron 90: No activating nodes\n",
      "Neuron 91: No activating nodes\n",
      "Neuron 92: No activating nodes\n",
      "Neuron 93: No activating nodes\n",
      "Neuron 94: No activating nodes\n",
      "Neuron 95: No activating nodes\n",
      "Neuron 96: No activating nodes\n",
      "Neuron 97: No activating nodes\n",
      "Neuron 98: No activating nodes\n",
      "Neuron 99: No activating nodes\n",
      "Neuron 100: No activating nodes\n",
      "Neuron 101: No activating nodes\n",
      "Neuron 102: No activating nodes\n",
      "Neuron 103: No activating nodes\n",
      "Neuron 104: Found 1 activating nodes\n",
      "Neuron 105: No activating nodes\n",
      "Neuron 106: No activating nodes\n",
      "Neuron 107: No activating nodes\n",
      "Neuron 108: No activating nodes\n",
      "Neuron 109: No activating nodes\n",
      "Neuron 110: No activating nodes\n",
      "Neuron 111: Found 1 activating nodes\n",
      "Neuron 112: No activating nodes\n",
      "Neuron 113: No activating nodes\n",
      "Neuron 114: No activating nodes\n",
      "Neuron 115: No activating nodes\n",
      "Neuron 116: No activating nodes\n",
      "Neuron 117: No activating nodes\n",
      "Neuron 118: No activating nodes\n",
      "Neuron 119: No activating nodes\n",
      "Neuron 120: No activating nodes\n",
      "Neuron 121: No activating nodes\n",
      "Neuron 122: No activating nodes\n",
      "Neuron 123: Found 1 activating nodes\n",
      "Neuron 124: Found 2 activating nodes\n",
      "Neuron 125: No activating nodes\n",
      "Neuron 126: No activating nodes\n",
      "Neuron 127: No activating nodes\n",
      "Neuron 128: No activating nodes\n",
      "Neuron 129: No activating nodes\n",
      "Neuron 130: No activating nodes\n",
      "Neuron 131: No activating nodes\n",
      "Neuron 132: No activating nodes\n",
      "Neuron 133: No activating nodes\n",
      "Neuron 134: Found 1 activating nodes\n",
      "Neuron 135: Found 1 activating nodes\n",
      "Neuron 136: No activating nodes\n",
      "Neuron 137: No activating nodes\n",
      "Neuron 138: No activating nodes\n",
      "Neuron 139: No activating nodes\n",
      "Neuron 140: No activating nodes\n",
      "Neuron 141: No activating nodes\n",
      "Neuron 142: No activating nodes\n",
      "Neuron 143: No activating nodes\n",
      "Neuron 144: No activating nodes\n",
      "Neuron 145: No activating nodes\n",
      "Neuron 146: No activating nodes\n",
      "Neuron 147: No activating nodes\n",
      "Neuron 148: No activating nodes\n",
      "Neuron 149: No activating nodes\n",
      "Neuron 150: No activating nodes\n",
      "Neuron 151: No activating nodes\n",
      "Neuron 152: No activating nodes\n",
      "Neuron 153: No activating nodes\n",
      "Neuron 154: No activating nodes\n",
      "Neuron 155: No activating nodes\n",
      "Neuron 156: No activating nodes\n",
      "Neuron 157: No activating nodes\n",
      "Neuron 158: No activating nodes\n",
      "Neuron 159: No activating nodes\n",
      "Neuron 160: No activating nodes\n",
      "Neuron 161: No activating nodes\n",
      "Neuron 162: No activating nodes\n",
      "Neuron 163: No activating nodes\n",
      "Neuron 164: No activating nodes\n",
      "Neuron 165: No activating nodes\n",
      "Neuron 166: No activating nodes\n",
      "Neuron 167: No activating nodes\n",
      "Neuron 168: No activating nodes\n",
      "Neuron 169: No activating nodes\n",
      "Neuron 170: No activating nodes\n",
      "Neuron 171: No activating nodes\n",
      "Neuron 172: No activating nodes\n",
      "Neuron 173: No activating nodes\n",
      "Neuron 174: No activating nodes\n",
      "Neuron 175: No activating nodes\n",
      "Neuron 176: No activating nodes\n",
      "Neuron 177: No activating nodes\n",
      "Neuron 178: No activating nodes\n",
      "Neuron 179: No activating nodes\n",
      "Neuron 180: No activating nodes\n",
      "Neuron 181: No activating nodes\n",
      "Neuron 182: No activating nodes\n",
      "Neuron 183: No activating nodes\n",
      "Neuron 184: No activating nodes\n",
      "Neuron 185: No activating nodes\n",
      "Neuron 186: No activating nodes\n",
      "Neuron 187: No activating nodes\n",
      "Neuron 188: No activating nodes\n",
      "Neuron 189: No activating nodes\n",
      "Neuron 190: No activating nodes\n",
      "Neuron 191: No activating nodes\n",
      "Neuron 192: No activating nodes\n",
      "Neuron 193: No activating nodes\n",
      "Neuron 194: No activating nodes\n",
      "Neuron 195: No activating nodes\n",
      "Neuron 196: No activating nodes\n",
      "Neuron 197: No activating nodes\n",
      "Neuron 198: No activating nodes\n",
      "Neuron 199: No activating nodes\n",
      "\n",
      "Layer 5 Summary:\n",
      "Files found: 200\n",
      "Files with activating nodes: 10\n",
      "Total activating nodes: 12\n",
      "\n",
      "Detailed status:\n",
      "Neuron 0: No activating nodes\n",
      "Neuron 1: No activating nodes\n",
      "Neuron 2: No activating nodes\n",
      "Neuron 3: No activating nodes\n",
      "Neuron 4: No activating nodes\n",
      "Neuron 5: No activating nodes\n",
      "Neuron 6: No activating nodes\n",
      "Neuron 7: No activating nodes\n",
      "Neuron 8: No activating nodes\n",
      "Neuron 9: No activating nodes\n",
      "Neuron 10: No activating nodes\n",
      "Neuron 11: No activating nodes\n",
      "Neuron 12: No activating nodes\n",
      "Neuron 13: No activating nodes\n",
      "Neuron 14: No activating nodes\n",
      "Neuron 15: No activating nodes\n",
      "Neuron 16: No activating nodes\n",
      "Neuron 17: No activating nodes\n",
      "Neuron 18: Found 1 activating nodes\n",
      "Neuron 19: No activating nodes\n",
      "Neuron 20: No activating nodes\n",
      "Neuron 21: No activating nodes\n",
      "Neuron 22: No activating nodes\n",
      "Neuron 23: No activating nodes\n",
      "Neuron 24: No activating nodes\n",
      "Neuron 25: No activating nodes\n",
      "Neuron 26: No activating nodes\n",
      "Neuron 27: No activating nodes\n",
      "Neuron 28: No activating nodes\n",
      "Neuron 29: No activating nodes\n",
      "Neuron 30: No activating nodes\n",
      "Neuron 31: No activating nodes\n",
      "Neuron 32: No activating nodes\n",
      "Neuron 33: No activating nodes\n",
      "Neuron 34: No activating nodes\n",
      "Neuron 35: No activating nodes\n",
      "Neuron 36: No activating nodes\n",
      "Neuron 37: No activating nodes\n",
      "Neuron 38: No activating nodes\n",
      "Neuron 39: No activating nodes\n",
      "Neuron 40: No activating nodes\n",
      "Neuron 41: No activating nodes\n",
      "Neuron 42: No activating nodes\n",
      "Neuron 43: No activating nodes\n",
      "Neuron 44: No activating nodes\n",
      "Neuron 45: No activating nodes\n",
      "Neuron 46: No activating nodes\n",
      "Neuron 47: No activating nodes\n",
      "Neuron 48: No activating nodes\n",
      "Neuron 49: No activating nodes\n",
      "Neuron 50: No activating nodes\n",
      "Neuron 51: No activating nodes\n",
      "Neuron 52: No activating nodes\n",
      "Neuron 53: No activating nodes\n",
      "Neuron 54: No activating nodes\n",
      "Neuron 55: No activating nodes\n",
      "Neuron 56: No activating nodes\n",
      "Neuron 57: No activating nodes\n",
      "Neuron 58: No activating nodes\n",
      "Neuron 59: No activating nodes\n",
      "Neuron 60: No activating nodes\n",
      "Neuron 61: No activating nodes\n",
      "Neuron 62: No activating nodes\n",
      "Neuron 63: No activating nodes\n",
      "Neuron 64: No activating nodes\n",
      "Neuron 65: No activating nodes\n",
      "Neuron 66: No activating nodes\n",
      "Neuron 67: No activating nodes\n",
      "Neuron 68: No activating nodes\n",
      "Neuron 69: No activating nodes\n",
      "Neuron 70: No activating nodes\n",
      "Neuron 71: No activating nodes\n",
      "Neuron 72: No activating nodes\n",
      "Neuron 73: No activating nodes\n",
      "Neuron 74: No activating nodes\n",
      "Neuron 75: No activating nodes\n",
      "Neuron 76: No activating nodes\n",
      "Neuron 77: No activating nodes\n",
      "Neuron 78: No activating nodes\n",
      "Neuron 79: No activating nodes\n",
      "Neuron 80: Found 1 activating nodes\n",
      "Neuron 81: No activating nodes\n",
      "Neuron 82: No activating nodes\n",
      "Neuron 83: No activating nodes\n",
      "Neuron 84: No activating nodes\n",
      "Neuron 85: No activating nodes\n",
      "Neuron 86: No activating nodes\n",
      "Neuron 87: No activating nodes\n",
      "Neuron 88: No activating nodes\n",
      "Neuron 89: No activating nodes\n",
      "Neuron 90: No activating nodes\n",
      "Neuron 91: No activating nodes\n",
      "Neuron 92: No activating nodes\n",
      "Neuron 93: No activating nodes\n",
      "Neuron 94: No activating nodes\n",
      "Neuron 95: No activating nodes\n",
      "Neuron 96: No activating nodes\n",
      "Neuron 97: No activating nodes\n",
      "Neuron 98: No activating nodes\n",
      "Neuron 99: No activating nodes\n",
      "Neuron 100: No activating nodes\n",
      "Neuron 101: Found 1 activating nodes\n",
      "Neuron 102: No activating nodes\n",
      "Neuron 103: No activating nodes\n",
      "Neuron 104: No activating nodes\n",
      "Neuron 105: No activating nodes\n",
      "Neuron 106: No activating nodes\n",
      "Neuron 107: No activating nodes\n",
      "Neuron 108: No activating nodes\n",
      "Neuron 109: No activating nodes\n",
      "Neuron 110: No activating nodes\n",
      "Neuron 111: Found 1 activating nodes\n",
      "Neuron 112: No activating nodes\n",
      "Neuron 113: No activating nodes\n",
      "Neuron 114: Found 1 activating nodes\n",
      "Neuron 115: No activating nodes\n",
      "Neuron 116: No activating nodes\n",
      "Neuron 117: No activating nodes\n",
      "Neuron 118: No activating nodes\n",
      "Neuron 119: No activating nodes\n",
      "Neuron 120: No activating nodes\n",
      "Neuron 121: No activating nodes\n",
      "Neuron 122: Found 1 activating nodes\n",
      "Neuron 123: No activating nodes\n",
      "Neuron 124: No activating nodes\n",
      "Neuron 125: No activating nodes\n",
      "Neuron 126: No activating nodes\n",
      "Neuron 127: No activating nodes\n",
      "Neuron 128: No activating nodes\n",
      "Neuron 129: No activating nodes\n",
      "Neuron 130: No activating nodes\n",
      "Neuron 131: No activating nodes\n",
      "Neuron 132: No activating nodes\n",
      "Neuron 133: No activating nodes\n",
      "Neuron 134: No activating nodes\n",
      "Neuron 135: No activating nodes\n",
      "Neuron 136: No activating nodes\n",
      "Neuron 137: No activating nodes\n",
      "Neuron 138: No activating nodes\n",
      "Neuron 139: No activating nodes\n",
      "Neuron 140: No activating nodes\n",
      "Neuron 141: No activating nodes\n",
      "Neuron 142: No activating nodes\n",
      "Neuron 143: Found 1 activating nodes\n",
      "Neuron 144: No activating nodes\n",
      "Neuron 145: No activating nodes\n",
      "Neuron 146: Found 1 activating nodes\n",
      "Neuron 147: No activating nodes\n",
      "Neuron 148: No activating nodes\n",
      "Neuron 149: No activating nodes\n",
      "Neuron 150: No activating nodes\n",
      "Neuron 151: No activating nodes\n",
      "Neuron 152: No activating nodes\n",
      "Neuron 153: No activating nodes\n",
      "Neuron 154: No activating nodes\n",
      "Neuron 155: No activating nodes\n",
      "Neuron 156: Found 2 activating nodes\n",
      "Neuron 157: No activating nodes\n",
      "Neuron 158: No activating nodes\n",
      "Neuron 159: No activating nodes\n",
      "Neuron 160: Found 2 activating nodes\n",
      "Neuron 161: No activating nodes\n",
      "Neuron 162: No activating nodes\n",
      "Neuron 163: No activating nodes\n",
      "Neuron 164: No activating nodes\n",
      "Neuron 165: No activating nodes\n",
      "Neuron 166: No activating nodes\n",
      "Neuron 167: No activating nodes\n",
      "Neuron 168: No activating nodes\n",
      "Neuron 169: No activating nodes\n",
      "Neuron 170: No activating nodes\n",
      "Neuron 171: No activating nodes\n",
      "Neuron 172: No activating nodes\n",
      "Neuron 173: No activating nodes\n",
      "Neuron 174: No activating nodes\n",
      "Neuron 175: No activating nodes\n",
      "Neuron 176: No activating nodes\n",
      "Neuron 177: No activating nodes\n",
      "Neuron 178: No activating nodes\n",
      "Neuron 179: No activating nodes\n",
      "Neuron 180: No activating nodes\n",
      "Neuron 181: No activating nodes\n",
      "Neuron 182: No activating nodes\n",
      "Neuron 183: No activating nodes\n",
      "Neuron 184: No activating nodes\n",
      "Neuron 185: No activating nodes\n",
      "Neuron 186: No activating nodes\n",
      "Neuron 187: No activating nodes\n",
      "Neuron 188: No activating nodes\n",
      "Neuron 189: No activating nodes\n",
      "Neuron 190: No activating nodes\n",
      "Neuron 191: No activating nodes\n",
      "Neuron 192: No activating nodes\n",
      "Neuron 193: No activating nodes\n",
      "Neuron 194: No activating nodes\n",
      "Neuron 195: No activating nodes\n",
      "Neuron 196: No activating nodes\n",
      "Neuron 197: No activating nodes\n",
      "Neuron 198: No activating nodes\n",
      "Neuron 199: No activating nodes\n",
      "\n",
      "Layer 6 Summary:\n",
      "Files found: 200\n",
      "Files with activating nodes: 7\n",
      "Total activating nodes: 8\n",
      "\n",
      "Detailed status:\n",
      "Neuron 0: No activating nodes\n",
      "Neuron 1: No activating nodes\n",
      "Neuron 2: No activating nodes\n",
      "Neuron 3: No activating nodes\n",
      "Neuron 4: No activating nodes\n",
      "Neuron 5: No activating nodes\n",
      "Neuron 6: No activating nodes\n",
      "Neuron 7: No activating nodes\n",
      "Neuron 8: No activating nodes\n",
      "Neuron 9: No activating nodes\n",
      "Neuron 10: No activating nodes\n",
      "Neuron 11: No activating nodes\n",
      "Neuron 12: No activating nodes\n",
      "Neuron 13: No activating nodes\n",
      "Neuron 14: No activating nodes\n",
      "Neuron 15: No activating nodes\n",
      "Neuron 16: No activating nodes\n",
      "Neuron 17: No activating nodes\n",
      "Neuron 18: No activating nodes\n",
      "Neuron 19: No activating nodes\n",
      "Neuron 20: No activating nodes\n",
      "Neuron 21: No activating nodes\n",
      "Neuron 22: No activating nodes\n",
      "Neuron 23: No activating nodes\n",
      "Neuron 24: No activating nodes\n",
      "Neuron 25: No activating nodes\n",
      "Neuron 26: No activating nodes\n",
      "Neuron 27: No activating nodes\n",
      "Neuron 28: No activating nodes\n",
      "Neuron 29: No activating nodes\n",
      "Neuron 30: No activating nodes\n",
      "Neuron 31: No activating nodes\n",
      "Neuron 32: No activating nodes\n",
      "Neuron 33: No activating nodes\n",
      "Neuron 34: No activating nodes\n",
      "Neuron 35: No activating nodes\n",
      "Neuron 36: No activating nodes\n",
      "Neuron 37: No activating nodes\n",
      "Neuron 38: No activating nodes\n",
      "Neuron 39: No activating nodes\n",
      "Neuron 40: No activating nodes\n",
      "Neuron 41: No activating nodes\n",
      "Neuron 42: No activating nodes\n",
      "Neuron 43: No activating nodes\n",
      "Neuron 44: No activating nodes\n",
      "Neuron 45: No activating nodes\n",
      "Neuron 46: No activating nodes\n",
      "Neuron 47: No activating nodes\n",
      "Neuron 48: No activating nodes\n",
      "Neuron 49: No activating nodes\n",
      "Neuron 50: No activating nodes\n",
      "Neuron 51: No activating nodes\n",
      "Neuron 52: No activating nodes\n",
      "Neuron 53: No activating nodes\n",
      "Neuron 54: No activating nodes\n",
      "Neuron 55: No activating nodes\n",
      "Neuron 56: No activating nodes\n",
      "Neuron 57: No activating nodes\n",
      "Neuron 58: No activating nodes\n",
      "Neuron 59: No activating nodes\n",
      "Neuron 60: No activating nodes\n",
      "Neuron 61: No activating nodes\n",
      "Neuron 62: No activating nodes\n",
      "Neuron 63: No activating nodes\n",
      "Neuron 64: No activating nodes\n",
      "Neuron 65: No activating nodes\n",
      "Neuron 66: No activating nodes\n",
      "Neuron 67: No activating nodes\n",
      "Neuron 68: No activating nodes\n",
      "Neuron 69: No activating nodes\n",
      "Neuron 70: No activating nodes\n",
      "Neuron 71: No activating nodes\n",
      "Neuron 72: No activating nodes\n",
      "Neuron 73: No activating nodes\n",
      "Neuron 74: No activating nodes\n",
      "Neuron 75: No activating nodes\n",
      "Neuron 76: No activating nodes\n",
      "Neuron 77: No activating nodes\n",
      "Neuron 78: No activating nodes\n",
      "Neuron 79: No activating nodes\n",
      "Neuron 80: No activating nodes\n",
      "Neuron 81: No activating nodes\n",
      "Neuron 82: No activating nodes\n",
      "Neuron 83: Found 1 activating nodes\n",
      "Neuron 84: No activating nodes\n",
      "Neuron 85: No activating nodes\n",
      "Neuron 86: No activating nodes\n",
      "Neuron 87: No activating nodes\n",
      "Neuron 88: No activating nodes\n",
      "Neuron 89: No activating nodes\n",
      "Neuron 90: No activating nodes\n",
      "Neuron 91: No activating nodes\n",
      "Neuron 92: No activating nodes\n",
      "Neuron 93: No activating nodes\n",
      "Neuron 94: No activating nodes\n",
      "Neuron 95: No activating nodes\n",
      "Neuron 96: No activating nodes\n",
      "Neuron 97: No activating nodes\n",
      "Neuron 98: Found 1 activating nodes\n",
      "Neuron 99: No activating nodes\n",
      "Neuron 100: No activating nodes\n",
      "Neuron 101: No activating nodes\n",
      "Neuron 102: No activating nodes\n",
      "Neuron 103: No activating nodes\n",
      "Neuron 104: No activating nodes\n",
      "Neuron 105: No activating nodes\n",
      "Neuron 106: No activating nodes\n",
      "Neuron 107: No activating nodes\n",
      "Neuron 108: No activating nodes\n",
      "Neuron 109: Found 1 activating nodes\n",
      "Neuron 110: No activating nodes\n",
      "Neuron 111: No activating nodes\n",
      "Neuron 112: No activating nodes\n",
      "Neuron 113: No activating nodes\n",
      "Neuron 114: No activating nodes\n",
      "Neuron 115: No activating nodes\n",
      "Neuron 116: Found 1 activating nodes\n",
      "Neuron 117: No activating nodes\n",
      "Neuron 118: No activating nodes\n",
      "Neuron 119: No activating nodes\n",
      "Neuron 120: No activating nodes\n",
      "Neuron 121: No activating nodes\n",
      "Neuron 122: No activating nodes\n",
      "Neuron 123: No activating nodes\n",
      "Neuron 124: No activating nodes\n",
      "Neuron 125: No activating nodes\n",
      "Neuron 126: No activating nodes\n",
      "Neuron 127: No activating nodes\n",
      "Neuron 128: No activating nodes\n",
      "Neuron 129: No activating nodes\n",
      "Neuron 130: No activating nodes\n",
      "Neuron 131: No activating nodes\n",
      "Neuron 132: No activating nodes\n",
      "Neuron 133: No activating nodes\n",
      "Neuron 134: No activating nodes\n",
      "Neuron 135: No activating nodes\n",
      "Neuron 136: No activating nodes\n",
      "Neuron 137: No activating nodes\n",
      "Neuron 138: No activating nodes\n",
      "Neuron 139: No activating nodes\n",
      "Neuron 140: No activating nodes\n",
      "Neuron 141: No activating nodes\n",
      "Neuron 142: No activating nodes\n",
      "Neuron 143: No activating nodes\n",
      "Neuron 144: No activating nodes\n",
      "Neuron 145: No activating nodes\n",
      "Neuron 146: No activating nodes\n",
      "Neuron 147: No activating nodes\n",
      "Neuron 148: No activating nodes\n",
      "Neuron 149: No activating nodes\n",
      "Neuron 150: No activating nodes\n",
      "Neuron 151: No activating nodes\n",
      "Neuron 152: No activating nodes\n",
      "Neuron 153: No activating nodes\n",
      "Neuron 154: No activating nodes\n",
      "Neuron 155: No activating nodes\n",
      "Neuron 156: No activating nodes\n",
      "Neuron 157: No activating nodes\n",
      "Neuron 158: No activating nodes\n",
      "Neuron 159: Found 2 activating nodes\n",
      "Neuron 160: No activating nodes\n",
      "Neuron 161: No activating nodes\n",
      "Neuron 162: No activating nodes\n",
      "Neuron 163: No activating nodes\n",
      "Neuron 164: No activating nodes\n",
      "Neuron 165: No activating nodes\n",
      "Neuron 166: No activating nodes\n",
      "Neuron 167: No activating nodes\n",
      "Neuron 168: No activating nodes\n",
      "Neuron 169: No activating nodes\n",
      "Neuron 170: No activating nodes\n",
      "Neuron 171: No activating nodes\n",
      "Neuron 172: No activating nodes\n",
      "Neuron 173: No activating nodes\n",
      "Neuron 174: No activating nodes\n",
      "Neuron 175: No activating nodes\n",
      "Neuron 176: No activating nodes\n",
      "Neuron 177: No activating nodes\n",
      "Neuron 178: No activating nodes\n",
      "Neuron 179: Found 1 activating nodes\n",
      "Neuron 180: No activating nodes\n",
      "Neuron 181: No activating nodes\n",
      "Neuron 182: No activating nodes\n",
      "Neuron 183: No activating nodes\n",
      "Neuron 184: No activating nodes\n",
      "Neuron 185: No activating nodes\n",
      "Neuron 186: No activating nodes\n",
      "Neuron 187: No activating nodes\n",
      "Neuron 188: No activating nodes\n",
      "Neuron 189: No activating nodes\n",
      "Neuron 190: No activating nodes\n",
      "Neuron 191: No activating nodes\n",
      "Neuron 192: No activating nodes\n",
      "Neuron 193: No activating nodes\n",
      "Neuron 194: No activating nodes\n",
      "Neuron 195: No activating nodes\n",
      "Neuron 196: No activating nodes\n",
      "Neuron 197: No activating nodes\n",
      "Neuron 198: Found 1 activating nodes\n",
      "Neuron 199: No activating nodes\n",
      "\n",
      "Layer 7 Summary:\n",
      "Files found: 200\n",
      "Files with activating nodes: 10\n",
      "Total activating nodes: 13\n",
      "\n",
      "Detailed status:\n",
      "Neuron 0: No activating nodes\n",
      "Neuron 1: No activating nodes\n",
      "Neuron 2: No activating nodes\n",
      "Neuron 3: No activating nodes\n",
      "Neuron 4: No activating nodes\n",
      "Neuron 5: No activating nodes\n",
      "Neuron 6: No activating nodes\n",
      "Neuron 7: No activating nodes\n",
      "Neuron 8: No activating nodes\n",
      "Neuron 9: Found 1 activating nodes\n",
      "Neuron 10: No activating nodes\n",
      "Neuron 11: No activating nodes\n",
      "Neuron 12: No activating nodes\n",
      "Neuron 13: No activating nodes\n",
      "Neuron 14: Found 1 activating nodes\n",
      "Neuron 15: No activating nodes\n",
      "Neuron 16: No activating nodes\n",
      "Neuron 17: No activating nodes\n",
      "Neuron 18: No activating nodes\n",
      "Neuron 19: No activating nodes\n",
      "Neuron 20: No activating nodes\n",
      "Neuron 21: No activating nodes\n",
      "Neuron 22: No activating nodes\n",
      "Neuron 23: No activating nodes\n",
      "Neuron 24: No activating nodes\n",
      "Neuron 25: No activating nodes\n",
      "Neuron 26: No activating nodes\n",
      "Neuron 27: No activating nodes\n",
      "Neuron 28: No activating nodes\n",
      "Neuron 29: No activating nodes\n",
      "Neuron 30: No activating nodes\n",
      "Neuron 31: No activating nodes\n",
      "Neuron 32: No activating nodes\n",
      "Neuron 33: No activating nodes\n",
      "Neuron 34: No activating nodes\n",
      "Neuron 35: No activating nodes\n",
      "Neuron 36: No activating nodes\n",
      "Neuron 37: No activating nodes\n",
      "Neuron 38: No activating nodes\n",
      "Neuron 39: No activating nodes\n",
      "Neuron 40: No activating nodes\n",
      "Neuron 41: No activating nodes\n",
      "Neuron 42: No activating nodes\n",
      "Neuron 43: No activating nodes\n",
      "Neuron 44: No activating nodes\n",
      "Neuron 45: No activating nodes\n",
      "Neuron 46: No activating nodes\n",
      "Neuron 47: No activating nodes\n",
      "Neuron 48: No activating nodes\n",
      "Neuron 49: No activating nodes\n",
      "Neuron 50: No activating nodes\n",
      "Neuron 51: No activating nodes\n",
      "Neuron 52: No activating nodes\n",
      "Neuron 53: No activating nodes\n",
      "Neuron 54: No activating nodes\n",
      "Neuron 55: No activating nodes\n",
      "Neuron 56: No activating nodes\n",
      "Neuron 57: No activating nodes\n",
      "Neuron 58: Found 2 activating nodes\n",
      "Neuron 59: No activating nodes\n",
      "Neuron 60: No activating nodes\n",
      "Neuron 61: No activating nodes\n",
      "Neuron 62: No activating nodes\n",
      "Neuron 63: No activating nodes\n",
      "Neuron 64: No activating nodes\n",
      "Neuron 65: No activating nodes\n",
      "Neuron 66: No activating nodes\n",
      "Neuron 67: No activating nodes\n",
      "Neuron 68: No activating nodes\n",
      "Neuron 69: No activating nodes\n",
      "Neuron 70: Found 1 activating nodes\n",
      "Neuron 71: No activating nodes\n",
      "Neuron 72: No activating nodes\n",
      "Neuron 73: No activating nodes\n",
      "Neuron 74: No activating nodes\n",
      "Neuron 75: No activating nodes\n",
      "Neuron 76: No activating nodes\n",
      "Neuron 77: No activating nodes\n",
      "Neuron 78: No activating nodes\n",
      "Neuron 79: No activating nodes\n",
      "Neuron 80: No activating nodes\n",
      "Neuron 81: No activating nodes\n",
      "Neuron 82: No activating nodes\n",
      "Neuron 83: No activating nodes\n",
      "Neuron 84: No activating nodes\n",
      "Neuron 85: No activating nodes\n",
      "Neuron 86: No activating nodes\n",
      "Neuron 87: Found 1 activating nodes\n",
      "Neuron 88: No activating nodes\n",
      "Neuron 89: No activating nodes\n",
      "Neuron 90: No activating nodes\n",
      "Neuron 91: No activating nodes\n",
      "Neuron 92: No activating nodes\n",
      "Neuron 93: No activating nodes\n",
      "Neuron 94: No activating nodes\n",
      "Neuron 95: No activating nodes\n",
      "Neuron 96: No activating nodes\n",
      "Neuron 97: No activating nodes\n",
      "Neuron 98: No activating nodes\n",
      "Neuron 99: No activating nodes\n",
      "Neuron 100: No activating nodes\n",
      "Neuron 101: No activating nodes\n",
      "Neuron 102: Found 1 activating nodes\n",
      "Neuron 103: No activating nodes\n",
      "Neuron 104: No activating nodes\n",
      "Neuron 105: Found 1 activating nodes\n",
      "Neuron 106: No activating nodes\n",
      "Neuron 107: No activating nodes\n",
      "Neuron 108: No activating nodes\n",
      "Neuron 109: No activating nodes\n",
      "Neuron 110: No activating nodes\n",
      "Neuron 111: No activating nodes\n",
      "Neuron 112: No activating nodes\n",
      "Neuron 113: No activating nodes\n",
      "Neuron 114: No activating nodes\n",
      "Neuron 115: No activating nodes\n",
      "Neuron 116: No activating nodes\n",
      "Neuron 117: No activating nodes\n",
      "Neuron 118: No activating nodes\n",
      "Neuron 119: No activating nodes\n",
      "Neuron 120: No activating nodes\n",
      "Neuron 121: No activating nodes\n",
      "Neuron 122: No activating nodes\n",
      "Neuron 123: No activating nodes\n",
      "Neuron 124: No activating nodes\n",
      "Neuron 125: No activating nodes\n",
      "Neuron 126: No activating nodes\n",
      "Neuron 127: Found 2 activating nodes\n",
      "Neuron 128: No activating nodes\n",
      "Neuron 129: No activating nodes\n",
      "Neuron 130: No activating nodes\n",
      "Neuron 131: No activating nodes\n",
      "Neuron 132: No activating nodes\n",
      "Neuron 133: No activating nodes\n",
      "Neuron 134: No activating nodes\n",
      "Neuron 135: No activating nodes\n",
      "Neuron 136: No activating nodes\n",
      "Neuron 137: No activating nodes\n",
      "Neuron 138: Found 1 activating nodes\n",
      "Neuron 139: No activating nodes\n",
      "Neuron 140: No activating nodes\n",
      "Neuron 141: No activating nodes\n",
      "Neuron 142: No activating nodes\n",
      "Neuron 143: No activating nodes\n",
      "Neuron 144: No activating nodes\n",
      "Neuron 145: No activating nodes\n",
      "Neuron 146: No activating nodes\n",
      "Neuron 147: Found 2 activating nodes\n",
      "Neuron 148: No activating nodes\n",
      "Neuron 149: No activating nodes\n",
      "Neuron 150: No activating nodes\n",
      "Neuron 151: No activating nodes\n",
      "Neuron 152: No activating nodes\n",
      "Neuron 153: No activating nodes\n",
      "Neuron 154: No activating nodes\n",
      "Neuron 155: No activating nodes\n",
      "Neuron 156: No activating nodes\n",
      "Neuron 157: No activating nodes\n",
      "Neuron 158: No activating nodes\n",
      "Neuron 159: No activating nodes\n",
      "Neuron 160: No activating nodes\n",
      "Neuron 161: No activating nodes\n",
      "Neuron 162: No activating nodes\n",
      "Neuron 163: No activating nodes\n",
      "Neuron 164: No activating nodes\n",
      "Neuron 165: No activating nodes\n",
      "Neuron 166: No activating nodes\n",
      "Neuron 167: No activating nodes\n",
      "Neuron 168: No activating nodes\n",
      "Neuron 169: No activating nodes\n",
      "Neuron 170: No activating nodes\n",
      "Neuron 171: No activating nodes\n",
      "Neuron 172: No activating nodes\n",
      "Neuron 173: No activating nodes\n",
      "Neuron 174: No activating nodes\n",
      "Neuron 175: No activating nodes\n",
      "Neuron 176: No activating nodes\n",
      "Neuron 177: No activating nodes\n",
      "Neuron 178: No activating nodes\n",
      "Neuron 179: No activating nodes\n",
      "Neuron 180: No activating nodes\n",
      "Neuron 181: No activating nodes\n",
      "Neuron 182: No activating nodes\n",
      "Neuron 183: No activating nodes\n",
      "Neuron 184: No activating nodes\n",
      "Neuron 185: No activating nodes\n",
      "Neuron 186: No activating nodes\n",
      "Neuron 187: No activating nodes\n",
      "Neuron 188: No activating nodes\n",
      "Neuron 189: No activating nodes\n",
      "Neuron 190: No activating nodes\n",
      "Neuron 191: No activating nodes\n",
      "Neuron 192: No activating nodes\n",
      "Neuron 193: No activating nodes\n",
      "Neuron 194: No activating nodes\n",
      "Neuron 195: No activating nodes\n",
      "Neuron 196: No activating nodes\n",
      "Neuron 197: No activating nodes\n",
      "Neuron 198: No activating nodes\n",
      "Neuron 199: No activating nodes\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def check_graph_files(start_layer=0, end_layer=7, neurons_per_layer=200):\n",
    "    base_path = Path('neuron_graphs/lbl')\n",
    "    \n",
    "    for layer in range(start_layer, end_layer + 1):\n",
    "        print(f\"\\nLayer {layer} Summary:\")\n",
    "        found_files = 0\n",
    "        files_with_nodes = 0\n",
    "        total_activating_nodes = 0\n",
    "        \n",
    "        # Track which neurons have graphs and activating nodes\n",
    "        neuron_status = []\n",
    "        \n",
    "        # Check each neuron\n",
    "        for neuron in range(neurons_per_layer):\n",
    "            graph_path = base_path / f'layer_{layer}' / f'l{layer}_n{neuron}_graph.json'\n",
    "            \n",
    "            if graph_path.exists():\n",
    "                found_files += 1\n",
    "                try:\n",
    "                    with open(graph_path) as f:\n",
    "                        graph_data = json.load(f)\n",
    "                        \n",
    "                    # Count activating nodes\n",
    "                    activating_nodes = [n for n, data in graph_data['nodes'].items() \n",
    "                                      if data.get('is_activating', False)]\n",
    "                    \n",
    "                    if activating_nodes:\n",
    "                        files_with_nodes += 1\n",
    "                        total_activating_nodes += len(activating_nodes)\n",
    "                        status = f\"Found {len(activating_nodes)} activating nodes\"\n",
    "                    else:\n",
    "                        status = \"No activating nodes\"\n",
    "                except json.JSONDecodeError:\n",
    "                    status = \"Invalid JSON\"\n",
    "                except Exception as e:\n",
    "                    status = f\"Error: {str(e)}\"\n",
    "            else:\n",
    "                status = \"File not found\"\n",
    "            \n",
    "            neuron_status.append(f\"Neuron {neuron}: {status}\")\n",
    "        \n",
    "        # Print summary for this layer\n",
    "        print(f\"Files found: {found_files}\")\n",
    "        print(f\"Files with activating nodes: {files_with_nodes}\")\n",
    "        print(f\"Total activating nodes: {total_activating_nodes}\")\n",
    "        print(\"\\nDetailed status:\")\n",
    "        for status in neuron_status:\n",
    "            print(status)\n",
    "            \n",
    "# Run the check\n",
    "check_graph_files(start_layer=3, end_layer=7, neurons_per_layer=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Loading model...\n",
      "Model loaded successfully from configs/lbl_model_20241016.pt\n",
      "Model type: ablated (layer-by-layer)\n",
      "Number of parameters: 8709504\n",
      "Model loaded successfully!\n",
      "\n",
      "Initializing analyzer...\n",
      "Initializing NeuronAnalyzer...\n",
      "Detected ablated model - initializing HookedTransformer\n",
      "HookedTransformer initialized successfully\n",
      "NeuronAnalyzer initialized (Model type: ablated)\n",
      "Loading BERT model for token suggestions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzer initialized!\n",
      "\n",
      "Results will be saved to: substitution_results_20250117_151849\n",
      "\n",
      "Test configuration:\n",
      "start_layer: 3\n",
      "end_layer: 7\n",
      "debug_mode: True\n",
      "\n",
      "Starting analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 18 neurons with activating patterns in layer 3:\n",
      "Neuron 18: 1 patterns\n",
      "Neuron 21: 1 patterns\n",
      "Neuron 39: 1 patterns\n",
      "Neuron 53: 1 patterns\n",
      "Neuron 60: 1 patterns\n",
      "Neuron 80: 1 patterns\n",
      "Neuron 101: 1 patterns\n",
      "Neuron 113: 1 patterns\n",
      "Neuron 116: 1 patterns\n",
      "Neuron 119: 1 patterns\n",
      "Neuron 120: 1 patterns\n",
      "Neuron 126: 1 patterns\n",
      "Neuron 145: 1 patterns\n",
      "Neuron 148: 1 patterns\n",
      "Neuron 153: 1 patterns\n",
      "Neuron 163: 1 patterns\n",
      "Neuron 164: 2 patterns\n",
      "Neuron 188: 2 patterns\n",
      "\n",
      "Analyzing L3N18\n",
      "\n",
      "Analyzing L3N18\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  box\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  the\n",
      "\n",
      "Substitution:  box -> .\n",
      "Activation: -0.0003\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  the\n",
      "\n",
      "Substitution:  box -> ;\n",
      "Activation: -0.0044\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  the\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  the\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  the\n",
      "\n",
      "Substitution:  box -> |\n",
      "Activation: -0.0867\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "Found 3 substitutions\n",
      "\n",
      "Analyzing L3N21\n",
      "\n",
      "Analyzing L3N21\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern: \"\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: \" -> .\n",
      "Activation: -0.0002\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: \" -> -\n",
      "Activation: -0.0763\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "Found 2 substitutions\n",
      "\n",
      "Analyzing L3N39\n",
      "\n",
      "Analyzing L3N39\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  live\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  live -> .\n",
      "Activation: -0.0100\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  live -> |\n",
      "Activation: -0.0004\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  live -> !\n",
      "Activation: -0.0006\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 3 substitutions\n",
      "\n",
      "Analyzing L3N53\n",
      "\n",
      "Analyzing L3N53\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  the\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  the -> address\n",
      "Activation: -0.0002\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "Found 1 substitutions\n",
      "\n",
      "Analyzing L3N60\n",
      "\n",
      "Analyzing L3N60\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern: ,\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: , -> rest\n",
      "Activation: 0.5992\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: , -> down\n",
      "Activation: 2.4289\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 2 substitutions\n",
      "\n",
      "Analyzing L3N80\n",
      "\n",
      "Analyzing L3N80\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  animal\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  favorite\n",
      "\n",
      "Substitution:  animal -> .\n",
      "Activation: -0.0037\n",
      "Ratio to max: 7.4119\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  favorite\n",
      "\n",
      "Substitution:  animal -> !\n",
      "Activation: -0.0258\n",
      "Ratio to max: 51.2895\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  favorite\n",
      "\n",
      "Substitution:  animal -> ;\n",
      "Activation: 0.2453\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  favorite\n",
      "\n",
      "Substitution:  animal -> ?\n",
      "Activation: 1.2210\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  favorite\n",
      "\n",
      "Substitution:  animal -> |\n",
      "Activation: -0.0835\n",
      "Ratio to max: 165.8232\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Analyzing L3N101\n",
      "\n",
      "Analyzing L3N101\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern: ?\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  of\n",
      "\n",
      "Substitution: ? -> course\n",
      "Activation: -0.0167\n",
      "Ratio to max: 0.0020\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  of\n",
      "\n",
      "Substitution: ? -> god\n",
      "Activation: -0.0850\n",
      "Ratio to max: 0.0104\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  off\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  of\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  of\n",
      "\n",
      "Substitution: ? -> christ\n",
      "Activation: -0.0626\n",
      "Ratio to max: 0.0076\n",
      "Meets minimum threshold\n",
      "Found 3 substitutions\n",
      "\n",
      "Analyzing L3N113\n",
      "\n",
      "Analyzing L3N113\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  family\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  family -> .\n",
      "Activation: -0.0828\n",
      "Ratio to max: 0.4109\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  family -> |\n",
      "Activation: 1.0303\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 2 substitutions\n",
      "\n",
      "Analyzing L3N116\n",
      "\n",
      "Analyzing L3N116\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  The\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  The -> one\n",
      "Activation: -0.0138\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "Found 1 substitutions\n",
      "\n",
      "Analyzing L3N119\n",
      "\n",
      "Analyzing L3N119\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern: \"\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  shouted\n",
      "\n",
      "Substitution: \" -> out\n",
      "Activation: -0.0377\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  shouted\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  shouted\n",
      "\n",
      "Substitution: \" -> !\n",
      "Activation: -0.0030\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  shouted\n",
      "\n",
      "Substitution: \" -> ,\n",
      "Activation: -0.0768\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  shouted\n",
      "\n",
      "Substitution: \" -> down\n",
      "Activation: -0.0880\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "Found 4 substitutions\n",
      "\n",
      "Analyzing L3N120\n",
      "\n",
      "Analyzing L3N120\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  needle\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  needle -> .\n",
      "Activation: 2.9591\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  needle -> ;\n",
      "Activation: 2.8210\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  needle -> !\n",
      "Activation: 3.4155\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  needle -> ?\n",
      "Activation: 2.1955\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  needle -> |\n",
      "Activation: 1.1490\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Analyzing L3N126\n",
      "\n",
      "Analyzing L3N126\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern: .\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " - ep\n",
      "\n",
      "Substitution: . -> .\n",
      "Activation: -0.0697\n",
      "Ratio to max: 91.6551\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 1 substitutions\n",
      "\n",
      "Analyzing L3N145\n",
      "\n",
      "Analyzing L3N145\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  live\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  live -> .\n",
      "Activation: 0.1364\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  live -> ;\n",
      "Activation: -0.1670\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  live -> !\n",
      "Activation: -0.1393\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  live -> ?\n",
      "Activation: -0.1432\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  live -> |\n",
      "Activation: -0.1687\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Analyzing L3N148\n",
      "\n",
      "Analyzing L3N148\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern: ,\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: , -> name\n",
      "Activation: -0.0002\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: , -> death\n",
      "Activation: -0.0012\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: , -> mother\n",
      "Activation: -0.0009\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: , -> husband\n",
      "Activation: -0.0052\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: , -> house\n",
      "Activation: -0.0001\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Analyzing L3N153\n",
      "\n",
      "Analyzing L3N153\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  car\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  car -> .\n",
      "Activation: -0.0020\n",
      "Ratio to max: 0.8637\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  car -> |\n",
      "Activation: -0.0004\n",
      "Ratio to max: 0.1566\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  car -> ?\n",
      "Activation: -0.0065\n",
      "Ratio to max: 2.8367\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "Found 3 substitutions\n",
      "\n",
      "Analyzing L3N163\n",
      "\n",
      "Analyzing L3N163\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern: 's\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: 's -> ?\n",
      "Activation: 0.2360\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: 's -> .\n",
      "Activation: -0.1059\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: 's -> !\n",
      "Activation: -0.1055\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: 's -> ;\n",
      "Activation: -0.0314\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: 's -> ...\n",
      "Activation: -0.0018\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Analyzing L3N164\n",
      "\n",
      "Analyzing L3N164\n",
      "Found 2 activating patterns\n",
      "\n",
      "Testing pattern:  them\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  with\n",
      "\n",
      "Substitution:  them -> .\n",
      "Activation: -0.1206\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  with\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  with\n",
      "\n",
      "Substitution:  them -> |\n",
      "Activation: -0.0028\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  with\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  with\n",
      "\n",
      "Substitution:  them -> !\n",
      "Activation: -0.0015\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Testing pattern:  gently\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  paw\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  paw\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  paw\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  paw\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  paw\n",
      "Found 3 substitutions\n",
      "\n",
      "Analyzing L3N188\n",
      "\n",
      "Analyzing L3N188\n",
      "Found 2 activating patterns\n",
      "\n",
      "Testing pattern: .\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: . -> out\n",
      "Activation: 2.1092\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: . -> ,\n",
      "Activation: -0.0574\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: . -> in\n",
      "Activation: 0.3073\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: . -> down\n",
      "Activation: 0.0081\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: . -> up\n",
      "Activation: 0.0044\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Testing pattern: \n",
      "\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: \n",
      " -> out\n",
      "Activation: 2.1092\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: \n",
      " -> ,\n",
      "Activation: -0.0574\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: \n",
      " -> in\n",
      "Activation: 0.3073\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: \n",
      " -> down\n",
      "Activation: 0.0081\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  20%|██        | 1/5 [00:45<03:01, 45.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: \n",
      " -> up\n",
      "Activation: 0.0044\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 10 substitutions\n",
      "\n",
      "Layer 3 complete: 63 total substitutions\n",
      "\n",
      "Found 11 neurons with activating patterns in layer 4:\n",
      "Neuron 6: 1 patterns\n",
      "Neuron 19: 1 patterns\n",
      "Neuron 49: 1 patterns\n",
      "Neuron 62: 1 patterns\n",
      "Neuron 87: 2 patterns\n",
      "Neuron 104: 1 patterns\n",
      "Neuron 111: 1 patterns\n",
      "Neuron 123: 1 patterns\n",
      "Neuron 124: 2 patterns\n",
      "Neuron 134: 1 patterns\n",
      "Neuron 135: 1 patterns\n",
      "\n",
      "Analyzing L4N6\n",
      "\n",
      "Analyzing L4N6\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  are\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  are -> .\n",
      "Activation: -0.0991\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  are -> ;\n",
      "Activation: -0.0080\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "Found 2 substitutions\n",
      "\n",
      "Analyzing L4N19\n",
      "\n",
      "Analyzing L4N19\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  you\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  you -> .\n",
      "Activation: -0.1268\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  you -> !\n",
      "Activation: -0.1472\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  you -> ;\n",
      "Activation: -0.1007\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  you -> ?\n",
      "Activation: -0.1546\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  you -> |\n",
      "Activation: -0.1068\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Analyzing L4N49\n",
      "\n",
      "Analyzing L4N49\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  were\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L4N62\n",
      "\n",
      "Analyzing L4N62\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  even\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " - ..\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L4N87\n",
      "\n",
      "Analyzing L4N87\n",
      "Found 2 activating patterns\n",
      "\n",
      "Testing pattern:  a\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  a -> me\n",
      "Activation: -0.0014\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Testing pattern:  so\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  It\n",
      "\n",
      "Substitution:  so -> .\n",
      "Activation: -0.0003\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  It\n",
      "\n",
      "Substitution:  so -> !\n",
      "Activation: -0.0091\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  It\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  It\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  It\n",
      "Found 3 substitutions\n",
      "\n",
      "Analyzing L4N104\n",
      "\n",
      "Analyzing L4N104\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  cried\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " - 't\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " - 't\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " - 't\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " - 't\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " - 't\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L4N111\n",
      "\n",
      "Analyzing L4N111\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  not\n",
      "\n",
      "Found 3 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  are\n",
      " -  did\n",
      " -  were\n",
      "\n",
      "Substitution:  not -> .\n",
      "Activation: -0.0054\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 3 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  are\n",
      " -  did\n",
      " -  were\n",
      "\n",
      "Substitution:  not -> ;\n",
      "Activation: -0.0007\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 3 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  are\n",
      " -  did\n",
      " -  were\n",
      "\n",
      "Substitution:  not -> ?\n",
      "Activation: -0.0503\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 3 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  are\n",
      " -  did\n",
      " -  were\n",
      "\n",
      "Substitution:  not -> !\n",
      "Activation: -0.0626\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 3 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  are\n",
      " -  did\n",
      " -  were\n",
      "\n",
      "Substitution:  not -> |\n",
      "Activation: -0.0005\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Analyzing L4N123\n",
      "\n",
      "Analyzing L4N123\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  dad\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L4N124\n",
      "\n",
      "Analyzing L4N124\n",
      "Found 2 activating patterns\n",
      "\n",
      "Testing pattern:  in\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  in -> store\n",
      "Activation: -0.0054\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  in -> only\n",
      "Activation: -0.0894\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  in -> '\n",
      "Activation: -0.0251\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  in -> box\n",
      "Activation: -0.0220\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Testing pattern:  more\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  more -> .\n",
      "Activation: -0.0006\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  more -> |\n",
      "Activation: -0.0003\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "Found 6 substitutions\n",
      "\n",
      "Analyzing L4N134\n",
      "\n",
      "Analyzing L4N134\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  goodbye\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L4N135\n",
      "\n",
      "Analyzing L4N135\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  candy\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  candy -> .\n",
      "Activation: 1.6221\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  candy -> !\n",
      "Activation: 0.0727\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  candy -> ;\n",
      "Activation: -0.0518\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  candy -> ?\n",
      "Activation: 0.1122\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  40%|████      | 2/5 [01:18<01:54, 38.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  candy -> |\n",
      "Activation: -0.0559\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Layer 4 complete: 26 total substitutions\n",
      "\n",
      "Found 10 neurons with activating patterns in layer 5:\n",
      "Neuron 18: 1 patterns\n",
      "Neuron 80: 1 patterns\n",
      "Neuron 101: 1 patterns\n",
      "Neuron 111: 1 patterns\n",
      "Neuron 114: 1 patterns\n",
      "Neuron 122: 1 patterns\n",
      "Neuron 143: 1 patterns\n",
      "Neuron 146: 1 patterns\n",
      "Neuron 156: 2 patterns\n",
      "Neuron 160: 2 patterns\n",
      "\n",
      "Analyzing L5N18\n",
      "\n",
      "Analyzing L5N18\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern: \"\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: \" -> a\n",
      "Activation: -0.0024\n",
      "Ratio to max: 0.0503\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: \" -> yes\n",
      "Activation: -0.0386\n",
      "Ratio to max: 0.8006\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: \" -> no\n",
      "Activation: -0.1076\n",
      "Ratio to max: 2.2328\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "Found 3 substitutions\n",
      "\n",
      "Analyzing L5N80\n",
      "\n",
      "Analyzing L5N80\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  why\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  why -> ;\n",
      "Activation: -0.0234\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  why -> |\n",
      "Activation: -0.0024\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  why -> ?\n",
      "Activation: -0.0012\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  why -> !\n",
      "Activation: -0.1517\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "Found 4 substitutions\n",
      "\n",
      "Analyzing L5N101\n",
      "\n",
      "Analyzing L5N101\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  pink\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  pink -> .\n",
      "Activation: -0.0014\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  pink -> ;\n",
      "Activation: -0.0044\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  pink -> |\n",
      "Activation: -0.1125\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "Found 3 substitutions\n",
      "\n",
      "Analyzing L5N111\n",
      "\n",
      "Analyzing L5N111\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern: \"\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: \" -> yes\n",
      "Activation: 1.0472\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: \" -> no\n",
      "Activation: -0.1420\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "Found 2 substitutions\n",
      "\n",
      "Analyzing L5N114\n",
      "\n",
      "Analyzing L5N114\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  wiped\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  wiped -> .\n",
      "Activation: 0.9767\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  wiped -> !\n",
      "Activation: 4.6328\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  wiped -> ?\n",
      "Activation: 0.9332\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  wiped -> ;\n",
      "Activation: 5.2434\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  wiped -> |\n",
      "Activation: 5.8472\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Analyzing L5N122\n",
      "\n",
      "Analyzing L5N122\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern: ed\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: ed -> .\n",
      "Activation: -0.0051\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: ed -> ;\n",
      "Activation: -0.0010\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "Found 2 substitutions\n",
      "\n",
      "Analyzing L5N143\n",
      "\n",
      "Analyzing L5N143\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  buttons\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  buttons -> .\n",
      "Activation: -0.0264\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  buttons -> ;\n",
      "Activation: 0.9003\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  buttons -> ?\n",
      "Activation: -0.0026\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "Found 3 substitutions\n",
      "\n",
      "Analyzing L5N146\n",
      "\n",
      "Analyzing L5N146\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  in\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  in -> made\n",
      "Activation: -0.0006\n",
      "Ratio to max: 0.0079\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  in -> ,\n",
      "Activation: -0.0081\n",
      "Ratio to max: 0.1074\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  in -> ...\n",
      "Activation: -0.0019\n",
      "Ratio to max: 0.0250\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  in -> broken\n",
      "Activation: -0.1362\n",
      "Ratio to max: 1.8025\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  in -> .\n",
      "Activation: -0.0636\n",
      "Ratio to max: 0.8425\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Analyzing L5N156\n",
      "\n",
      "Analyzing L5N156\n",
      "Found 2 activating patterns\n",
      "\n",
      "Testing pattern:  likes\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  it\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  it\n",
      "\n",
      "Substitution:  likes -> !\n",
      "Activation: -0.0040\n",
      "Ratio to max: 10.7308\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  it\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  it\n",
      "\n",
      "Substitution:  likes -> ;\n",
      "Activation: -0.0002\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  it\n",
      "\n",
      "Testing pattern: ,\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: , -> ,\n",
      "Activation: -0.1698\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "Found 3 substitutions\n",
      "\n",
      "Analyzing L5N160\n",
      "\n",
      "Analyzing L5N160\n",
      "Found 2 activating patterns\n",
      "\n",
      "Testing pattern:  too\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  late\n",
      "\n",
      "Substitution:  too -> .\n",
      "Activation: 0.8582\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  late\n",
      "\n",
      "Substitution:  too -> !\n",
      "Activation: -0.0851\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  late\n",
      "\n",
      "Substitution:  too -> ?\n",
      "Activation: 1.0425\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  late\n",
      "\n",
      "Substitution:  too -> ;\n",
      "Activation: 0.9925\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  late\n",
      "\n",
      "Substitution:  too -> |\n",
      "Activation: -0.0335\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Testing pattern:  loved\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  60%|██████    | 3/5 [01:48<01:08, 34.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "Found 5 substitutions\n",
      "\n",
      "Layer 5 complete: 35 total substitutions\n",
      "\n",
      "Found 7 neurons with activating patterns in layer 6:\n",
      "Neuron 83: 1 patterns\n",
      "Neuron 98: 1 patterns\n",
      "Neuron 109: 1 patterns\n",
      "Neuron 116: 1 patterns\n",
      "Neuron 159: 2 patterns\n",
      "Neuron 179: 1 patterns\n",
      "Neuron 198: 1 patterns\n",
      "\n",
      "Analyzing L6N83\n",
      "\n",
      "Analyzing L6N83\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  friends\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L6N98\n",
      "\n",
      "Analyzing L6N98\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  Tim\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L6N109\n",
      "\n",
      "Analyzing L6N109\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  tails\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " - .\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " - .?\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " - .\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L6N116\n",
      "\n",
      "Analyzing L6N116\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  wished\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  wished -> .\n",
      "Activation: -0.0010\n",
      "Ratio to max: 0.0330\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  wished -> !\n",
      "Activation: -0.0002\n",
      "Ratio to max: 0.0064\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  wished -> ;\n",
      "Activation: -0.0004\n",
      "Ratio to max: 0.0142\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  wished -> ?\n",
      "Activation: -0.1364\n",
      "Ratio to max: 4.5836\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  wished -> |\n",
      "Activation: -0.0218\n",
      "Ratio to max: 0.7314\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Analyzing L6N159\n",
      "\n",
      "Analyzing L6N159\n",
      "Found 2 activating patterns\n",
      "\n",
      "Testing pattern:  while\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Testing pattern:  But\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  But -> .\n",
      "Activation: -0.0433\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  But -> !\n",
      "Activation: -0.0749\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  But -> ;\n",
      "Activation: -0.0353\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  But -> ?\n",
      "Activation: -0.0113\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  But -> |\n",
      "Activation: -0.1700\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Analyzing L6N179\n",
      "\n",
      "Analyzing L6N179\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern: my\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: my -> .\n",
      "Activation: -0.1318\n",
      "Ratio to max: 759.9187\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: my -> ;\n",
      "Activation: 2.8492\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: my -> ?\n",
      "Activation: -0.0135\n",
      "Ratio to max: 77.9271\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: my -> !\n",
      "Activation: -0.0440\n",
      "Ratio to max: 253.5088\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: my -> |\n",
      "Activation: -0.0983\n",
      "Ratio to max: 566.8556\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Analyzing L6N198\n",
      "\n",
      "Analyzing L6N198\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern: \"\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: \" -> i\n",
      "Activation: -0.0745\n",
      "Ratio to max: 0.8606\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: \" -> a\n",
      "Activation: -0.0001\n",
      "Ratio to max: 0.0014\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: \" -> yes\n",
      "Activation: -0.1031\n",
      "Ratio to max: 1.1919\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: \" -> no\n",
      "Activation: 0.0835\n",
      "Ratio to max: 0.9651\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:  80%|████████  | 4/5 [02:07<00:28, 28.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution: \" -> 1\n",
      "Activation: -0.0034\n",
      "Ratio to max: 0.0393\n",
      "Meets minimum threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Layer 6 complete: 20 total substitutions\n",
      "\n",
      "Found 10 neurons with activating patterns in layer 7:\n",
      "Neuron 9: 1 patterns\n",
      "Neuron 14: 1 patterns\n",
      "Neuron 58: 2 patterns\n",
      "Neuron 70: 1 patterns\n",
      "Neuron 87: 1 patterns\n",
      "Neuron 102: 1 patterns\n",
      "Neuron 105: 1 patterns\n",
      "Neuron 127: 2 patterns\n",
      "Neuron 138: 1 patterns\n",
      "Neuron 147: 2 patterns\n",
      "\n",
      "Analyzing L7N9\n",
      "\n",
      "Analyzing L7N9\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  started\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  started -> .\n",
      "Activation: 3.0420\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  started -> ;\n",
      "Activation: 2.9627\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  started -> !\n",
      "Activation: 2.6050\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  started -> ?\n",
      "Activation: 2.0830\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  started -> |\n",
      "Activation: 2.5894\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Analyzing L7N14\n",
      "\n",
      "Analyzing L7N14\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern: ers\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  hamm\n",
      "\n",
      "Substitution: ers -> !\n",
      "Activation: -0.0385\n",
      "Ratio to max: 6.3175\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  hamm\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  hamm\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  hamm\n",
      "\n",
      "Substitution: ers -> ?\n",
      "Activation: -0.0736\n",
      "Ratio to max: 12.0793\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  hamm\n",
      "Found 2 substitutions\n",
      "\n",
      "Analyzing L7N58\n",
      "\n",
      "Analyzing L7N58\n",
      "Found 2 activating patterns\n",
      "\n",
      "Testing pattern:  and\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  as\n",
      "\n",
      "Substitution:  and -> .\n",
      "Activation: 2.1634\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  as\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  as\n",
      "\n",
      "Substitution:  and -> well\n",
      "Activation: 4.0169\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  as\n",
      "\n",
      "Substitution:  and -> |\n",
      "Activation: -0.1111\n",
      "Ratio to max: 0.1107\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  as\n",
      "\n",
      "Substitution:  and -> in\n",
      "Activation: 1.7346\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Testing pattern:  using\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  as\n",
      "\n",
      "Substitution:  using -> .\n",
      "Activation: 2.1634\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  as\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  as\n",
      "\n",
      "Substitution:  using -> |\n",
      "Activation: -0.1111\n",
      "Ratio to max: 0.1107\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  as\n",
      "\n",
      "Substitution:  using -> ?\n",
      "Activation: 0.0362\n",
      "Ratio to max: 0.0361\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  as\n",
      "\n",
      "Substitution:  using -> !\n",
      "Activation: 3.8238\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 8 substitutions\n",
      "\n",
      "Analyzing L7N70\n",
      "\n",
      "Analyzing L7N70\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  mouth\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  its\n",
      "\n",
      "Substitution:  mouth -> .\n",
      "Activation: -0.1483\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  its\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  its\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  its\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  its\n",
      "\n",
      "Substitution:  mouth -> !\n",
      "Activation: -0.0499\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "Found 2 substitutions\n",
      "\n",
      "Analyzing L7N87\n",
      "\n",
      "Analyzing L7N87\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  said\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  mom\n",
      "\n",
      "Substitution:  said -> .\n",
      "Activation: 1.7585\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  mom\n",
      "\n",
      "Substitution:  said -> !\n",
      "Activation: 2.6532\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  mom\n",
      "\n",
      "Substitution:  said -> ?\n",
      "Activation: 2.7931\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  mom\n",
      "\n",
      "Substitution:  said -> ;\n",
      "Activation: 3.0330\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  mom\n",
      "\n",
      "Substitution:  said -> |\n",
      "Activation: -0.0169\n",
      "Ratio to max: 3.6662\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Analyzing L7N102\n",
      "\n",
      "Analyzing L7N102\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  front\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  front -> ;\n",
      "Activation: 1.6329\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  front -> .\n",
      "Activation: 0.8180\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  front -> |\n",
      "Activation: 2.7932\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  front -> !\n",
      "Activation: 0.5344\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  front -> ?\n",
      "Activation: 3.1804\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Analyzing L7N105\n",
      "\n",
      "Analyzing L7N105\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  that\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  that -> .\n",
      "Activation: -0.0165\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  that -> !\n",
      "Activation: -0.0184\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  that -> ;\n",
      "Activation: -0.0083\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  that -> ?\n",
      "Activation: -0.1201\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  that -> |\n",
      "Activation: -0.0049\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Analyzing L7N127\n",
      "\n",
      "Analyzing L7N127\n",
      "Found 2 activating patterns\n",
      "\n",
      "Testing pattern: .\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Testing pattern:  split\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  was\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  was\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  was\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  was\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  was\n",
      "No substitutions found\n",
      "\n",
      "Analyzing L7N138\n",
      "\n",
      "Analyzing L7N138\n",
      "Found 1 activating patterns\n",
      "\n",
      "Testing pattern:  sky\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  castle\n",
      "\n",
      "Substitution:  sky -> .\n",
      "Activation: -0.0084\n",
      "Ratio to max: 2.2209\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  castle\n",
      "\n",
      "Substitution:  sky -> ;\n",
      "Activation: -0.0130\n",
      "Ratio to max: 3.4493\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  castle\n",
      "\n",
      "Substitution:  sky -> !\n",
      "Activation: -0.1560\n",
      "Ratio to max: 41.4368\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  castle\n",
      "\n",
      "Substitution:  sky -> ?\n",
      "Activation: -0.0751\n",
      "Ratio to max: 19.9566\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 1 important tokens using threshold 0.5\n",
      "Important tokens found:\n",
      " -  castle\n",
      "\n",
      "Substitution:  sky -> |\n",
      "Activation: -0.1645\n",
      "Ratio to max: 43.7125\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "Found 5 substitutions\n",
      "\n",
      "Analyzing L7N147\n",
      "\n",
      "Analyzing L7N147\n",
      "Found 2 activating patterns\n",
      "\n",
      "Testing pattern:  way\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  way -> ;\n",
      "Activation: 0.0616\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  way -> |\n",
      "Activation: 0.2355\n",
      "Ratio to max: 1.0000\n",
      "Meets minimum threshold\n",
      "Meets relative threshold\n",
      "\n",
      "Testing pattern:  scary\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  scary -> .\n",
      "Activation: -0.0019\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Substitution:  scary -> ;\n",
      "Activation: -0.0059\n",
      "Ratio to max: 0.0000\n",
      "Meets minimum threshold\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers: 100%|██████████| 5/5 [02:40<00:00, 32.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 0 important tokens using threshold 0.5\n",
      "No tokens above threshold - neuron may be suppressed by ablation\n",
      "Found 4 substitutions\n",
      "\n",
      "Layer 7 complete: 41 total substitutions\n",
      "\n",
      "Saving results...\n",
      "Results saved to: substitution_results_20250117_151849/substitution_analysis.json\n",
      "\n",
      "Summary Statistics:\n",
      "Total layers analyzed: 5\n",
      "Layers with results: 5\n",
      "Total substitutions found: 185\n",
      "Average substitutions per layer with results: 37.00\n",
      "\n",
      "Cleaning up...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from model_loader import load_model\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def find_neurons_with_activations(layer, base_path='neuron_graphs/lbl'):\n",
    "    \"\"\"Find all neurons in a layer that have activating patterns\"\"\"\n",
    "    neurons = []\n",
    "    layer_path = Path(base_path) / f'layer_{layer}'\n",
    "    \n",
    "    if not layer_path.exists():\n",
    "        print(f\"Warning: Layer path {layer_path} not found\")\n",
    "        return []\n",
    "        \n",
    "    for graph_file in layer_path.glob(f'l{layer}_n*_graph.json'):\n",
    "        try:\n",
    "            # Extract neuron number from filename like 'l3_n144_graph'\n",
    "            neuron = int(graph_file.stem.split('_n')[1].split('_')[0])\n",
    "            with open(graph_file) as f:\n",
    "                graph_data = json.load(f)\n",
    "                \n",
    "            # Check for activating nodes\n",
    "            activating_nodes = [n for n, data in graph_data['nodes'].items() \n",
    "                              if data.get('is_activating', False)]\n",
    "                              \n",
    "            if activating_nodes:\n",
    "                neurons.append((neuron, len(activating_nodes)))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {graph_file}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return sorted(neurons)  # Sort by neuron index\n",
    "\n",
    "# Set up device and paths\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nLoading model...\")\n",
    "model_path = 'configs/lbl_model_20241016.pt'\n",
    "model, config = load_model(model_path, device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Initialize analyzer\n",
    "print(\"\\nInitializing analyzer...\")\n",
    "analyzer = AblatedBatchedTokenSubstitutionAnalyzer(model, device)\n",
    "print(\"Analyzer initialized!\")\n",
    "\n",
    "# Create output directory for results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = Path(f'substitution_results_{timestamp}')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "print(f\"\\nResults will be saved to: {output_dir}\")\n",
    "\n",
    "# Test parameters\n",
    "test_config = {\n",
    "    'start_layer': 3,\n",
    "    'end_layer': 7,\n",
    "    'debug_mode': True\n",
    "}\n",
    "\n",
    "print(\"\\nTest configuration:\")\n",
    "for key, value in test_config.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Run analysis with targeted neurons\n",
    "print(\"\\nStarting analysis...\")\n",
    "results = {}\n",
    "try:\n",
    "    # Iterate through layers\n",
    "    layer_pbar = tqdm(range(test_config['start_layer'], test_config['end_layer'] + 1), \n",
    "                     desc=\"Processing layers\")\n",
    "    \n",
    "    for layer in layer_pbar:\n",
    "        # Find neurons with activating patterns\n",
    "        neurons = find_neurons_with_activations(layer)\n",
    "        if not neurons:\n",
    "            print(f\"\\nNo neurons with activating patterns found in layer {layer}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nFound {len(neurons)} neurons with activating patterns in layer {layer}:\")\n",
    "        for neuron, num_patterns in neurons:\n",
    "            print(f\"Neuron {neuron}: {num_patterns} patterns\")\n",
    "            \n",
    "        layer_results = []\n",
    "        for neuron, _ in neurons:\n",
    "            try:\n",
    "                # Check graph file\n",
    "                graph_path = Path('neuron_graphs/lbl') / f'layer_{layer}' / f'l{layer}_n{neuron}_graph.json'\n",
    "                if not graph_path.exists():\n",
    "                    print(f\"\\nWarning: No graph file found at {graph_path}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Analyze neuron\n",
    "                print(f\"\\nAnalyzing L{layer}N{neuron}\")\n",
    "                substitutions = analyzer.analyze_neuron_substitutions(\n",
    "                    layer=layer,\n",
    "                    neuron=neuron,\n",
    "                    graph_path=graph_path,\n",
    "                    debug_mode=test_config['debug_mode']\n",
    "                )\n",
    "                \n",
    "                # Process results\n",
    "                if substitutions:\n",
    "                    print(f\"Found {len(substitutions)} substitutions\")\n",
    "                    layer_results.extend(substitutions)\n",
    "                else:\n",
    "                    print(\"No substitutions found\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing neuron {neuron} in layer {layer}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        # Store layer results if any found\n",
    "        if layer_results:\n",
    "            results[layer] = layer_results\n",
    "            print(f\"\\nLayer {layer} complete: {len(layer_results)} total substitutions\")\n",
    "        else:\n",
    "            print(f\"\\nLayer {layer} complete: No substitutions found\")\n",
    "    \n",
    "    # Save results\n",
    "    if results:\n",
    "        print(\"\\nSaving results...\")\n",
    "        summary_data = {\n",
    "            'timestamp': timestamp,\n",
    "            'config': test_config,\n",
    "            'results': results\n",
    "        }\n",
    "        \n",
    "        output_file = output_dir / 'substitution_analysis.json'\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(summary_data, f, indent=2)\n",
    "            \n",
    "        print(f\"Results saved to: {output_file}\")\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        total_layers = len(range(test_config['start_layer'], test_config['end_layer'] + 1))\n",
    "        layers_with_results = len(results)\n",
    "        total_substitutions = sum(len(layer_results) for layer_results in results.values())\n",
    "        avg_substitutions = total_substitutions / layers_with_results if layers_with_results else 0\n",
    "        \n",
    "        print(f\"Total layers analyzed: {total_layers}\")\n",
    "        print(f\"Layers with results: {layers_with_results}\")\n",
    "        print(f\"Total substitutions found: {total_substitutions}\")\n",
    "        print(f\"Average substitutions per layer with results: {avg_substitutions:.2f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nError during analysis: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # Cleanup\n",
    "    print(\"\\nCleaning up...\")\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
